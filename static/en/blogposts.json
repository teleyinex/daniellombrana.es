{
  "2012-02-26-earthquakes": {
    "title": "Earthquakes & citizen scientists",
    "template": "entry",
    "slug": "earthquakes-citizen-science",
    "icon": "asiathome",
    "icon_author": "Daniel Lombraña",
    "icon_url": "http://www.flickr.com/photos/32985084@N00/6930671175/",
    "tags": "hackfest, science, citizen, earthquake, damage assessment, geology",
    "location": "Academia Sinica, Taipei",
    "meta_description": "Asia@Home citizen science workshop on earthquakes",
    "headline": "Asia@Home a workshop about citizen scientists studying earthquakes.",
    "layout": "blog",
    "preview": "The Asia@Home workshop started focused on earthquakes and volunteers …",
    "content": "\n\nThe Asia@Home workshop started focused on earthquakes and volunteers to track them. \nThe project The Quake Catcher Network presented their results an approach in different parts of the world (Chile, Christchurch, California, etc.) and a collaboration with Taiwan for monitoring earthquakes.\n\n<!--more-->\n![Jesse Lawrence](/assets/img/blog/jesse-lawrence.jpg){: .img-responsive}\n<p class=\"post-caption\">Jesse Lawrance by Daniel Lombraña</p>\n\n![Angela Chung](/assets/img/blog/angela-chung.jpg){: .img-responsive}\n<p class=\"post-caption\">Angela Chung by Daniel Lombraña</p>\n\nThis project uses low cost USB sensors to detect earthquakes that can be attached to any computer with an USB port. The only requirement is to screw the sensor to a wall or a surface so when there is an earthquake you the sensor will not jump.\n\nThere was a talk about a different sensor, <b>Palert</b>, that tries to measure earthquakes but using a different approach: self-sustained sensors that can have a battery to record the shock waves even though there is no power supply.\n\n![The Quake Catcher Network Sensor](/assets/img/blog/quake-catcher.jpg){: .img-responsive}\n<p class=\"post-caption\">The Quake Catcher Network Sensor by Daniel Lombraña</p>\n![Gadget](/assets/img/blog/gadget.jpg){: .img-responsive}\n<p class=\"post-caption\">A gadget by Daniel Lombraña</p>\n\n\nWhile all these talks presented the point of view of the projects, we have a nice talk from the point of view of the volunteers: Dudumomo. He presented several charts about how volunteers provide lots of CPU cycles to different projects, and only ask -in most of the cases- news about the research and progress that the project is achieving thanks to their donations.\n\n![Volunteer](/assets/img/blog/volunteer.jpg){: .img-responsive}\n<p class=\"post-caption\">The point of view of a vounteer by Daniel Lombraña</p>\n\nThere was a lot of discussion about different ideas to engage more volunteers, communities, outreach, new places for deployment, etc. After a first successful day, we relaxed a bit having dinner in one of my favorite Taiwanese restaurants: The House of tea, where everything, and when I say everything, is everything, is cooked with tea leaves <i class=\"twa twa-smile\"></i>.\n\n![Beef](/assets/img/blog/beef.jpg){: .img-responsive}\n<p class=\"post-caption\">Beef by Daniel Lombraña</p>\n![Shrimps](/assets/img/blog/shrimps.jpg){: .img-responsive}\n<p class=\"post-caption\">Shrimps by Daniel Lombraña</p>\n\n",
    "basename": "2012-02-26-earthquakes"
  },
  "2012-08-09-cernsummerwebfest": {
    "title": "CERN Summer Webfest",
    "template": "entry",
    "slug": "cernsummerwebfest2012",
    "icon": "cernsummerwebfest",
    "icon_author": "Daniel Lombraña",
    "icon_url": "http://www.flickr.com/photos/32985084@N00/7771808714/",
    "tags": "CERN, Webfest, hackfest, science, physics",
    "location": "CERN, Geneva, Switzerland",
    "meta_description": "CERN Summer Student Webfest",
    "headline": "CERN hosts the first webfest for building new tools with students.",
    "layout": "blog",
    "preview": "The first weekend of August of 2012, CERN hosted the first <a …",
    "content": "\n\nThe first weekend of August of 2012, CERN hosted the first <a href=\"http://www.citizencyberscience.net/cern-webfest/index.htm\">Summer Student Webfest</a> where physicists, designers, computer scientists and engineers worked together in a 48 hours marathon to create a set of amazing web applications prototypes involving physics and modern web technologies.\n<!--more-->\n![CERN Restaurant 1](/assets/img/blog/cern-resto1.jpg){: .img-responsive}\n<p class=\"post-caption\">CERN Restaurant 1</p>\n\nThe first day, different projects were presented so the students could decide in which one they wanted to participate. Some of the ideas were proposed by the organizers, but also some of the students came with really good suggestions. At the end the projects that were chosen to work over the 48 hours period were:<br />\n<ul class=\"angle-list\">\n<li><a href=\"http://www.citizencyberscience.net/wiki/index.php?title=Create_an_open_data_website_for_CERN\" target=\"_blank\">Open Data for CERN site</a></li>\n<li><a href=\"http://www.citizencyberscience.net/wiki/index.php?title=ParticleQuest_game\" target=\"_blank\">ParticleQuest a game to learn physics</a></li>\n<li><a href=\"http://www.citizencyberscience.net/wiki/index.php?title=A_cheaper_distributed_cosmic_ray_detector\" target=\"_blank\">A cheap Cosmic Ray detector built with Arduino and Android</a></li>\n<li><a href=\"http://www.citizencyberscience.net/wiki/index.php?title=Standard_Model,_Standard_Infographic\" target=\"_blank\">A new infographics and explanation for the Standard Model</a></li>\n<li><a href=\"http://www.citizencyberscience.net/wiki/index.php?title=Improve_the_LHC_Dashboard\" target=\"_blank\">A revamped LHC Dashboard using HTML5</a></li>\n<li><a href=\"http://www.citizencyberscience.net/wiki/index.php?title=Build_a_computer_center_in_a_virtual_world_to_show_volunteer_participation\" target=\"_blank\">Virtual Worlds for the LHC@Home platform</a></li>\n</ul>\n\n![CERN Restaurant 1](/assets/img/blog/webfest-2012-people.jpg){: .img-responsive}\n<p class=\"post-caption\">Hacking on science!</p>\n\n\nOne of the most repeated \"memes\" during this first day was that everyone wanted to&nbsp; make as easy as possible the explanations about physics for the general public. As a consequence of this overlapping,&nbsp; a huge team was created regarding the idea of explaining physics via a new Standard Model as <a href=\"http://press.web.cern.ch/press/PressReleases/Releases2012/PR17.12E.html\" target=\"_blank\">one month ago the Higgs Boson discovery was announced at CERN</a>.\n\nThe next 48 hours were amazing! The students arrived the Saturday morning around 10:00 am and they worked almost 48 hours non-stop in order to win the prize: a trip to the <span class=\"text-basic\">Mozilla Festival in London in November, courtesy of the Mozilla Foundation.\n\nThe event was free-form based, so some of the projects evolved during the event, and for example the mega-team around making more accessible physics for the public split themselves in two small teams that worked together in the Standard Model and a new awesome idea about creating a new web tool to create <a href=\"http://en.wikipedia.org/wiki/Feynman_diagram\" target=\"_blank\">Feyman diagrams</a> and use it to explain physics <i class=\"twa twa-smile\"></i>.\n\nIt is really difficult to tell you how amazing it was, so I \"interviewed\" each participant so they have to explain what they have done during the weekend and if they were enjoying the event. The following video (14 minutes long!) shows how people worked together in different teams and their projects. The video was recorded before the prize ceremony, so you can feel the pressure of not having enough time to improve a bit more their project <i class=\"twa twa-smile\"></i>.\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/cky9OFcoIsI\" frameborder=\"0\" allowfullscreen></iframe>\n</div>\n\nThe winning project was the <a href=\"http://particlequest.com/\">ParticleQuest game</a> (you can actually play it at http://particlequest.com) a fork of the Mozilla's Open Source game <a href=\"http://browserquest.mozilla.org/\" target=\"_blank\">BrowserQuest</a>.\n\nThe competition was really tough as the projects created really awesome applications, but the best part for me was seeing how these students got really involved in the event.\n\nWe asked the participants what they liked about the event and if they were happy. The mega-team basically summarized the event like this:\n<ul class=\"angle-list\">\n    <li><b>The event should start earlier</b>, so we have <b>more time to work</b> on the project.</li>\n    <li>We have been able to <b>self-organized</b> ourselves <b>without a supervisor</b>!</li>\n</ul>\n\nAmazing!! Indeed all the participants loved the event. Just to give you an example: after the prize ceremony <a href=\"http://en.wikipedia.org/wiki/John_Ellis_%28physicist%29\" target=\"_blank\">John Ellis</a> gave a special talk about the State of the Higgs Address. A few minutes after the full event was over, the students started to organized themselves again to keep working in their respective projects.\n\nAnother interesting outcome from the event was that only a few designers joined, but they were key persons to the success of all the projects, as they actually helped in every project (special mention to Andre-Pierre Olivier for his help in almost every project!).\n\nThe source code of the projects is available in <a href=\"https://github.com/CERNSummerWebfest\" target=\"_blank\">Github</a>, so if you want to know what we actually did during those 48 hours, go to Github and enjoy it!\n\nIf you want to see more photos from the event <a href=\"https://pl72543786574640449\" target=\"_blank\">check the album</a> I've created with pictures from almost all the participants!\n\nThe event was co-organized by the <a href=\"http://www.citizencyberscience.net/\">Citizen Cyberscience Centre</a> and the <a href=\"http://p2pu.org/\">Peer 2 Peer University</a>, and sponsored by the <a href=\"http://www.mozilla.org/foundation/\">Mozilla Fondation</a>  and the <a href=\"http://www.shuttleworthfoundation.org/\">Shuttleworth Fondation</a>.\n",
    "basename": "2012-08-09-cernsummerwebfest"
  },
  "2013-07-31-crodcrafting-data-journalism": {
    "title": "Analyzing Icelandic conviction rates",
    "template": "entry",
    "slug": "icelandic-conviction-rates-crowdcrafting",
    "icon": "justice",
    "icon_author": "mira66",
    "icon_url": "http://www.flickr.com/photos/21804434@N02/6250645786/",
    "tags": "crowdcrafting, PYBOSSA, data journalism, social",
    "location": "Adalstraeti, Reykjavik, Iceland",
    "meta_description": "Analyzing Icelandic conviction rates with PYBOSSA",
    "headline": "One single sample is not enought to draw conclusions.",
    "layout": "blog",
    "preview": "[Crowdcrafting.org](http://crowdcrafting.org) hosts a wide variety …",
    "content": "\n\n[Crowdcrafting.org](http://crowdcrafting.org) hosts a wide variety of applications that range from [science](http://crowdcrafting.org/app/airquality/) to \n[humanities](http://crowdcrafting.org/app/bardomatic/). Since the official launch of [Crowdcrafting.org](http://crowdcrafting.org), [lots of applications have been created](http://crowdcrafting.org/app/category/featured/)\n, but one of them has done a really impressive job: [Héraðsdómar - \nsýknað eða sakfellt](http://crowdcrafting.org/app/heradsdomar/).\n\n<!--more-->\n\n**Héraðsdómar - sýknað eða sakfellt** is an application developed by [Páll Hilmarsson](http://gogn.in/) ([@pallih](https://twitter.com/pallih), [Github](https://github.com/pallih)). The application was one of the most popular and active\napplications in Crowdcrafting.org when it was published (300 volunteers helped!), \nso I wanted to interview the author and ask some questions about it: why he created the application, \nwhat was the result, etc.\n\nPáll told me that he created the application after reading the [an article](http://www.visir.is/simon-sigvaldason-sakfellir-naer-alltaf/article/2012121229180) published in an Icelandic news web site.\n\n![](http://i.imgur.com/6GlMJ1p.png){: .img-responsive}\n\nThe article analyzed the **conviction rates of a named judged in the Reykjavik district court**,\nstating that the conviction rates for cases where he presided as a judge was 99%. \nPáll found it interesting, but also \"biased\" as the reporter only analyzed one judge.\n\nAfter the publication of the story, some bloggers and readers of the post, discussed \nabout why analyzing only one judge, reporting it back to the author. The journalist \naddressed all the questions and comments answering that  \ncalculating all the conviction rates for every case would take too long.\n\nPáll was not happy with this answer, so he decided to show him, and other reporters, that\nthis could be easily done by crowdsourcing the job, and that it would not take too long.\n\n**Páll uploaded around 4,700 rulings as tasks, and the volunteers analyze them in 7 days!** Each ruling \nwent to at least three different users, totaling 14,208 assessments. In the end more than\n17,000 assessments were made by over 300 users! (you can check the stats [here](http://crowdcrafting.org/app/heradsdomar/stats)).\n\nBut here it comes the best part, **Páll only spent 10 hours in this project** (including\nthe time to scrape the rulings, set up the tasks on Crowdcrafting.org and displaying\nthe [results on his blog](http://gogn.in/heradsdomar/). Amazing!\n",
    "basename": "2013-07-31-crodcrafting-data-journalism"
  },
  "2013-08-06-cernsummerwebfest2013": {
    "title": "2nd CERN Summer Webfest",
    "template": "entry",
    "slug": "cernsummerwebfest2013",
    "icon": "cernsummerwebfest2013",
    "icon_url": "http://www.flickr.com/photos/32985084@N00/5389725627/",
    "icon_author": "Daniel Lombraña",
    "tags": "CERN, Webfest, physics, science",
    "location": "CERN, Geneva, Switzerland",
    "meta_description": "CERN Summer Student Webfest",
    "headline": "Brilliant students from CERN participate in the 2nd CERN Summer Webfest.",
    "layout": "blog",
    "preview": "The second <a href=\"http://www.citizencyberscience.net/wiki\">CERN …",
    "content": "\n\n\nThe second <a href=\"http://www.citizencyberscience.net/wiki\">CERN Summer Student Webfest</a> has been an amazing event. Physicists, designers, computer scientists and engineers worked together in a 48 hours marathon to create a set of amazing [projects](http://www.citizencyberscience.net/wiki/index.php?title=Projects) involving physics and modern web technologies.\n\nThis year, students worked in data visualizations for the most popular scientists at CERN, an antimatter Crowdcrafting.org application, games for teaching and learning physics using different type of games: simulators, puzzles, strategy, etc., reproducible science, and more many more!\n\n\n<!--more-->\n\nThe first day, the students pitched several projects and ideas. Most of the students proposed a [game](http://www.citizencyberscience.net/wiki/index.php?title=Projects#Gaming) based solution for teaching and learning physics, becoming this category the most popular with 8 games! There were other categories:\n\n *  [Tools for science](http://www.citizencyberscience.net/wiki/index.php?title=Projects#Tools_for_Science)\n *  [Physics/Standard model](http://www.citizencyberscience.net/wiki/index.php?title=Projects#Physics.2FStandard_Model)\n *  [Data Visualization](http://www.citizencyberscience.net/wiki/index.php?title=Projects#Data_Visualization)\n *  [Education](http://www.citizencyberscience.net/wiki/index.php?title=Projects#Education)\n\n**This year we have introduced a series of tutorials** where the students could learn new technologies, techniques, etc. Additionally, we invited them to also propose to give some of the tutorials, making the webfest a very alive event where not only the organizers take part but also the participants suggesting and creating new elements in it.\n\nThe tutorials were really interesting:\n\n * [Making Website and Media using Mozilla Webmaker and Popcorn Maker](http://www.citizencyberscience.net/wiki/index.php?title=Making_Website_and_Media_using_Mozilla_Webmaker_and_Popcorn_Maker), by Michael Kohler (Mozilla).\n * [Introduction to Crowdcrafting/PYBOSSA](http://www.citizencyberscience.net/wiki/index.php?title=Introduction_to_Crowdcrafting/Pybossa), by Daniel Lombraña González (Citizen Cyberscience Centre).\n * [Facemesh Clone -- Web Programming Workshop](http://www.citizencyberscience.net/wiki/index.php?title=Facemesh_Clone_--_Web_Programming_Workshop) by S.p. Mohanty a CERN Summer Student.\n * [Hardware Programming and Arduino](http://www.citizencyberscience.net/wiki/index.php?title=Hardware_Programming_and_Arduino) by James Devine (CERN).\n * [Communicating Your Science](http://www.citizencyberscience.net/wiki/index.php?title=Communicating_Your_Science) by Julie Gould (The Movile Collective).\n * [Masterclass on particle physics -- Measuring lifetime of D0 particle](http://www.citizencyberscience.net/wiki/index.php?title=Masterclass_on_particle_physics_--_Measuring_lifetime_of_D0_particle) by Andrey Ustyuzhanin (CERN).\n\nAs in the previous year, the atmosphere was amazing. The students participated in the tutorials (when they were not really busy working on their projects), chat and about everything: code, code, code and work ;-)\n\nOn Sunday afternoon, the final pitches about the projects were made and the winner was announced: [Mother Hunt](http://www.citizencyberscience.net/wiki/index.php?title=Mother_Hunt). In this game you are an end state particle that explores CERN to try to reconstruct his family history of decay mothers and ancestors. \n\nThe team created a 3D world using some of the available models at CERN, making a very interesting and immersing game in the CERN scenario. They created a short story line that could be played in one computer, and they are hoping to release the software, so more people could play it and learn physics.\n\n![](http://i.imgur.com/fyxppic.png){: .img-responsive}\n\nThe other two final chosen projects were:\n\n * [Reproducible Science](http://www.citizencyberscience.net/wiki/index.php?title=Reproducible_Science_--_improving_scientific_research_to_the_next_level_of_clearness_%26_reproducibility): an IPython module for ROOT that allows any scientist using the ROOT software to share and improve their research on the web.\n * [Popular Physics in History](http://www.citizencyberscience.net/wiki/index.php?title=Popular_Physics_in_History) a web tool that analyzes the most popular scientists at CERN querying back end services like CERN Document Server and INSPIRE High Energy Physics Library.\n\nThe event was a success, and we hope that all the students had a lot of fun participating and developing their projects. We will be update this entry with more photos and videos, as soon as they are available.\n",
    "basename": "2013-08-06-cernsummerwebfest2013"
  },
  "2013-08-06-mapping-antimatter-with-crowdcrafting": {
    "title": "Mapping Antimatter tracks with Crowdcrafting.org",
    "template": "entry",
    "slug": "mapping-antimatter-crowdcrafting",
    "icon": "antimatter",
    "icon_url": "http://home.web.cern.ch/sites/home.web.cern.ch/files/styles/medium/public/image/experiment/2013/01/alpha.jpg?itok=ZPb7wNsC",
    "icon_author": "CERN",
    "tags": "CERN, crowdcrafting, Webfest, science, physics",
    "location": "CERN, Geneva, Switzerland",
    "meta_description": "A CERN antimatter citizen science application built with PYBOSSA",
    "headline": "Does antimatter fall up or down?",
    "layout": "blog",
    "preview": "This last weekend, CERN hosted a very special event: [the 2nd CERN …",
    "content": "\n\nThis last weekend, CERN hosted a very special event: [the 2nd CERN Summer Student Webfest](http://www.citizencyberscience.net/wiki/index.php?title=Main_Page) organized by the [Citizen Cyberscience Centre](http://www.citizencyberscience.net/).\n\nThe Webfest invites CERN summer students to participate in a 48 hours marathon hacking new applications, tools, games, etc. about physics. This year, I participated and worked in a very interesting one: [The Antimatter project](http://crowdcrafting.org/app/antimatter/).\n\n<!--more-->\n\nWith a team of around 8 persons, we divided the work in different areas and learned about the project and the goals for the Crowdcrafting application.\n\nMichael Doser from CERN and the spokesperson from the [AEgIS experiment](http://aegis.web.cern.ch/aegis/), is studying antimatter.\n\nBut, **what is antimatter?** The observable Universe is composed almost entirely of matter but we can produce stuff called antimatter in the lab. Antimatter is material composed of antiparticles. So for example, a positron (the antiparticle of an electron) combines with an antiproton to form an antihydrogen atom.\n\nAntiparticles have the same mass as normal matter particles but the opposite charge. When an antiparticle collides with an ordinary matter particle they both obliterate to emit radiation and some other particles - this is called annihilation.\n\nBecause of Einstein's weak equivalence principle (gravity doesn't depend on composition) antiparticles should interact gravitationally just like particles of ordinary matter - and that's what scientist's expect to observe - but if they don't then Einstein was wrong...\n\nWhat's the experiment?\n----------------------\n\nThe Antihydrogen Experiment: Gravity, Interferometry, Spectroscopy (AEgIS) experiment at CERN shoot antihydrogen atoms horizontally, whereupon they fly (and drop) until they hit a wall made of matter - any matter will do, silicon, silver, paper,... - and annihilate there\n\nOn hitting the wall, the antihydrogen annihilates with a nucleus of the wall to produce mostly pions and some other particles - which we'll call starburst.\n\nThe starburst travel through a special gel called an emulsion and we can see its tracks. If we trace these tracks to their point of origin then we know exactly where annihilation occurred.\n\nThen as we know the starting position of the antiparticles, the distance they travelled to the point of annihilation and how much they dropped - we can work out how far the antiparticle fell during its journey.\n\nThen we can figure out how antimatter interacts gravitationally.\n\n![CERN](http://i.imgur.com/uVVjKzD.jpg){: .img-responsive}\n\nMichael Doser gave us access to a set of 99 areas photographed with a microscope, that allows us to see tracks and the starbursts. Each of the areas have 40 pictures. These pictures cover the same area but at a differen depth.\n\nAs we discussed about the project, we decided to create a \"movie style\" task, where the Crowdcrafting application will be playing in a loop all the images for the same area. Then, we will allow the volunteers to map the tracks using their mouse as in any image software. The coordinates of the tracks, starting and ending points, will be saved, and we will use those points to render in real time a 3D model of the tracks thanks to WebGL.\n\nWe divided the work between different groups, and we worked together in the different areas:\n\n * Creating of tasks based on the data.\n * 2D movie style using HTML5 canvas feature.\n * 3D model of tracks using HTML5 WebGL.\n * Physics description of the problem and tutorial.\n\nFor the 2D Canvas solution we decided to use the popular [Kinetic.JS](http://www.kineticjs.com/) library. This library is very versatile as you can not only render images in the 2D canvas, but also paint lines. \n\nFor the 3D model we decided to use the popular [Three.JS library](http://threejs.org/). We created a 3D area using the Tron colors palette to draw the reported tracks by the users.\n\nThen, we have another group that worked really hard in explaining the physics of the experiment and the tutorial. We even created a [Mozilla Webmaker project](https://juanracasti.makes.org/popcorn/1adt) about it.\n\nAt the end of Sunday we had a fully operational prototype that allows you to actually track antimatter in Crowdcrafting:\n\n![Crowdcrafting project](https://github-camo.global.ssl.fastly.net/9a7c3a33b5470bf0c42f19f74a7443adf0e116ef/687474703a2f2f692e696d6775722e636f6d2f716b32393067352e706e67){: .img-responsive}\n<p class=\"post-caption\">Crowdsourcing antimatter prototype</p>\n\nFrom here I would like to thank to all the team members because the actually loved the project and push it to the next level. This efforts will help other Crowdcrafting/PYBOSSA developers to use the new HTML5 Canvas and WebGL features developed for this application, as the source code is already available in Github and can be used as a template for any Crowdcrafting/PYBOSSA application.\n\nIf you want, you can follow the [Github repository](https://github.com/CERNSummerWebfest/antimatter) development of the project.\n\n",
    "basename": "2013-08-06-mapping-antimatter-with-crowdcrafting"
  },
  "2013-09-23-video-tutorial": {
    "title": "Shooting and Editing videos in GNU/Linux",
    "template": "entry",
    "slug": "video-tutorial",
    "icon": "film",
    "icon_author": "Rami",
    "icon_url": "https://www.flickr.com/photos/rhk313/4846377172",
    "tags": "video, open source, tutorial, shuttleworth, tools, crowdcrafting, PYBOSSA",
    "location": "Madrid, Spain",
    "meta_description": "How to make a short film using only open source tools",
    "headline": "Open source tools for video editing.",
    "layout": "blog",
    "preview": "This year, in summer, I decided to apply for the prestigious …",
    "content": "\n\nThis year, in summer, I decided to apply for the prestigious [Shuttleworth Fellowship program](http://www.shuttleworthfoundation.org/fellowship-model/).\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n    <iframe src=\"//player.vimeo.com/video/33213160?title=0&amp;byline=0&amp;portrait=0&amp;color=8cbd01\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n</div>\n\nThe Fellowship program is incredible!!! Why? Because all the fellows are amazing and their projects and ideas are mind blowing :-) Just to give you an idea, here is a *short* list of their Alumni:\n\n * [Mark Surman, Executive Director of Mozilla](http://www.shuttleworthfoundation.org/fellows/mark-surman/)\n * [Francois Grey, Coordinator of Citizen Cyberscience Centre](http://www.shuttleworthfoundation.org/fellows/francois-grey/)\n * [Rufus Pollock, co-founder & director of Open Knowledge Foundation](http://www.shuttleworthfoundation.org/fellows/dr-rufus-pollock/), or \n * [Phillip Schmidt, Executive Director & Co-founder of Peer 2 Peer University](http://www.shuttleworthfoundation.org/fellows/philipp-schmidt/).\n\n<!--more-->\n\nAmazing right!? As you can see, the level is pretty high!!! If you additionally take a look to the [current fellows](http://www.shuttleworthfoundation.org/funding/current-fellows/) then you can start to tremble.\n\n## Fellowship Application\n\n[Applying for the fellowship](http://www.shuttleworthfoundation.org/funding/fellowship-programme/) has two steps: \n * the regular paperwork, plus\n * *a short video* pitching why they should fund you and your idea that would change the world.\n\n**Pitching a project or idea is always hard**. *Really hard*.\n\nYou need to work on it as much as possible, trying to express as clear as possible what do you want to achieve if they fund you. \n\nFor the written proposal, a good colleague and friend helped me to shape the document. However *for the video I was alone!*\n\nBefore moving forward I have to say that I own a *good* [<abbr>DSLR</abbr> camera](http://en.wikipedia.org/wiki/Digital_single-lens_reflex_camera). It is not a professional one, those are *really* expensive, but a modest one that allows me to record video in high quality without too many problems. \n\nAs you can see, the gear was not the problem. **The problem was that I've never filmed a short-movie in my life**.\n\n## Learning how to shoot a video \n\nOK, so I do not know how to shot a video, and I only have 2 weeks to write the application, shoot the video, edit it, and send everything to the program. Did I hear the word: stress? :-)\n\nWell, as I didn't have any idea about how to shoot a video with my DSLR camera *I decided to learn about it*. I love films, art, etc. so I usually visit the video website [Vimeo](http://vimeo.com). Vimeo is like the playground for artists, where they show their creations. Just to give you an idea how creative are the users, check this video:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n    <iframe src=\"//player.vimeo.com/video/67809013?title=0&amp;byline=0&amp;portrait=0&amp;color=0094f0\"  frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n</div>\n\nObviously, I would love to create something like that, but I've to be realistic. First I didn't know how to shoot properly a video, second you need to be *very creative* to build something similar to the previous movie, and third I only had two weeks. Additionally, I only had used a very simple video editor in my life: [OpenShot](http://www.openshotvideo.com/). Hence, I just wanted to learn the very basics principles, and apply them for my video application.\n\nLuckily, the Vimeo crew have created the [School of Video](http://vimeo.com/videoschool) with a specific set of lessons just for [DSLR Cameras](http://vimeo.com/videoschool/lesson/11/introduction-to-dslr-cameras).\n\nThe DSLR lessons are a must if you want to shoot with your reflex camera. I watched all of them while I had my camera with me, configuring it properly, and experimenting a bit with it. If you can, watch all videos, they are very helpful and you will learn a lot.\n\nOnce I had the proper configuration for the camera, I had to actually shoot the video. But before going crazy and shooting the video I decided to do a bit of research again. I basically expended one afternoon browsing Vimeo and Youtube looking for other video applications for the Shuttleworth program to see what other people have done.\n\n*In general all the videos were shot with one cut, with the applicant in front of it pitching his/her project, using almost all the time available (5 minutes)*. \n\nWhile I was watching them I knew that I had to do something different, if I wanted to get the attention of the Shuttleworth judges. Hence, I decided to create several cuts and reduce the video length as much as possible. Why? Because I didn't want to get them bored :-)\n\nNow that I knew the initial structure, it was time to learn a bit more about how to tell a story with a film. \n\n## The Art of Storytelling\n\nAgain, I went back to the school of video for more resources and I found this amazing video:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n    <iframe src=\"//player.vimeo.com/video/64207362?title=0&amp;byline=0&amp;portrait=0&amp;color=0094f0\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n</div>\n\nThe video explains perfectly well how your script has to look like. Basically you need to answer the following questions or points:\n\n * **People: Who is in the story?** This one was easy to answer: me!\n * **Place: Where does the story take place?** Easy: Madrid, Spain.\n * **Plot: What is the conflict and the journey?** This is the tough one! Basically pitching my project.\n * **Purpose: Why should anyone care about this?** Another complicated one. Here I explained why I'm pitching my project and why is important.\n\nAfter watching the video I started to write down my script answering those questions. The process helped me a lot to focus on what I wanted to tell, and more importantly: how I wanted to tell it. \n\nMy pitch is about **Citizen Science**: *citizens doing science themselves with simple tools that I am building*. \n\n*Citizens are a key aspect of my pitch*, so I wanted to give them a lot of relevance and I decided that I needed to film citizens, but where? \n\nWell, the question **Place** gave me the answer. As I am based in Madrid, I decided to present the place, Madrid, filming in the most popular places: Sol, Callao and Plaza Mayor. \n\nAs I was going to shoot in the streets, I needed to add also a sound track according to the city style, so I browsed several open licensed tracks from Hip-Hop that could fit with the video. As you can see, I was giving body to the video thanks to answering the four P's.\n\nThen I wrote plot. I expend a lot of time refining it until I was happy with it. I recorded myself several times repeating the text, just to get used to what I wanted to say, to check the rhythm, etc. This process helped me to rephrase several times parts of the script and to make it the way I wanted. It may sound silly, but I recommend you to film yourself to relax, learn your script, look natural, etc.\n\nFinally, my wife and I went to the final place where she was going to film me. But when we were preparing I realized that we had a problem: the quality of sound as we were going to shoot in a public space. \n\nThe previous cuts were fine, because the sound of the camera were going to be overwritten by the sound track. However, in this new cuts I needed to sound as clear as possible. Therefore, how can I do it without a professional camera? The solution: back to Vimeo and Google to do some research about it.\n\n## Recording clear sound\n\nIn general people recommend you to use a separated voice recorder, so you can sync the audio later in the video editing tool. I didn't want to buy a recorder just for this video, but it looked like it was the only feasible solution. While I was checking prices, I had an idea: what if I used my own smartphone with its headphones and integrated mic? It sounded crazy, but it was worthy to give it a try.\n\nI set it up: the phone in my pocket, the headphones inside my polo using one of the buttons to lock the mic properly and hide it from the view of the camera. Then, I pressed the record button, say the standard \"hello, hello, one, two three\"  and a new audio clip was saved. It worked like a charm, and the cost of it was 0 EUR :-)\n\n## A cheap prompter\n\nWith everything more or less solved, it was time to create a prompter. The solution: a spiral notebook. Each page was the text I've to say when recording a cut, so it was really easy for me to read it. Tips: use capital letters, so you can read it clearly, and adjust the length of the sentence in a way that it fits the rhythm that you want to give to your pitch. Try it several times before going to shoot, until you are really comfortable with the result.\n\nWe filmed every cut several times, from different angles, and in different places. This work flow allowed me to choose between different videos and try different ideas when editing it. With all the materials saved in my hard disk, it was time to start the video editing, or how I called it: *the panicking area*.\n\n## Editing the video (aka panicking mode on!)\n\nI am a big fan and user of open source software. I release [all my photos under an open license](http://flickr.com/photos/teleyinex) and all [the code I develop is also open source](http://github.com/teleyinex). In other words, I am a true believer in open source, so I wanted to create the full video using only open source editing tools. In this case [OpenShot](http://www.openshotvideo.com/).\n\nOpenShot is very handy. You can separate several tracks in your movie, add vectorial tittles (that you can edit with [Inkscape](http://inkscape.org/)), sound clips, make transitions, etc. It looks amazing, right? Well,I guess that for small projects is not a problem, but for me it was a really frustrating time because every time I saved the project or changed something, the application crashed.\n\nThe good thing though, was that even tough it crashed almost all the time, the progress was always saved and I never lost any information.\n\nThe first thing I worked on were the titles. I used [Inkscape](http://inkscape.org/) to create them, and again, in order to give the same look and feel to the whole video, I searched for a nice font and a background to overlay the text. As the main theme for the video is the city, I decided to use some graffiti droplets as the background for the text, keeping a clean and modern font.\n\nThe next step was to sync the audio files with the videos. To my surprise *I discovered that OpenShot doesn't have that feature*, so I started to freak out. Literally. Two breaths later, I did some *Googling* and fortunately I found another open source video editor that actually can do the sync \"automatically\": [PiTiVi](http://www.pitivi.org/).\n\nI installed the software, and I tried out. The first time it worked, so I was really happy (later I discovered that for some video and audio clips, it never synchronized the audio tracks; as I filmed several times the same cut, I'd always a pair of clips that worked). Thus, my current work flow was the following:\n\n * Import video and sound clip in PiTiVi,\n * Sync the sound in both items,\n * Export the final video with the synchronized audio,\n * Import the video in OpenShot to do the final editing.\n\nDid you say: painful? Yes, really painful. Add also that every time I changed something in OpenShot it crashed, so you can have an idea how painful it was. However, I wanted to create the video using only open software tools so I forced myself, and the result was this:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n    <iframe src=\"//player.vimeo.com/video/66246703?title=0&amp;byline=0&amp;portrait=0\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n</div>\n\nAfter all, the experience was really beautiful because I learned a lot. Some people would say that open source tools sucks (I do not share that feeling at all), but actually, thanks to that I've learned a lot, due to all the research I did. \n\nWhen I finished my video, I checked the OpenShot web site to see if there is a new version that fixes all the problems I had,  and luckily it seems like a new version is in the oven. Thus, in the future it should be much easier to do it in GNU/Linux.\n\nFinal note: I want to try the popular [Lightworks](http://libregraphicsworld.org/blog/entry/first-look-at-lightworks-beta-for-linux) and see how good it is, but as usual, I do not have enough time <i class=\"twa twa-smile\"></i>.\n",
    "basename": "2013-09-23-video-tutorial"
  },
  "2013-11-26-pybossa-cache": {
    "title": "Adding a load balanced & high-availability cache to PYBOSSA",
    "template": "entry",
    "slug": "pybossa-cache",
    "icon": "balance",
    "icon_author": "Felipe Gabaldón",
    "icon_url": "https://www.flickr.com/photos/felipe_gabaldon/4334984556",
    "tags": "cache, high-availability, load balance, crowdcrafting, PYBOSSA",
    "location": "Madrid, Spain",
    "meta_description": "Adding a load balanced & high-availability cache to PYBOSSA",
    "headline": "Redis.io is an amazing software for building a cache system.",
    "layout": "blog",
    "preview": "In the last days I have been working really hard to add a new cache …",
    "content": "\n\nIn the last days I have been working really hard to add a new cache system to PYBOSSA.\n\nUp to now, [PYBOSSA](http://daniellombrana.es/pybossa.html) has been using [Flask-Cache](http://pythonhosted.org/Flask-Cache/), an extension for the [Flask](http://flask.pocoo.org/) micro-framework\nthat allows you to use several types of caching backends (i.e. [memcached](http://memcached.org/) or [Redis](http://redis.io)).\n\n<!--more-->\n\n[Crowdcrafting](http://crowdcrafting.org/) was using memcached, as it is a robust solution and it is really well documented.\nHowever in the last days due to high loads in the server, I've been thinking how I can\nimprove the situation, and have a cache system that meets the following criteria:\n\n* Load-balanced: queries should be balanced between a master-slave architecture within PYBOSSA.\n* High-availability: if one slave or master node goes down, the system should recover itself without having to do anything special in PYBOSSA.\n* Persistence: avoid warming up the cache every time you need to reboot the server.\n\nAfter checking several solutions, the best candidate that meets those requirements is: [Redis](http://redis.io).\n\n[Redis](http://redis.io) is *amazing*. Let me repeat it: **amazing**. \n\nThe reason I decided to start using \nRedis as the main cache for PYBOSSA is that it has an incredible set of [features](http://redis.io/topics/introduction), \na very simple master-slave setup, it is asynchronous, and over all those features: it gives you [Sentinel](http://redis.io/topics/sentinel).\n\nSentinel is a system that allows you to monitor your master-slave setup, and discover \nRedis services in a very simple way. Fortunately, there is a [Python client](https://github.com/andymccurdy/redis-py) that uses \nthe Sentinel mode, and allows you to connect your Python software to Redis, handling behind\nthe curtains which nodes are alive and which nodes are not. \n\nAs Sentinel is perfect for what I wanted to build, I decided to start using it. \nThe result a new PYBOSSA caching module based on Redis.\n\nThe [new module is very simple](https://github.com/PYBOSSA/pybossa/blob/master/pybossa/cache/__init__.py), it uses Sentinel to discover the master-slave setup\nand configure two clients: master and slave. Then, all read actions are loaded from the slave node, and all write actions are done in the master, balancing the \nload between the infrastructure.\n\nThe most interesting aspect of this setup, is that you can add more slaves in real time, \nand Sentinel will handle them for you. If the master node dies, then it will look for \nanother master to configure it (this needs another Sentinel node), and voile: system \nup and running without having to do any special.\n\nToday I've deployed this new version (bumping PYBOSSA version to v0.2.0) in [Crowdcrafting](http://crowdcrafting.org) \nand the speed of the site is really awesome!\n\nThe only \"issue\" with Redis, is that Ubuntu LTS does not have the most recent version \nof it (not even 2.6), so you have to download it manually, and compile it. The good news are\nthat after installing the Ubuntu package *build-essential* you should have all the requirements to run\na simple *make* command and build it. \n\n**NOTE**: in order to add it to the init.d section of Ubuntu, install first the old version via the\npackage manager, copy the files */etc/init.d/redis-server* and */etc/redis/redis.conf* to your\nhome folder, modify them to your needs (use the redis.conf as a guide for your redis.conf 2.6 config\nfile) and you will be done. Another option is to configure everything with [Supervisor](http://supervisord.org/)\nwhich would be pretty handy (mental note, write a blog post about it).\n\n",
    "basename": "2013-11-26-pybossa-cache"
  },
  "2013-12-02-hackfest-cienciaciudadana": {
    "title": "Citizen Science hackfest in Madrid",
    "template": "entry",
    "slug": "cienciaciudadana",
    "icon": "balloon",
    "icon_url": "http://www.flickr.com/photos/32985084@N00/11170324726/",
    "icon_author": "Daniel Lombraña",
    "tags": "citizen, science, spectrometer, balloonmapping, epicollect, crowdcrafting, PYBOSSA",
    "location": "Madrid, Spain",
    "meta_description": "Citizen Science hackfest in Madrid",
    "headline": "Madrid citizens participate in a citizen science hackfest.",
    "layout": "blog",
    "preview": "The 29 and 30 of November of 2013 I organized with the support of …",
    "content": "\n\nThe 29 and 30 of November of 2013 I organized with the support of [FECYT](http://fecyt.es/), [La Caixa Forum](http://obrasocial.lacaixa.es/nuestroscentros/caixaforummadrid/caixaforummadrid_es.html) and\n[Medialab-Prado](http://medialab-prado.es) the second [citizen science hackfest](http://medialab-prado.es/article/encuentrodecienciaciudadana) in Madrid, Spain.\n\n<!--more-->\nThe event this time had the participation of two respected experts in the field of Citizen Science:\n\n* Francois Grey, from [Citizen Cyberscience Centre](http://citizencyberscience.net),\n* Shannon Dosemagen, from [Public Labs](http://publiclab.org/), and\n* Fermín Serrano, from [Ibercivis Foundation](http://ibercivis.es/)\n\nThe hackfest started with a morning session with talks from all the experts in a dedicated session\nfor the FECYT ComCiRed event at Medialab-Prado. \n\nFrancois Grey introduced the past,\npresent and future of citizen science. \n\nShannon explained how her team create open hardware tools\nfor measuring the world (balloon mapping tools, spectrometers, infrared cameras, etc.) representing\nthe *volunteer sensing* area. \n\nThen, Fermín Serrano presented the Ibercivis Foundation and its work\nin Spain regarding *volunteer computing* with several research projects. \n\nI closed the session\ntalking about *volunteer thinking* showing how [Crowdcrafting](http://crowdcrafting.org) can\nbe used by citizens or researchers to create citizen science projects in minutes.\n\n\nAfter lunch, we meet again and presented the proposed projects to work during the 30th of November\nat Medialab-Prado. The Saturday started with a short briefing for the day, explaining the [micro-workshops](https://etherpad.mozilla.org/cienciaciudadana)\nthat we will be having along the day:\n\n* Balloon Mapping: mapping your surroundings,\n* EpiCollect+: gathering data with your phone,\n* Crowdcrafting: creating your own citizen science project in minutes, and\n* DIY Spectrometer\n\n## Balloon Mapping\n\nWe started with the balloon mapping workshop. Shannon explained step by step how to \nbuild the balloon mapping using one of the kits they shell (but that you can build yourself if you want).\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe width=\"560\" height=\"315\" style=\"max-width:100%;min-width:200px;\" src=\"https://mixbit.com/embed/119uFaWsCf3DVXsxoWYFg3\" frameborder=\"0\" scrolling=\"no\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n</div>\n\nWe had a very good weather in terms of wind: the balloon was really stable as well as the\ncamera, however it was really cold!\n\n![The materials](https://farm4.staticflickr.com/3828/11170358894_7e6d1d8a4c_k_d.jpg){: .img-responsive}\n<p class=\"post-caption\">The materials</p>\n\n![Filling in the ballon](https://farm6.staticflickr.com/5521/11170361244_c6c44c51f2_k_d.jpg){: .img-responsive}\n<p class=\"post-caption\">Filling in the ballon</p>\n\n## EpiCollect+\n\nThen, we had the [EpiCollect+](http://plus.epicollect.net/) workshop. We worked in creating an Odour Collect project\nthat basically asks volunteers to quantify or calify the odours that you can find in the\ncity. All you need to participate, is to install the EpiCollect+ application in your Android\nphone (iOS coming soon!) and join the [project](http://plus.epicollect.net/OdourCollect/).\n\nWe also worked in adding more data samples for the [Crowdcrafting Lichens project](http://crowdcrafting.org/app/airquality/)\nwith the participants of the events. We went to park El Retiro, split in teams, and take\nseveral samples with our phones. The result: [almost 50 new samples from Madrid city](http://plus.epicollect.net/lichens/Lichens) that\ncan be directly analyzed in the [Crowdcrafting project](http://crowdcrafting.org/app/airquality/).\n\n## Crowdcrafting\n\nThen we moved to the Crowdcrafting workshop, were I teached and showed how you can create a\ncitizen science project using the available templates, and more impotantly, how you can import\nthe data from the previous two workshops into a Crowdcrafting application, and analyze the data\nwith the help of the volunteers.\n\nFor example, after taking the pictures of the balloon mapping workshop, a group of the participants\northo-referenced the pictures for creating the following map:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe style=\"border:none;\" width=\"500\" height=\"375\" src=\"http://archive.publiclab.org/leaflet/?tms=http://mapknitter.org/tms/plant-wall-in-madrid/&lat=40.4110814393&lon=-3.6931872467\"></iframe>\n</div>\n\nThat was imported into a [Crowdcrafting application](http://crowdcrafting.org/app/balloonmappingmadrid/), using this [template](http://github.com/PYBOSSA/app-mapknitter). \nThe result, people can help you to count the number of participants in the workshop really easily. \n\nThe template provides also an example for measuring areas within the map, and this was the clue to \npropose to the [Medialab-Prado Urban Bees Keepers](http://mieldebarrio.wordpress.com/) to use this\ntechnique to map the roofs of the city, and quantify which ones are good or not (sometimes they cannot\naccess the roofs) and also to measure how many square meters are available to place several hives.\n\n## DIY Spectrometer\n\nWe ended the hackfest with the last workshop: build your own spectrometer. This time, we had the collaboration\nof the Medialab-Prado Origami team, as this spectrometer is built using a sheet of paper. In the following\nvideo you can see how we built several spectrometers and play with them.\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe width=\"560\" height=\"315\" style=\"max-width:100%;min-width:200px;\" src=\"https://mixbit.com/embed/_37DiF9IzVhbfd908kPeKzi\" frameborder=\"0\" scrolling=\"no\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n</div>\n\nAs you can see, we learned a lot, we produced several prototypes along the day, and\nabove all: *we had a lot of fun while learning*. If you want to participate in the next\ncitizen science hackfest in Madrid, keep an eye in the [Medialab-Prado website](http://medialab-prado.es),\nmy site and/or [Twitter account](http://twitter.com/teleyinex).\n\nPS: I would like to give a **big thank you** to FECYT, La Caixa Forum and Medialab-Prado\nfor supporting the event, to Francois, Shanon and Fermín for coming to Madrid and present their\nwork in citizen science, and also to all the participants: without you this would not be possible! THANKS!!!\n",
    "basename": "2013-12-02-hackfest-cienciaciudadana"
  },
  "2013-12-16-analyzing-pybossa": {
    "title": "Sharing your PYBOSSA application analysis with Enki",
    "template": "entry",
    "slug": "pybossa-enki",
    "icon": "enki",
    "icon_author": "Georgelazenby",
    "icon_url": "http://en.wikipedia.org/wiki/File:Chaos_Monster_and_Sun_God.png",
    "tags": "enki, science, analysis, sharing, open science, crowdcrafting, PYBOSSA",
    "location": "Madrid, Spain",
    "meta_description": "Sharing your PYBOSSA application analysis with Enki",
    "headline": "Analyzing your PYBOSSA project with Python.",
    "layout": "blog",
    "preview": "Up to now if you have created your own application in your [PYBOSSA …",
    "content": "\n\nUp to now if you have created your own application in your [PYBOSSA server](http://daniellombrana.es/pybossa.html) or\nin [Crowdcrafting](http://crowdcrafting.org) you have to create your own library or script to download the results\nfrom the server and process them.\n\nPYBOSSA provides links for every application to download the tasks and task runs in a CSV file or in JSON (if you want\nto hack around it more easily), however doing the analysis usually involved some work that could be tedious.\n\nIn order to solve this problem, I decided to creat a very simple to use Python package that downloads all the completed tasks\nand its associated answers and gives you the right tools for doing the analysis without having to write none of the usual tests.\n\nThe result: [Enki](http://github.com/PYBOSSA/enki)\n\n\n## Enki\n\nEnki is a very simple package that uses the [PYBOSSA-Client](http://github.com/PYBOSSA/pybossa-client) and [Pandas](http://pandas.pydata.org/)\npackage to achieve the following two items:\n\n* Get all the tasks and task_runs,\n* Analyze them using the amazing Pandas framework.\n\nEnki is so simple to use, that all you have to do to analyze your app is the following:\n\n```\n    >>> import enki\n    >>> e = enki.Enki(api_key='private', endpoint='http://server', app_short_name='your-app-slug')\n    >>> e.get_all()\n```\n\nOnce all the tasks and task_runs have been downloaded, Enki will create automatically for you a list of data frames for the tasks,\nand a dictionary where the keys are the tasks IDs and the values are data frames of the associated task runs.\n\nThen, if your application was for example asking if you see a human face in a picture, and the available answers are: Yes, No and I do not know,\ngetting the most voted answer is as simple as this:\n\n```\n    >>> task = e.tasks[0]\n    >>> e.task_runs_df[task.id]['info'].describe()\n```\n\nAs Enki uses Pandas software, the describe function will detect if it is dealing with strings, or with numbers. In the latter case you\nwill get the mean, avg, std, etc. as a result of the output. Handy, right?\n\n## Ipython + Enki = sharing the analysis \n\nTherefore, Enki is the right tool for doing the analysis of any of your PYBOSSA applications. However, I wanted to provide a more\ninteresting tool. I wanted to easily share the analysis in order to embrace **open science**, so other researchers and citizens could\nreplicate my analysis and let me know if I made a mistake.\n\nFor these reasons, as Pandas is integrated with Ipython you can create a notebook and share it with the rest of the world using\nthe [Ipython Notebook viewer](http://nbviewer.ipython.org).\n\nThis integration is **amazing**. Now you can not only share your study, you can also get feedback and patches on your notebook,\ntweet about it, and improve it over time because you can save the notebook as a [Github Gist](https://gist.github.com/).\n\nIn order to show how awesome is this feature, I have created a simple notebook to analyze the [Crowdcrafting Video Pattern Recognition application](http://crowdcrafting.org/app/vimeo/).\n\nThe Gist is available [here](https://gist.github.com/teleyinex/7991086), and you can directly view it [here](http://nbviewer.ipython.org/gist/teleyinex/7991086).\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe class=\"hidden-phone\" src=\"http://nbviewer.ipython.org/gist/teleyinex/7991086\" style=\"width:100%; height:500px;\">\n</iframe>\n</div>\n\nThus, if you have a Crowdcrafting application and you need to analyze it, start using [Enki](http://github.com/PYBOSSA/enki).\n\n**NOTE**: [Enki is the name of a Sumerian god of the intelligence](http://en.wikipedia.org/wiki/Enki).\n",
    "basename": "2013-12-16-analyzing-pybossa"
  },
  "2014-01-14-la-aventura-del-saber": {
    "title": "The adventure of knowing",
    "template": "entry",
    "slug": "la-aventura-del-saber",
    "icon": "adventure",
    "icon_url": "http://www.flickr.com/photos/thekellyscope/4450666660/",
    "icon_author": "Sean Kelly",
    "tags": "learning, tv, show, open science, crowdcrafting, PYBOSSA",
    "location": "Madrid, Spain",
    "meta_description": "The adventure of knowing",
    "headline": "Hacking together at Medialab-Prado.",
    "layout": "blog",
    "preview": "Last year, I was the director of a [citizen science …",
    "content": "\n\nLast year, I was the director of a [citizen science hackfest](http://daniellombrana.es/blog/2013/12/02/cienciaciudadana.html), with international and well respected professionals on the field like Francois Grey from the [Citizen Cyberscience Centre](http://citizencyberscience.net/) and Shannon Dosemagen from [Public Labs](http://publiclab.org/).\n\nThe event [was covered](http://www.rtve.es/alacarta/videos/la-aventura-del-saber/aventura-del-saber-13-01-14/2306728/) -*starts at minute 0:40:19*- by the Spanish national tv, [La 2](http://www.rtve.es/alacarta/tve/la2/) for a show named [La aventura del saber](http://www.rtve.es/television/la-aventura-del-saber/) with more than 18 years on air.\n\n<!--more-->\nHere you have the snippet of the program showing the event (*starts at minute 00:05:30*) and the venue of the event [Medialab-Prado](http://medialab-prado.es/).\n\n<div  style=\"width:100%;padding-top:64%;position:relative;display:inline-block;background:#eee;background:rgba(255,255,255,0.9);\"  >\n    <iframe frameborder=\"0\" src=\"http://www.rtve.es/drmn/embed/video/2306808\"\n            name=\"La Aventura del Saber. Medialab-Prado\" scrolling=\"no\" style=\"width:100%;height:90%;position:absolute;left:0;top:0;overflow:hidden;\"  ></iframe>\n    <div style=\"position:absolute;bottom:0;left:0;font-family:arial,helvetica,sans-serif;font-size:12px;line-height:1.833;display:inline-block;padding:5px 0 5px 10px;\">\n        <span style=\"float:left;margin-right:10px;\"><img\n                style=\"height:20px;width:auto;background: transparent;padding:0;margin:0;\"\n                src=\"http://img.irtve.es/css/rtve.commons/rtve.header.footer/i/logoRTVEes.png\"></span> <a\n            style=\"color:#333;font-weight:bold;\" title=\"La Aventura del Saber. Medialab-Prado\"\n            href=\"http://www.rtve.es/alacarta/videos/la-aventura-del-saber/aventura-del-saber-medialab-prado/2306808/\"><strong>La Aventura del Saber. Medialab-Prado</strong></a></div>\n</div>\n\nLa aventura del saber (or the adventure of knowing) is a daily show in the mornings of the Spanish national tv, with the goal of teaching and divulge using interviews and documentaries. \n\n\nEnjoy!!!\n\n",
    "basename": "2014-01-14-la-aventura-del-saber"
  },
  "2014-02-18-entrevista-aventura-del-saber": {
    "title": "Interview",
    "slug": "interview-aventura-del-saber",
    "icon": "camera",
    "icon_author": "EstatesGazzette",
    "icon_url": "http://www.flickr.com/photos/48698930@N04/7637177726/",
    "tags": "citizen science, tv, interview, open science, crowdcrafting, PYBOSSA",
    "location": "Madrid, Spain",
    "meta_description": "The adventure of knowing interview",
    "headline": "Citizen science means opening the doors of the labs.",
    "layout": "blog",
    "preview": "Last week I was invited by [La Aventura del …",
    "content": "\n\nLast week I was invited by [La Aventura del Saber](http://www.rtve.es/television/la-aventura-del-saber/), a TV show from the **Spanish National TV**, \nto talk about Citizen Science and explain it.\n\nTo me it was a great surprise, one of those that make your day, and I felt really honored and proud to be part of their program.\n\nToday, the interview was aired, and thanks to their on demand service, you can now watch it here (note, only in Spanish, sorry!). Enjoy!\n\n<!--more-->\n\n<div  style=\"width:100%;padding-top:64%;position:relative;display:inline-block;background:#eee;background:rgba(255,255,255,0.9);\"  >\n    <iframe frameborder=\"0\" src=\"http://www.rtve.es/drmn/embed/video/2403790\"\n            name=\"La aventura del Saber. Daniel Lombraña. Ciencia ciudadana\" scrolling=\"no\" style=\"width:100%;height:90%;position:absolute;left:0;top:0;overflow:hidden;\"  ></iframe>\n    <div style=\"position:absolute;bottom:0;left:0;font-family:arial,helvetica,sans-serif;font-size:12px;line-height:1.833;display:inline-block;padding:5px 0 5px 10px;\">\n        <span style=\"float:left;margin-right:10px;\"><img\n                style=\"height:20px;width:auto;background: transparent;padding:0;margin:0;\"\n                src=\"http://img.irtve.es/css/rtve.commons/rtve.header.footer/i/logoRTVEes.png\"></span> <a\n            style=\"color:#333;font-weight:bold;\" title=\"La aventura del Saber. Daniel Lombraña. Ciencia ciudadana\"\n            href=\"http://www.rtve.es/alacarta/videos/la-aventura-del-saber/aventura-del-saber-daniel-lombrana-ciencia-ciudadana/2403790/\"><strong>La aventura del Saber. Daniel Lombraña. Ciencia ciudadana</strong></a></div>\n</div>\n\n\n",
    "basename": "2014-02-18-entrevista-aventura-del-saber"
  },
  "2014-02-26-reinventing-the-wheel": {
    "title": "Reinventing the wheel",
    "template": "entry",
    "slug": "reinventing-the-wheel",
    "icon": "wheel",
    "icon_author": "Rego Korosi",
    "icon_url": "https://www.flickr.com/photos/korosirego/4849943530",
    "tags": "citizen science, open source, PYBOSSA, evolution, discussion",
    "location": "London, United Kingdom",
    "meta_description": "Reinventing the wheel",
    "headline": "Reinventing the wheel can be treated as a technique that will assure enough genetic diversity.",
    "layout": "blog",
    "preview": "In the last three days I've participated in the third [Citizen …",
    "content": "\n\nIn the last three days I've participated in the third [Citizen Cyberscience Summit](http://cybersciencesummit.org/).\n\nThe event has been the biggest so far, and you could appreciate it not only in the number of people, but also in the number of new projects coming out from the crowd.\n\nAfter several talks and workshops you could actually see a trend: **different groups are reinventing the wheel**. Now the obligated question: **is this a good or a bad thing?**\n\n<!--more-->\n\nBefore going forward, let me be clear, in my personal case I'm doing the same, I'm developing *another* citizen science framework, so to some extent I'm contributing to reinventing the wheel, right?\n\nReinventing the wheel is not new to me. I'm a GNU/Linux user since Red Hat 5.2 and I can say that I've heard the same argument several times in the open source community: another window manager? This is the year for GNU/Linux.\n\n![meme](http://i.imgur.com/JBYFSxa.png){: .img-responsive}\n\nIf you want to check what I'm saying, all you have to do is the following: search in Google the name of an open source project, go to its e-mail list and your chances of finding a heated discussion about this specific issue are high, quite high (just check the last controversy about SystemD vs Upstart [here](http://www.reddit.com/r/linux/comments/1w9qtv/the_design_flaws_of_upstart/) and [here](http://ewontfix.com/14/) as two different points of view).\n\nWhile I'm used to this type of controversy, the truth is that *I've not been thinking really carefully about it, until the conference*. In the summit we have had the same conversations, everyone complaining about reinventing the wheel but all of us creating our own wheel and saying to the rest of the world: **this IS THE wheel**.\n\nMoreover, **it looks like for everyone reinventing the wheel is something bad**. **But is it really bad?** Have you think about it carefully?\n\nOne of the most used arguments to say that **reinventing the wheel** is something bad, is that it is a **waste of energy and resources**, repeating almost exactly the same features. The point is that: both solutions should join forces and build a better unified solution.\n\nWhat do you think? Is it a waste of time, energy and resources? My answer: yes, it is, but is this enough to stop trying reinventing the wheel?\n\nDuring these three days I've thinking about all these issues and I've realized that when we are arguing about it, what we are actually doing is describing [the evolution of species](http://en.wikipedia.org/wiki/On_the_Origin_of_Species) (thanks Darwin!).\n\nYes, you have read it right: the evolution of species. Let me explain it to you. \n\nLet's pick a topic completely randomly (cough): *citizen science*. Now let's pick a problem within it: *developing a citizen science framework*. \n\nDeveloping the best possible and imaginable citizen science framework is challenging, right? Thus, we can say that finding a solution is going to be complicated, really difficult.\n\nLet's imagine that the perfect framework exists. However, we do not know where it is, or how it looks like. For this reason we have a huge search space. Really big (deep space). Even though it looks like it is almost impossible to find a solution, some people will try and start building their own solution.\n\n![meme](http://i.imgur.com/tn0NZMx.png){: .img-responsive}\n\nAs with every new born, the very first version is not going to be the best one, it lacks a lot of features, fails a lot, and has a very small community (generation zero). While this team is developing their solution, another group starts something similar without even knowing anything about the previous work. Interestingly, they are focused on the same goals, and therefore they will build similar solutions: they share some [building blocks](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4983245&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4983245). In other words, they are reinventing the wheel.\n\n**But the truth is that even though they share some building blocks, both projects are not quite the same**. They have their own unique features: how the project is handled, the employed technology, the community around it, etc, etc.\n\nIn summary we have two individuals (aka as candidate solutions) looking to **survive** in the field of citizen science. In order to know which solution is the best one, we have a fitness function that can compare them: i.e. **sustainability**. At this point you might realize that what I'm describing it is an [evolutionary algorithm](http://en.wikipedia.org/wiki/Evolutionary_algorithm). In a pure evolutionary algorithm, the candidate solutions will be breed together to create new offspring (crossover and mutation operations) with the goal of generating better solutions for the next generation.\n\nFrom the point of view of software, this is a bit more complicated, but the reproduction could be done by implementing features that another framework lacks because that feature has proved to be a successful solution (crossover operation). On the other hand, new features are developed to distinguish it from its competitors (mutation operation). While it is not 100% the same, I think that we can see the parallelism between both approaches, as all the projects at some point implement similar solutions in other to compete and survive while adding new features that make them unique.\n\nTherefore, all the features of every framework can be treated, from this point of view, as **traits or genetic features of the frameworks**. In evolution theory we know that: [\"without genetic variation a population cannot evolve in response to changing environmental variables and, as a result, may face an increased risk of extinction\"](http://evolution.berkeley.edu/evosite/relevance/IIIA2Lowvariation.shtml).\n\nMoreover, in a recent study it is shown that [loss of genetic diversity threatens species diversity](http://www.enn.com/wildlife/article/23391). According to Dr. Richard Lankau: *\"Diversity within a species is necessary to maintain diversity among species, and at the same time, diversity among species is necessary to maintain diversity within a species. And if any one type is removed from the system, the cycle can break down, and the community becomes dominated by a single species.\"*\n\nIf we add to the equation that **genetic diversity plays an important role in the survival of the species** (see this [paper](http://www.sciencedirect.com/science/article/pii/S0006320705002089) from Dr. Richard Frankman), then **we can assume that having different similar frameworks that try to solve a problem is not bad, it is good, really good**.\n\nIn summary, **to me**, *reinventing the wheel can be treated as a technique that will assure enough genetic diversity* (different features) for frameworks, so the species can survive. What do you think?\n\nNOTE: The photo of this entry has been modified to remove a banner on right bottom corner.\n",
    "basename": "2014-02-26-reinventing-the-wheel"
  },
  "2014-07-04-commandline-candy-for-pybossa": {
    "title": "Command line candy for PYBOSSA",
    "template": "entry",
    "slug": "command-line-candy-for-pybossa",
    "icon": "candy",
    "icon_author": "Lost in space",
    "icon_url": "https://www.flickr.com/photos/gentagrafie/12723952215/",
    "tags": "citizen science, open source, PYBOSSA, tools",
    "location": "Madrid, Spain",
    "meta_description": "Command line candy for PYBOSSA",
    "headline": "Managing a PYBOSSA project was never so easy.",
    "layout": "blog",
    "preview": "PYBOSSA is a great framework, however we were lacking a nice command …",
    "content": "\n\nPYBOSSA is a great framework, however we were lacking a nice command line tool to \ninteract with it.\n\nFor this reason, after providing several different scripts to create projects, add \ntasks, etc, etc. I've decided to finally create a **simple** command line tool to \nmaster the PYBOSSA API.\n\n<!--more-->\n\n## pbs - mastering PYBOSSA API\n\nAs I wanted to create a powerful but simple tool to manage PYBOSSA projects via the \ncommand line, I looked for different solutions: argparse, docopt, etc. From all the\navailable solutions I found [Click](http://click.pocoo.org/) from \n[Armin Ronacher](http://lucumr.pocoo.org/) and after trying it I was simply AMAZED.\n\n*Click is really powerful, simple and its feature for nesting commands is incredible.*\n\nThanks to Click I've managed to develop a command line tool for PYBOSSA in two days, \nthat behaves more or less like *git* with commands, sub-commands and --help options \nto make your life much simpler!\n\n## Installing pbs\n\nInstalling *pbs* is very simple. Just install it with **pip** with the following command:\n\n{% highlight bash %}\n pip install pybossa-pbs\n{% endhighlight %}\n\nThen all the magic happens <i class=\"twa twa-wink\"></i>.\n\n## Configuring pbs\n\nI've designed pbs to be very flexible, so all the options can be passed as arguments,\ngiving you all the flexibility that you could need.\n\nOne of the key aspects that I love from pbs is the possibility of having a config file \nfor storing my credentials for different PYBOSSA servers. This simplifies my life, \nreduces the ammount of typing and I don't have to check all the time my API-KEY in the \nservers that I'm using :-)\n\nThe config file is very simple. It's just a file named **.pybossa.cfg** that looks something\nlike this:\n\n{% highlight python %}\n [default]\n server: yourserver\n apikey: yourkey\n\n [anotherserver]\n server: youranotherserver\n apikey: youranotherkey\n{% endhighlight%}\n\nBy default, pbs will use the *default* section, but if you want to authenticate against\nanother server, all you've to do is to pass the following command line option: **--credentials anotherserver**.\nDone!\n\nYou don't actually need that file, but if you are working a lot with PYBOSSA I would \nrecommend you to create it. It's really amazing.\n\n## Creating a project\n\nNow that we've pbs configured all we've to do is to create a project. Creating a project\nis as simple as always. All you need is a **project.json** with something like this:\n\n{% highlight JSON%}\n { \n    'name': 'Name of your application',\n    'short_name': 'theslug',\n    'description': 'description'\n }\n{% endhighlight%}\n\nThen, if you run pbs from the same folder where that file has been created, all you've\nto do for creating a project is running the following command:\n\n{% highlight bash %}\npbs create_project\n{% endhighlight%}\n\nTwo words, and your project is created!\n\n## Adding tasks\n\nNow that we've our project available in the server, we can add tasks to it. With pbs\nI wanted to allow users to import tasks from PYBOSSA servers without having to do \nnothing special. If you visit a project in [Crowdcrafting](http://crowdcrafting.org)\nyou will see that right now PYBOSSA allows you to download tasks as files in two \ndifferent formats: CSV and JSON.\n\nOnce you've downloaded one set of tasks from Crowdcrafting, pbs allows you to **re-use** \nthe data, as all the projects are using an open-data license. Cool, right? \n\nHow do you re-use the tasks's file? If you've downloaded the tasks in CSV format, all\nyou have to do is running the following command:\n\n{% highlight bash %}\npbs add_tasks --tasks-file file --tasks-type=csv\n{% endhighlight%}\n\nDone! **You've even a progress bar and if you have more than 300 tasks, pbs will auto\nenable the throttling to respect the PYBOSSA limits of the server.** I love this ;-)\n\n## Adding the task presenter, long description and tutorial\n\nNow, all we've to do is to add the tutorial, task presenter and long description. \n\nFor adding those files to the project, you can have those files created in a folder, \nwith the following names:\n\n * **template.html**\n * **lon_description.md**\n * **tutorial.html**\n\nIf those file names exist where you are running the command, then you don't have to\ntype almost anything, just this command:\n\n{% highlight bash %}\npbs update_project\n{% endhighlight%}\n\nDone! Quick, fast and simple. If you are testing something new, or if you want to \nreuse a template from another project, all you've to do is tell it to pbs:\n\n{% highlight bash %}\npbs update_project --task-template /path/to/template.html\n{% endhighlight%}\n\n\n## An example\n\nIn the following video you can see how quickly you can create a project in \nCrowdcrafting (or any PYBOSSA server) using pbs with the [Flickr Person Finder template](https://github.com/PYBOSSA/app-flickrperson/).\n\nEnjoy!\n\n### Installing pbs\n\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src='http://player.vimeo.com/video/99921525' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n</div>\n\n### Creating a project and adding tasks\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src='http://player.vimeo.com/video/99921526' frameborder='0' webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></div>\n\nPS: I almost forgot to mention it: everything is open source and you can check the \nsource code [here](https://github.com/PYBOSSA/pbs/).\n",
    "basename": "2014-07-04-commandline-candy-for-pybossa"
  },
  "2014-09-22-shuttleworth-fellow": {
    "title": "Year in review as a Shuttleworth Fellow",
    "template": "entry",
    "slug": "shuttleworth-fellow",
    "icon": "open",
    "icon_author": "Open Source",
    "icon_url": "https://www.flickr.com/photos/opensourceway/7496803264",
    "tags": "citizen science, open source, PYBOSSA, tools, shuttleworth",
    "location": "Madrid, Spain",
    "meta_description": "Shuttleworth Fellow",
    "headline": "Being a Shuttleworth Fellow has changed my life forever.",
    "layout": "blog",
    "preview": "One year ago I started a …",
    "content": "\n\nOne year ago I started a [fellowship](http://shuttleworthfoundation.org) that\nchanged my life.\n\nOne year later, the [Shuttleworth\nFoundation](http://shuttleworthfoundation.org) has renewed my fellowship for\nanother year. Amazing!\n\n<!--more-->\n\n## Becoming a fellow\n\nOne of the most interesting aspects of pitching to Shuttleworth Foundation is\nthe [video pitch](https://shuttleworthfoundation.org/applications/). Last year, \nI wrote a [blog post about it](/blog/2013/09/23/video-tutorial.html) explaining\nhow I shot that video.\n\n*The video is a very interesting exercise*, as you are forced to express your\nideas in a very succinct way, as you only have 5 minutes, and you have to use\nthem wisely. \n\nMy first video was done in two days, and I loved the result, however I needed\nsomething better for my second year. \n\nI wanted to show *how much I love my work*,\nwho am I, and what do I do thanks to the foundation's support. The result?\nWell, judge it yourself (please leave me a comment about the video): \n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src='http://www.youtube.com/embed/rQ3yLqdEhvc' frameborder='0' allowfullscreen></iframe></div>\n\n*I'll write another blog post about the video and its creation. I promise.*\n\nThe foundation liked it, and I have another year to do many more things thanks\nto their support. \n\n## Year in review\n\nThe first fellowship year helped me to have a [team](http://pybossa.com/about/)\nof awesome people. Thanks to the their support, I've managed to\nhire a UX person (to me he's the best in the world), two developers (that work\nreally hard and love what they do), and a young communications person (who writes\nawesome blog posts about our work) -don't be shy and check our [team\npage](http://pybossa.com/about/).\n\nNow, with a team behind [PYBOSSA](http://pybossa.com) and\n[Crowdcrafting](http://Crowdcrafting.org), I could move quickly, learn from the\nteam, attend more meatings, develope new features and make Crowdcrafting a place \nto hang out for citizen scientists.\n\n## Growing step by step\n\nWhile we were working, we got a great opportunity: the [British\nMuseum](http://www.britishmuseum.org/) and the [UCL](http://www.ucl.ac.uk/)\nwere interested in [PYBOSSA](http://pybossa.com) and more importantly: *they\nwanted to use it for their [own citizen science project](http://micropasts.org/).*\n\n![Happy animated gif](http://i.giphy.com/13k4VSc3ngLPUY.gif){: .img-responsive}\n\nIn April of 2014, the project was launched with great [coverage](http://www.ucl.ac.uk/news/news-articles/0414/160414-crowdsourcing-bronze-age) \nfrom the\n[press](http://www.theguardian.com/science/2014/aug/18/volunteers-british-museum-crowdsourcing-archeology).\nAt the time of this writing, the project has managed to have more than 1\nthousand contributors with 18 published projects. Awesome!\n\nWe worked with them closely to build two templates that have being used to build\nthe 18 projects:\n\n * a transcription template to make available a huge card catalogue of British\n     prehistoric metal artefacts discovered in the 19th and 20th century, and\n * a template that enables the creation of a high quality 3D model of an\n     archaeological artefact via process known as photo-masking.\n\nI'm really proud of the work done for this project and its successthe work done\nfor this. This achievement has proved that PYBOSSA is mature to be used as a\ntool for doing citizen science, and more importantly that international\ninstitutions trust our software, tools and methodologies.\n\n## Exciting times ahead\n\nI collaborate with the [Medialab-Prado](http://medialab-prado.es) institution\nin Madrid, Spain, and I coordinate there the citizen science workstation. As\npart of my collaboration, we organize international workshops where anyone can\npitch their project. If the project is interesting, it gets accepted, and a\ngroup of collaborators join you to collaborate.\n\nIn one of these\n[calls](http://medialab-prado.es/article/madridlaboratoriourbano), a research\ngroup from the [Complutense University](http://guaix.fis.ucm.es/node/1651)\napplied to create a citizen science project to analyze the light pollution of\ncities. The interesting part: they wanted to use photographs taken directly\nfrom the International Space Station by astronauts!\n\nCool, right? And best of all: *they wanted to use Crowdcrafting for developing\nthe project!!!*\n\n![Oh My God GIF](http://i.giphy.com/QMcamps7Gzj2g.gif){: .img-responsive}\n\nThe project was accepted and left beta in July. In this month the research\ngroup sent out a [press release about the\nproject](https://www.ucm.es/data/cont/media/www/pag-56948/Atlas%20de%20im%C3%A1genes%20nocturnas%20DEFbuena.pdf).\nThe press release was sent to [NASA](http://www.nasa.gov/) and\n[ESA](http://www.esa.int/ESA) and they supported the project with\ntweets like this one from ESA:\n\n<blockquote class=\"twitter-tweet\" lang=\"es\" align=\"center\"><p><a href=\"https://twitter.com/hashtag/Citizenscience?src=hash\">#Citizenscience</a> at work RT <a href=\"https://twitter.com/teleyinex\">@teleyinex</a>: <a href=\"https://twitter.com/esa\">@esa</a> thanks to your help on Twitter <a href=\"https://twitter.com/cities4tnight\">@cities4tnight</a> has 3000 tasks classified in <a href=\"https://twitter.com/crowdcrafting\">@crowdcrafting</a></p>&mdash; ESA (@esa) <a href=\"https://twitter.com/esa/status/487228335018475521\">julio 10, 2014</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\nThen, the unexpected happened. NASA wrote a [full\narticle](http://www.nasa.gov/mission_pages/station/research/news/crowdsourcing_night_images/#.U-zmA_ldWSo)\nabout the project and obviously tweeted it:\n\n<blockquote class=\"twitter-tweet\" lang=\"es\" align=\"center\"><p>Space station sharper images of Earth at night crowdsourced for science: <a href=\"http://t.co/bHBiLwvZSv\">http://t.co/bHBiLwvZSv</a>   <a href=\"https://twitter.com/hashtag/ISS?src=hash\">#ISS</a> <a href=\"http://t.co/bL9LymQ6cq\">pic.twitter.com/bL9LymQ6cq</a></p>&mdash; NASA (@NASA) <a href=\"https://twitter.com/NASA/status/499963958552711168\">agosto 14, 2014</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n\nThe result? Well lots of international media mentioned the project like\n[Popsci](http://www.popsci.com/blog-network/do-try-home/join-crowd),\n[Co.Exists](http://www.fastcoexist.com/3033228/these-incredible-photos-from-astronauts-show-the-brightest-cities-on-earth),\n[NBC\nNews](http://www.nbcnews.com/science/space/scientists-want-you-help-crowdsource-night-lights-n181116),\n[CNN](http://edition.cnn.com/2014/08/17/tech/nasa-earth-images-help-needed/index.html?hpt=hp_t2),\n[Gizmodo](http://gizmodo.com/), [Smithsonian\nMagazine](http://www.smithsonianmag.com/smart-news/help-nasa-out-looking-beautiful-pictures-space-180952407/?no-ist),\n[etc](http://pybossa.com/press/).\n\nAmazing right? Well, this was not yet the best part. Trust me. The 21 of\nAugust, FOX News TV showed on prime time\n[Crowdcrafting](http://crowdcrafting.org) and how to contribute to the\nproject:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src='https://video.foxnews.com/v/video-embed.html?video_id=3742323090001' style='border:0'></iframe></div>\n\nThanks to this *amazing coverage* **Crowdcrafting stored in one single day more than one\nanswer per second**, with thousands of new volunteers registering in the site\nand thousands of tasks completed in hours! (check the\n[statistics](http://crowdcrafting.org/app/darkskies/stats)).\n\n![Despicable Me Minion OMG](http://i.giphy.com/CHyxN9bNkMc3S.gif){: .img-responsive}\n\nSince then, we have had new users registering every day, tasks completed every\nday, and lots of contributions from volunteers. Amazing!\n\nAnd this has happened only in the first year of my fellowship, so what will\nbring my second year? Really looking into it!!!!\n",
    "basename": "2014-09-22-shuttleworth-fellow"
  },
  "2014-10-31-makers": {
    "title": "Makers vs Craftsmen",
    "template": "entry",
    "slug": "makers-vs-craftsmen",
    "icon": "carpenter",
    "icon_author": "Richard",
    "icon_url": "https://www.flickr.com/photos/rich701/8110384009",
    "tags": "fabrication, science, Crowdcrafting, PYBOSSA, makers",
    "location": "León, Spain",
    "meta_description": "Makers on the go",
    "headline": "Engineers lack the inventiveness to repair a piece or find an alternative for it, they don't wonder why it broke.",
    "layout": "blog",
    "preview": "Makers are getting a lot of press coverage lately. However, I …",
    "content": "\n\nMakers are getting a lot of press coverage lately. However, I haven't read any article \nabout the craftsmen: people who also build things with their hands and tools.\n\nWhy these articles do not write about carpenters, sculptors, turners,\netc.? Is it because they don't consider them makers?\n\n<!--more-->\n\nThe last weekend of September I was invited to give a talk about\n[Crowdcrafting](http://crowdcrafting.org) at the [Mini Maker Fair León](http://makerfaireleon.com/makers-en-leon/). The\nevent was mostly about new ways of thinking, with a big attention to fabrication and production systems in the\nmaker community.\n\nBetween sessions I listened carefully to the maker talks. In general they\nwere about people using 3D printers (I saw again a sculpture of Yoda), laser cutters,\nand new tools that allow you to build stuff \"very easily\". However,  I felt\nthat something was missing and I was puzzled. What was I looking for? \n\n## Makers, or should we say Craftsmen?\n\nWhile I was approaching the booths, I realized that:\n**makers are craftsmen using new tools, nothing else**. However, the fair was\nonly filled with the new so called *makers* and none of the old *craftsmen*.\n\n![Why](http://i.giphy.com/skXEIUJKHLrsk.gif){: .img-responsive}\n\nI think the maker community is trying to find the path for a brighter future,\nbut in my humble opinion, they are not asking to the right persons. They hang \ntogether, but they don't talk to the people that have been doing this, building stuff, \nfor centuries: carpenters, turners, sculptors, mechanics, etc. (the photo of this blog\nentry is from 1913).\n\nThese guilds produce prototypes, items, products, etc. using their hands and\ntools. If we compare them with the makers, the only difference is the tools\nthey are using. *However, I have never seen them invited to participate in these\nevents.*\n\nMoreover, there are lots of retired people that still have a passion for making\nstuff with their own hands and tools and they don't know anything either about \nthis new maker movement. Let me give you an example. *José Manuel Hermo Barreiro\nis a 72 years old man with a passion: building engines from scratch*.\n\nThanks to his passion, José has built the smallest V12 engine in the world\nusing only his hands and tools in his garage (does it ring a bell with you?). \n\nJosé starts drawing the blueprints, then building the metal pieces -one by one- accounting all \nthe hours that takes him to create one of these marvelous engines.\n\nIn the following video he explains this process, and best of all: you can see\nand feel his passion in every word. \n\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src='http://www.youtube.com/embed/c1pJIVqCC1E' frameborder='0' allowfullscreen></iframe></div>\n\nHe is amazing! He has become my hero! **I would love to meet him, talk with him just to learn from\nhis experience**. \n\nJosé (who started mechanics when he was 16 years old) is a master and \n**it is a pity that his knowledge is getting lost**. \n\nIn the video, he says: **\"engineers lack the inventiveness to repair a piece or\nfind an alternative for it, [...] they don't wonder why it broke\".** It touched my \nheart, because sadly I endorse his words. \n\nTo me José is a maker, but I don't think he would describe himself as a\n*maker*. **José is an artisan, a craftsman that knows how to build\namazing engines in his garage**. In other words, he shares the same passion and\npurpose as thousands of makers with only one tiny difference: they use\ndifferent tools, that's all.\n\nLike José there are hundreds of retired persons that to me are the voice of \nthe experience. These people really know how to\nsolve problems and they say that *new generations do not have the\ninventiveness to solve them*, so wouldn't be nice to invite them to next Maker\nfair?\n\n**I think it is crucial for the maker and 3D printing communities to connect with the old generations that keep building stuff in their garages.**\n\nListening to people like José will open the door to pure knowledge.\nNew generations will learn how they solve problems in the past, which tools\nwere used and more importantly see their passion and gentleness for their work.\n\nMoreover, old generations will discover new tools for their developments and \ntogether they will improve the production chains that will lead to better\nprototypes.\n\nFor these reasons I want to make a proposition to the Mini Maker Fair Leon\norganizers (well, this is actually a proposition to anyone that organizes Maker\nfairs, events, workshops, etc.):\n\n**I would love to co-organize a new fair where retired craftsmen and makers\ntalk to each other sharing their knowledge**. I would love to see José giving a talk \nabout how he builds his engines, participate in a workshop where he shows how\nhe produces them so the makers can adapt those methodologies to their 3D\nprinters and CNC machines.\n\nI would love to see how retired people take a more active role in teaching\nwhat they have learned in their life, so us, you and me, can learn and become\nbetter professionals in what we do. It's a crime losing these knowledge and us,\nthe society, should include them as they have a lot to teach.\n",
    "basename": "2014-10-31-makers"
  },
  "2014-11-26-video-pitch": {
    "title": "Video pitching one day of my life",
    "template": "entry",
    "slug": "video-pitching-my-life",
    "icon": "8mm",
    "icon_author": "Kevin Stanchfield",
    "icon_url": "https://www.flickr.com/photos/sgt_spanky/3970722137",
    "tags": "video, science, Crowdcrafting, PYBOSSA, application",
    "location": "Madrid, Spain",
    "meta_description": "Video Pitching",
    "headline": "Filming one day of my life to pitch why it matters to support me.",
    "layout": "blog",
    "preview": "Video pitching is an art. Why? Because you want to be remembered, …",
    "content": "\n\nVideo pitching is an art. Why? Because you want to be remembered, not just\nthrown away into the pool of *boring-nothing-new* videos basket.\n\n<!--more-->\n\nAs [I've told you\nrecently](http://daniellombrana.es/blog/2014/09/22/shuttleworth-fellow.html),\nI've been offered a second year fellowship at the \nShuttleworth Foundation, but in order to get a second year you've to pitch again\nyour project, and of course it has to have a video.\n\nWhen I got my first year, [I blogged about it](http://daniellombrana.es/blog/2013/09/23/video-tutorial.html). \nShooting was a lot of fun, but also very painful due to the software I used to \nedit the video (OpenShot if you were wondering which one).\n\nWhile recording a second video this year should be easier because the foundation \nalready knows me, **I wanted to shoot a video that explained\nwho am I, and more importantly, why it matters to support me**. For these\nreasons, *I decided to record a day of my life and show it to them*.\n\n## Step 0: The script\n\nIf you read my [previous blog post](http://daniellombrana.es/blog/2013/09/23/video-tutorial.html) \nabout how to shoot a video, you know that you've to ask yourself the 4 Ps before writing your script:\n\n * **People: Who is in the story?**\n * **Place: Where does the story take place?**\n * **Plot: What is the conflict and the journey?**\n * **Purpose: Why should anyone care about this?**\n\nThe answer to these questions was *more or less easy*:\n\n * **People**: my team in Madrid (flying to Oxford and Hannover would be too\n     much for my budget, hehe) and myself.\n * **Place**: the places I hang out: Medialab-Prado, Jorge's office, my\n     neighborhood and obviously my home.\n * **Plot**: One day of my life.\n * **Purpose**: To explain why is important to support me to implement my idea\n     for social change.\n\nWith these ideas in my mind, I expend a few days crafting the whole script,\nwriting it down, and discussing with my friends and family where I should shoot\nthe video.\n\n## Step 1: Video gear\n\nThis year I used the same camera as in the last one, my beloved Canon 550D. \nI borrowed a 35mm fixed lens to have a nice touch in the video (just because I\nlove that lens, hehe).\n\nAs I was going to film myself walking, I needed a way to shoot the video in a\nsteady mode. As I didn't want to expend lots of money in buying a professional rig, I\ndecided to create my own following some [tutorials on the web](https://www.youtube.com/watch?v=RNCwPDXODMs). \n\nNext photo shows the front side of the rig. The white plate is where you attach\nyour camera. You can also see the two handles to do some panning shots:\n\n![shoulder rig](/assets/img/blog/IMG_20141126_112836.jpg){: .img-responsive}\n\nThe best part of this design is that I can add weight to the back, so it gives\nyou a nice balance for the camera while you walk with it, or when you do a\npanning shots. The next photo shows how I've added to the back some weight to\ngive me a more steady movement of the camera:\n\n![shoulder rig](/assets/img/blog/IMG_20141126_112844.jpg){: .img-responsive}\n\nOnce I built my own rig (lots of fun!), I needed to work in the microphone of the camera.\nThe problem was that I was going to be shooting in the streets, so lots of\nnoise will get through ruining the sound due a feature known as Automatic Gain\nControl (AGC). \n\nMy camera has a microphone input, but it has an issue: you cannot disable the\nAGC. This feature continuously adjusts the audio levels so\nthat loud sounds won’t overload and distort, and soft sounds won’t go unheard.\n\nWhile this sounds fantastic, it actually ruins your video recordings, as you\nwill get all the ambient noise in your movie, and what you really want is to listen \nto the speakers. Hence I needed a way to disable AGC hacking the camera. My\nreaction:\n\n![Yes to all](http://i.giphy.com/aCrRttmzK1jKo.gif){: .img-responsive}\n\nLuckily Internet has the knowledge, and I found several people with the same\nissue, and they shared how you could build a small gadget to disable AGC and\nrecord sound without problems. After following one of the tutorials I built my\nanti-AGC gadget:\n\n![hacking AGC](/assets/img/blog/IMG_20140502_203554.jpg){: .img-responsive}\n![hacking AGC](/assets/img/blog/IMG_20140502_210400.jpg){: .img-responsive}\n\n\nWhile the gadget worked, sometimes failed, and nothing was recorded which was\nterrible. Thus, I decided to look for a more secure solution: borrowing a sound\nrecorder (I guess the main issue was my inexperience soldering the wires).\n\nWhile my last hack didn't work out, I really loved what I learned just for\npreparing to shoot my video. At this moment, I've not shot anything yet, but I've\nwritten the script and built my own gear to start filming.\n\n![dr. evil](http://i.giphy.com/2XfswSLHgkXXa.gif){: .img-responsive}\n\n## Editing the video\n\nLast year I used [OpenShot](http://www.openshot.org/) and I had lots of problems. \nBasically, every time I changed anything, the software crashed. Luckily for me,\neven though it crashed \nthe status was saved, so *I could work with it*. However, as you can imagine,\nthis was very painful as every change involved a crash, restart, wait to load\nall the video and sound clips, check the changes, do a modification and again a\ncrash.\n\n![frustrated](http://i.giphy.com/6xgslyYQCyLa8.gif){: .img-responsive}\n\nTo me this was the most stressful part, as I remembered quite vividly all the\nfrustration of going through that loop with every change, so I decided to try a\ndifferent open source tool for editing the video this time.\n\nThe chosen one: [Blender](http://www.blender.org/). And all I can say: yes, finally, something that\nworks, that never crashes, and that allows me to do whatever I want to do in a\nsimple way. I'll never ever look back!!\n\nThe only downside was to learn a new tool, as it takes time and effort. However,\nthis employed time was priceless as I could modify the video without a crash,\ndo fancy filtering (even color correction), and in the future if I want, many\nmore advance techniques.\n\n![I'm so happy](http://i.giphy.com/YFIn0ICJFwGNa.gif){: .img-responsive}\n\n## Making it personal\n\nWith everything in place, the only missing part was the setup for shooting the\nvideo. Where I could, I tried to control light, objects shown in the frame,\netc, etc. I wanted to tell a story not just with my words but also with the\nitems that are shown in it. The result, judge it yourself:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src='http://www.youtube.com/embed/rQ3yLqdEhvc' frameborder='0' allowfullscreen></iframe></div>\n",
    "basename": "2014-11-26-video-pitch"
  },
  "2015-02-06-teams": {
    "title": "3 steps to build a successful team",
    "template": "entry",
    "slug": "3-steps-to-build-a-successful-team",
    "icon": "team",
    "icon_author": "Yasin Hassan",
    "icon_url": "https://www.flickr.com/photos/yasinhasan/4431896656/",
    "tags": "team, Crowdcrafting, PYBOSSA,",
    "location": "Madrid, Spain",
    "meta_description": "Give the ones you love wings to fly, roots to come back and reasons to stay <br/> Dalai Lama XIV",
    "headline": "Give the ones you love wings to fly, roots to come back and reasons to stay <br/> Dalai Lama XIV",
    "layout": "blog",
    "preview": "How do you build a successful team? Moreover, how do you do it when …",
    "content": "\n\nHow do you build a successful team? Moreover, how do you do it when you don't\nhave any idea about recruiting? This is my story about how I've built the\namazing team behind [Crowdcrafting](http://Crowdcrafting.org) and\n[PYBOSSA](http://pybossa.com).\n\n<!--more-->\n\n## Let others help you\n\nOne of the best parts of being a [Shuttleworth\nFellow](http://shuttleworthfoundation.org/) is that you can \nbuild a team that will help you to achieve your goals.\n\nWhile this sounds exciting, at the same time is *terrifying*. Why? Because\nusually, for the very first time you'll be opening the door of your home to \nstrangers.\n\nWhile I confronted this feeling I decided the following: **if I want to build\nthe most amazing and successful team on earth, I've to trust them since the very\nbeginning; otherwise I'm doomed**. \n\nWith this very basic principle, I've created the following list of \"rules\" that\nI've followed to build an amazing team.\n\n## Give them Wings to fly\n\nAs Dalai Lama said, since the very beginning I knew one thing when I started\nrecruiting people: **I don't want to ruin their life, I want them to grow and\nimprove with me**.\n\nFor this reason I encourage them to learn while they work. Learning should be one \nof the main motivations to work with the team. Why? Because if they become\nbetter in what they do, everyone wins. As simple as that, plus I know that if\nthey quit, or our project fails, they'll have lots of expertise and skills that\nwill help them in the future.\n\nHow do I encourage them to fly? Well they've free will to decide about how they work. Any\nteam member can decide if this\nweek instead of coding, designing or writing a blog post, they prefer to test a\nfancy new methodology mentioned in Hacker News. **That's not lost time, it's\nan investment in learning that will pay you back**. \n\n## Give them Roots to come back\n\nIn every interview I told them that since day one they will have access to\nall the services that we have. Moreover, they'll have since minute 0 deployment\nrights to do releases, even though they are starting to work with me (yes, they\ncan break everything, but I'm fine with it).\n\nWhy am I doing this? Because I want to show to them that **I really trust\nthem**. If they trust you, they can do incredible things!\n\n![Trust](/assets/img/blog/trust.gif){: .img-responsive}\n<p class=\"post-caption\"><a href=\"http://imgur.com/gallery/hcc0iD3\"> Source</a></p>\n\nOne thing is saying it, and another one is proving it.\n\nIf they're going to be part of your team, they should have access to everything\nthat matters to them. And **trust and confidence in your work is a good reason to\ncome back**.\n\n## Give them Reasons to stay\n\nAll my team manage their own time, they decide which days they want to work,\nhow do they handle their holidays, etc. They even take days off just for\ndisconnecting and have time with their loved ones. Oh, you don't have to make\nup fancy stories to get any of those days, as I said: I trust them.\n\nAnother good reason to stay is that I encourage them to say what they really\nthink. They must know that I respect their point of view, and that I'm not\nalways right, far from that. I commit errors, but that's fine. An error is a\nstep forward for learning and improving. And showing that you commit errors, it\nwill help them to not be afraid of failing.\n\nThus, **I tell them that I don't want an echo chamber**. I need to know when \nI'm wrong so I can fix it. If they are afraid of discussing\nwith me, we're done. **Not listening to your team is one of the worst things you\ncan do**. Again, trust them!\n\n*In the last two years I've been trying to create a place where I would love to\nwork.*\nThis place is full of people like me that love what they do, that passion is what drives them, and\nthey are always trying to improve. *I've always imagined that perfect place to\nwork, and now -thanks to the Shuttleworth Foundation- I'm making it real.*\n",
    "basename": "2015-02-06-teams"
  },
  "2015-02-10-infrastructure": {
    "title": "Crowdcrafting stack",
    "template": "entry",
    "slug": "crowdcrafting-stack",
    "icon": "engine",
    "icon_author": "Sina",
    "icon_url": "https://www.flickr.com/photos/limerick6/14038228238",
    "tags": "Crowdcrafting, architecture, infrastructure",
    "location": "Madrid, Spain",
    "meta_description": null,
    "headline": "Whenever someone creates something with all of their heart, then that creation is given a soul",
    "layout": "blog",
    "preview": "Putting all your heart in what you do makes the difference. Why? …",
    "content": "\n\nPutting all your heart in what you do makes the difference. Why? Because as the\nBaron says in the movie, [The Cat\nReturns](http://en.wikipedia.org/wiki/The_Cat_Returns), the creation is given a\nsoul.\n\n<!--more-->\n\nOn December 4 of 2014, we got the very wonderful news that\n[Crowdcrafting](http://crowdcrafting.org) was recognized as [one of the social\ntechnological companies of the year](http://socialtech.org.uk/nominet-trust-100/).\n\nWining this price has been amazing, a recognition to our really hard to make our \nCrowdcrafting site robust, scalable and stable.\n\n**TL;DR** as this is going to describe our current infrastructure and how we run\nCrowdcrafting, so you are advised!\n\n## HTTP Load Balancer\n\nWe host all our services in Rackspace. The reason? Well, they've a handy\ncalculator that allows us to estimate how much is going to cost us running our\nservices there, and I love it. Basically, because they don't lie, and the\nnumbers fit.\n\nOne of the nice features that Rackspace offers, is the option to enable an\nHTTTP load balancer for your cloud servers. This simplifies a lot our set up,\nand we've configured it to balance the incoming requests to our PYBOSSA\nservers.\n\nTherefore, when a user requests a page from Crowdcrafting, the first service\nthat it's contacted is the load balancer. The balancer will distribute the\nrequests to our PYBOSSA servers.\n\n## Nginx & uWSGI\n\nOnce the request has been redirected to one of the PYBOSSA servers, the request\nhits the [Nginx](http://nginx.org) server. Nginx, checks its sites enabled, and directs the request\nto our PYBOSSA [Flask](http://flask.pocoo.org) application written in Python. At this point, the server\nis contacted and served via the [uWSGI](https://uwsgi-docs.readthedocs.org) middleware, that will take care of moving\nthe request through our infrastructure.\n\nHence, [Nginx](http://nginx.org) takes care of serving static files, while [uWSGI](https://uwsgi-docs.readthedocs.org) takes care of the\nrest.\n\nIn the very beginning of Crowdcrafting we used Apache2 and mod_wsgi, but we\nchanged to [Nginx](http://nginx.org) and [uWSGI](https://uwsgi-docs.readthedocs.org) because:\n\n - [Nginx](http://nginx.org) is really simple to configure.\n - [uWSGI](https://uwsgi-docs.readthedocs.org) gives the best performance.\n\nWhen we were running on Apache2 and mod_wsgi the\nperformance was suboptimal (we tested it with Locust.io) and we could see a\nclear detriment in the number of requests per second that we were delivering in\ncomparison with the current setup. For this reason, we looked for new solutions\nand we found that the best match for us is [Nginx](http://nginx.org) + [uWSGI](https://uwsgi-docs.readthedocs.org).\n\nWhile we've been developing PYBOSSA we've always keep in mind that PYBOSSA\nshould be able to **scale horizontally without problems**. While this seems *easy\nto achieve* the truth is that there are so many options out there that at the\nend it becomes a nightmare to decide which one is the best solution.\n\nFor example, serving avatars from N different servers should always return the\nsame image from all of them to the clients.\n\n*In our case we decided to keep things as simple as possible*, (the KISS principle).\n\nFor this reason, we've enabled the Rackspace CDN support in PYBOSSA (it can be\neasily extended to any other CDN as we've a generic class that can be\ninherited) for serving files from a central place. This solution allows us to\ngrow horizontally without taking care of where the files are being served.\n\nAdditionally, if someone does not want to enable the CDN, they can configure\nPYBOSSA to use the local uploader and use a Glusterfs to distribute the files\nacross all the servers. We didn't like this solution as it added another point for \nfailure in our systems and we've to take care of it ourselves, while the CDN does \nthis for us *automagically*.\n\nOnce the request is in the [uWSGI](https://uwsgi-docs.readthedocs.org) middleware, the PYBOSSA server will probably\nneed access data in the database, so it can render the HTML and return the\nresponse back to the client. The next section explains how we handle this part\nof the request.\n\n## PostgreSQL & PgBouncer\n\nOnce the request hits the [Flask](http://flask.pocoo.org) app, usually it will involve a query to the\ndatabase. We use [PostgreSQL](http://www.postgresql.org) 9.3 and we're really impressed by the quality,\nperformance and community around it. We LOVE IT! Best DB ever.\n\n![](http://i.giphy.com/WxxsVAJLSBsFa.gif){: .img-responsive}\n\nAs we will have lots of connections coming from different servers, we wanted to\nimprove how we handle those connections to the DB to reduce overhead and timing\nestablishing connections, closing, etc. For this issue we're using in each\nPYBOSSA server [PgBouncer](https://wiki.postgresql.org/wiki/PgBouncer) for pooling the connections to two [PostgreSQL](http://www.postgresql.org) servers: \n\n - Master node accepting read and write queries, and\n - Slave node accepting only read queries.\n\nPYBOSSA establishes two different connections to the databases in order to use\nread-only connections when we want to grab just information, or write\nconnections when we've to write something back to the DB. \n\nWhile [PgBouncer](https://wiki.postgresql.org/wiki/PgBouncer) pools connections, it does not load balance them, so for this\nreason we use [HAProxy](http://www.haproxy.org) to load balance the READ queries between the master and\nslave nodes transparently. The best part of this configuration is that\neverything is completely handled automagically and transparently by [HAProxy](http://www.haproxy.org), so\nPYBOSSA does not know anything about it.\n\nThanks to this set up we can add more slave nodes horizontally, scaling and load\nbalancing our infrastructure easily.\n\nWhile this solution is great, some queries need to be cached before hitting the\ndatabase as they take time to be processed (i.e. statistics for Crowdcrafting\nprojects). For this reason we're using [Redis](http://redis.io) and [Sentinel](http://redis.io/topics/sentinel) to cache almost \neverything.\n\n## Redis & Sentinel\n\nIf we're in love with [PostgreSQL](http://www.postgresql.org) what can we say about [Redis](http://redis.io) and [Sentinel](http://redis.io/topics/sentinel): we\nlove them too :-)\n\n![](http://i.giphy.com/f31DK1KpGsyMU.gif){: .img-responsive}\n\nSince the very beginning PYBOSSA has been using [Redis](http://redis.io) and [Sentinel](http://redis.io/topics/sentinel) to build a\nload-balance high-available cache solution. \n\nThere set up is pretty simple: one [Redis](http://redis.io) master node that accepts read and write \nqueries, while almost every other node in our infrastructure has a slave node.\n\nAdditionally [Sentinel](http://redis.io/topics/sentinel) takes care of handling all these nodes for us\ntransparently, and we don't have to do anything ourselves. This solution has\nbeen working great for us, and thanks to it we're saving lots of queries from\nthe DB, improving our performance.\n\nMore over, we are using [Redis](http://redis.io) also for background jobs (i.e. exporting results,\ncomputing statistics, sending emails, etc.) thanks to\n[Python-RQ](http://python-rq.org/) and [rq-scheduler](https://github.com/ui/rq-scheduler)\nto run periodic jobs. \n\nWe checked Celery but it was overkilling for what we are\nbuilding and we decided again to keep things simple. \n\n[Python-RQ](http://python-rq.org/) and\n[rq-scheduler](https://github.com/ui/rq-scheduler) are small libraries that can be easily adapted to our \nneeds, plus we already have in our systems [Redis](http://redis.io) so it was the best candidate\nfor us.\n\n### Summary\n\nIn summary, we're using micro frameworks to build our project paired with \na very simple infrastructure that allows us to grow \nhorizontally without problems and load balance our incoming traffic efficiently.\n\nThe next picture shows how a request goes through our current setup:\n\n![Infrastructure diagram](/assets/img/blog/infrastructurediagram.png){: .img-responsive}\n\n\n**UPDATE**: Some people have asked about our numbers. The truth is that\nthe current setup can serve up to 2.5k rpm in less than 200ms for 1500\nusers browsing the site at the same time (we've 2 PYBOSSA servers with 2GB of RAM\nand 2 cores each, while the DBs have 4GB of RAM and 4 cores -master and slave). \n\nIn August 2014 we managed to store in our servers more than 1.5 datum per second \none day. At that moment the DB servers have only 1GB of RAM, \nand taking into account that the OS takes around 200MB of it, \nthe DBs were using only 800MB of RAM.\n\n## Deployments & Ansible\n\nUp to now we've been managing all our infrastructure by hand. However, in the\nlast weeks we've been migrating our infrastructure to be completely\ncontrolled via [Ansible](http://www.ansible.com). \n\nAdditionally, we've developed our own in-house solution\nfor automatic deployments for all the team integrated with Github Deployments\nAPI and Slack to get notifications in our own team chat channels. Doing a\ndeployment right now consist in merging and closing a pull request. As simple\nas that.\n\n![](http://i.giphy.com/AMqCTHuCMFpM4.gif){: .img-responsive}\n\nUsing [Ansible](http://www.ansible.com) for everything has helped us to have similar playbooks reused\nacross different clients, allowing us to do faster deployments that are easy to\nmaintain, debug and deploy.\n\nOn the other hand the automatic deployments solution uses the same playbooks,\nso everything runs on the same tools and technologies.\n\nWe checked different solutions like HUBot, but we decided again to have a very\nsimple solution to integrate all these tools in our toolchain. The deployments\nserver has less than 300 lines of code, is 100% fully tested and covered, so\nit's really simple to adapt it and fix it. Moreover, it runs in the same\nservices that we are currently using: [Nginx](http://nginx.org) + [uWSGI](https://uwsgi-docs.readthedocs.org), so we don't have to add\nanything different to our stack.\n\n*NOTE*: I'll write a blog post about the [deployments](http://daniellombrana.es/blog/2015/02/25/autodeployments.html) solution :-)\n**EDIT**: You can read about the deployment solution\n[here](http://daniellombrana.es/blog/2015/02/25/autodeployments.html).\n\n## Continuous integration and code quality\n\nWe take really seriously code quality and tests. Right now we've almost 1000\ntests (953 at the time of the writing) covering almost all the source code (97%\ncovered) and with a health quality of 94%.\n\nOur deployments solution uses the Travis-CI Github Statuses API to do the\ndeployments, so we can know for sure that it will work in our production\nsystems.\n\nWe follow the Github Flow more or less, as we don't have a versioning schema\nper se for our PYBOSSA software. What we do is that everything that it is in\nmaster is stable, as our main service runs directly from it. For this reason,\nwe take really seriously the quality of our software as a bug or an issue will\nbreak our Crowdcrafting platform.\n\nWe usually do several deployments per week, adding new features, bug fixes,\netc. to PYBOSSA and therefore Crowdcrafting, as all the team has deployment\nrights. This has proven to be an amazing feature, as we deliver really fast\nfollowing the RERO principle: [Release Early Release Often](http://www.catb.org/esr/writings/homesteading/cathedral-bazaar/ar01s04.html).\n\nAnd that's all! I hope you like it. If you have questions, please, use the\ncomments section below and I'll try to answer you. Now:\n\n![](http://i.giphy.com/i2p0AzumArz3i.gif){: .img-responsive}\n\n",
    "basename": "2015-02-10-infrastructure"
  },
  "2015-02-25-autodeployments": {
    "title": "Autodeployments",
    "template": "entry",
    "slug": "autodeployments",
    "icon": "robot",
    "icon_author": "Daniel Lombraña González",
    "icon_url": "https://www.flickr.com/teleyinex",
    "tags": "Crowdcrafting, architecture, infrastructure",
    "location": "Madrid, Spain",
    "meta_description": "Whether we are based on carbon or on silicon makes no fundamental difference; we should each be treated with appropriate respect. Arthur C. Clarke",
    "headline": "Whether we are based on carbon or on silicon makes no fundamental difference; we should each be treated with appropriate respect. Arthur C. Clarke",
    "layout": "blog",
    "preview": "At Crowdcrafting we take really seriously shipping code. For this …",
    "content": "\n\nAt Crowdcrafting we take really seriously shipping code. For this reason, we've\ncreated a very simple web service (it's our own software robot) that automatically \ndeploys for us any Github project with Ansible playbooks and posts the status of \nthe deployment in our Slack chat channel.\n\n<!--more-->\n\n## Why another deployment server?\n\nA fairly good question. We checked different options like HUBot, however using\nthe service meant to add extra layers to our [current stack](http://daniellombrana.es/blog/2015/02/10/infrastructure.html). In the case of HUBot \nwe would have to install Node.js and learn coffee script to write our own\nplugins. IMHO too much work for just doing some deployments, plus we will add a\nstack to our infrastructure that we do not fully know.\n\nFor these reasons we decided to create something very simple that uses the\n[Github API for\ndeployments](https://developer.github.com/v3/repos/deployments/) and integrated\nwith [Ansible](http://www.ansible.com/home) (we use it for managing\nour own infrastructure) as well as with [Slack](http://slack.com) to follow the status of the\ndeployments.\n\nThe server uses the [Flask framework](http://flask.pocoo.org/) and we can host it in our current\ninfrastructure without adding any extra layer.\n\n## Our deployments solution (or our robot)\n\n[The web server](https://github.com/PYBOSSA/deployments) has less than 250 lines of \ncode. It's 100% tested, covered and with a code health quality of 100% according to \nLandscape.io. Oh, it's also open source!\n\nThe server uses a config file to specify which repositories from Github have to\nbe deployed. The structure is quite simple:\n\n```\nDEBUG = False\nSECRET = 'yoursecret-to-protect-your-server'\nTOKEN = 'your-github-token'\nSLACK_WEBHOOK = 'yourslackwebhook'\nREPOS = {\n    'user/repo': {'folder': '/repo',\n                  'required_contexts': [\"continuous-integration/travis-ci\"],\n                  'commands': [['git', 'fetch'],\n                               ['git', 'pull', 'origin', 'master']]}\n}\n```\n\nA very handy feature is that you can specify in the config file if you want to\nonly do a deployment when for example your continuous integration tests are passing. This is\noptional, but you are already testing your software, right?\n\n### Ansible integration\n\nIn the previous example you can add as many commands as you want. However, \nif you are already using Ansible playbooks all you have to do to use them with\nthe server is this:\n\n```\nDEBUG = False\nSECRET = 'yoursecret-to-protect-your-server'\nTOKEN = 'your-github-token'\nSLACK_WEBHOOK = 'yourslackwebhook'\nREPOS = {\n    'user/repo': {'ansible_hosts': 'hosts_file',\n                  'ansible_playbook': 'playbook.yml',\n                  'required_contexts': [\"continuous-integration/travis-ci\"],\n}\n```\nThanks to Ansible you can deploy the same software in different machines,\nsomething very handy when you have project with several nodes running the \nsame stack as we do.\n\n\n### Slack notifications\n\nIn order to get Slack notifications, all you have to do is to add a new\nintegration in your Slack team: [incoming webhooks](https://api.slack.com/incoming-webhooks). This integration will give\nyou a URL that you only have to copy and paste into the config file. Once you\nhave done it the server will post messages about the status of\nthe deployment. The messages are like this:\n\n![Deployment screenshot](/assets/img/blog/deployments.png){: .img-responsive}\n\nThe message includes the following information:\n\n* the repository that has been deployed,\n* the user that has done the deployment,\n* the status of the deployment.\n\nThe status is pretty handy because if something goes badly, you can debug what\nhappened as we store the error messages in the Github API, so you can review\nthem.\n\nBest part: the robot communicates his work!\n\n### Doing deployments\n\nHow do you actually do deployments? Well, we just wanted to make it very simple\nlike clicking a single button.\n\nOur solution? When a branch with fixes or a new feature in Github is merged\ninto the master branch, the service will deploy the changes into production \n(or the machines that you want). As simple as that! The system takes care of\nitself! Batteries included!!\n\n![BMO gif changing its batteries](http://i.giphy.com/AMqCTHuCMFpM4.gif){: .img-responsive}\n\nThanks to this solution now every member of my team can actually do deployments\ninto production. This has been a significant change in our work flow as\neveryone can deploy changes into production ([trust your team](http://daniellombrana.es/blog/2015/02/06/teams.html)), and you don't have to ask a favor to\ndo a deployment. You just simply click a button!\n",
    "basename": "2015-02-25-autodeployments"
  },
  "2015-03-05-uwsgi": {
    "title": "uWSGI, or why you don't need Varnish",
    "template": "entry",
    "slug": "uwsgi",
    "icon": "micro",
    "icon_author": "dorikowalski",
    "icon_url": "https://www.flickr.com/photos/ptr-an/2134521914",
    "tags": "Crowdcrafting, architecture, infrastructure",
    "location": "Madrid, Spain",
    "meta_description": "",
    "headline": "How to achieve a server response time of less than 50ms thanks to uWSGI",
    "layout": "blog",
    "preview": "As a web developer one of my main goals is performance. In this blog …",
    "content": "\n\nAs a web developer one of my main goals is performance. In this blog post I\nexplain how we have boosted the performance of\n[Crowdcrafting](http://crowdcrafting.org) without touching\nthe [PYBOSSA](http://pybossa.com) code or adding any extra layer to our\n[stack.](http://daniellombrana.es/blog/2015/02/10/infrastructure.html) \n\n<!--more-->\n\n## Boosting your web service performance\n\nIf you are developing a web service you know that you have to cache content in\norder to serve lots of requests quickly, right? You might be using a memory\ncache like [memcached](http://memcached.org/) or \n[Redis](http://redis.io/), like we do. However, sometimes this is\nnot enough because the request stills go through all your pipeline only saving\ntime from accessing the DB or computing a difficult value. Moreover, if you\nhave a distributed load-balanced high-available cache (as we do), the request\nwill take some time in retrieving the data from a node. Therefore you will end up\nsumming some precious milliseconds to that request just for fetching a value\nthat has been already computed (I always picture the requests like these\nskaters running to get to finish line).\n\n![Fast GIF](http://i.giphy.com/NUlSiaVrEaIdq.gif){: .img-responsive}\n\nWhen those milliseconds are precious, then you are looking for caching the whole\nrequest, not just some data in the DB.\n\nIf you are looking for a solution to this problem you will probably find \n[Varnish](https://www.varnish-cache.org) a web application accelerator also known \nas a caching HTTP reverse proxy. There is a\nlot of documentation on the web about it, and just to be fair we consider it\nfor some time but we decided to avoid it for a single reason: our\ninfrastructure uses cookies to handle sessions (we use [Flask-Login](https://flask-login.readthedocs.org/en/latest/)) and this\nmakes things [really complicated](https://www.varnish-cache.org/docs/3.0/tutorial/cookies.html). \n\n\n### Looking for alternatives: uWSGI cache capabilities\n\nAs I've explained in previous blog posts I love to keep things simple, so after\nchecking Varnish and all the issues that it will bring to our stack we decided\nto check the capabilities of uWSGI regarding caching (I even opened an\n[issue](https://github.com/maxcountryman/flask-login/issues/109) on\nFlask-Login about not using cookies for anonymous users in order to use Varnish\nwith no much luck).\n\nuWSGI has a very powerful plugin system that allows you to customize how your\nweb service will behave. For example you can use the [internal\nrooting](http://uwsgi-docs.readthedocs.org/en/latest/InternalRouting.html) plus\nthe cache route plugin for caching specific requests based on some rules that you \nconfigure. \n\n![Success GIF](http://i.giphy.com/dmt0NRgroyTPW.gif){: .img-responsive}\n\n\nIn the [uWSGI Caching Cookbook](https://github.com/unbit/uwsgi-docs/blob/master/tutorials/CachingCookbook.rst),\nthe explain step by step how you can do it for almost every single scenario,\nhowever the examples are very generic and you will need to work out your own\nrules to fit your project.\n\nAn example config file for uWSGI where you cache all the pages would be the\nfollowing:\n\n```\n[uwsgi]\nplugin = router_cache\nchdir = /your/project/\npythonpath = ..\nvirtualenv = /your/virtualenv\nmodule = run:app\nprocesses = 2\n; log response time with microseconds resolution\nlog-micros = true\n\n; create a cache with 100 items (default size per-item is 64k)\ncache2 = name=mycache,items=100\n\n; fallback to text/html all of the others request\nroute = .* cache:key=${REQUEST_URI},name=mycache\n; store each successfull request (200 http status code) in the 'mycache' cache using the REQUEST_URI as key\nroute = .* cachestore:key=${REQUEST_URI},name=mycache\n```\n\nThis set of rules are very simple. It will cache every request that returns a\n200 status code in the cache. This config file is really nice for project where\nthe site is delivering content and there is no much changing.\n\nHowever our site has a mixture of both things, pages that do not change too\nmuch over time and pages that have to be adapted for each user (specially for\nregistered users). \n\n### Dealing with cookies and sessions\n\nAs I've said before Flask-login place cookie for anonymous and authenticated\nusers. Hence, all users have a cookie, *but authenticated ones have an extra one \nin our project as this cookie is used to remember the session of the user for a\nperiod of time.*\n\nThanks to this configuration we can know that a user is a registered one if\nboth cookies exists, or the other way around: we can know if a user is an anonymous \nuser if only the remember me cookie does not exist.\n\nUsing this knowledge we can instruct uWSGI to cache some URLs (i.e. front page,\nabout page, etc.) only for anonymous users, as they don't need tailored\ninformation. If they sign up then, instead of serving their cached request we\nwill process the request as usual (remember that we've different levels of\ncaches, right?). \n\nTo us, for the moment, the most important aspect to cache is what anonymous\nusers see, as this segment is what's driving most of the traffic to our site.\nNow that we can distinguish between authenticated and anonymous users, we basically \nconfigure the uWSGI like this:\n\n```\nroute-if = empty${cookie[remember_token]} goto:cacheme\nroute-run = continue:\n\n; the following rules are executed only if remember_token is empty\nroute-label = cacheme\nroute = ^/about$ cache:key${REQUEST_URI},name=cache2\nroute = ^/about$ cachestore:key=${REQUEST_URI},name=cache2\n```\n\nThe above example caches for anonymous users the about page of our\nCrowdcrafting site. When the cache is clean, the first rule will fail, so it\nwill process the request,  stored it in the cache and then served it. Next time\nthe same anonymous user or another one request the same URI, the cached request\nwill be served boosting the performance a lot. Simple, right? Now you only\nadapt this snippet to your own URIs and web project and you will have an\namazing boost in performance. Best part? That you don't have to touch a single\nline of your source code. Amazing!\n\n![Clap GIF](http://i.giphy.com/DKqH1q9gN5AKA.gif){: .img-responsive}\n\n\nRegistered users will never receive any cached request with this configuration.\nYou could cache for every user each URI based on their remember_token cookie\nhowever that will require lots of memory and it will defeat the purpose of\nhaving a cache: that lots of requests are already served from the same data\npoint. Having a cached item per user is useless on this regard, as you will be\nloosing performance. In this case it's is much better to cache at the data\nlevel, as all the users would benefit from it: anonymous and authenticated\nones.\n\n## Summary\n\nThanks to this solution we've improved our performance a lot. Before these\nimprovements, the average response time of our servers were close to 250ms and now all of\nthem are responding in average below the 50ms. Saving 200ms is incredible!\nMost importantly because we've not added a new layer or anything special to our\nown stack. We've just configured it better!\n\n**NOTE**: The heading photo pictures the filament of a light bulb. To take the\npicture the photographer used a **micro** lens, and I've always pictured uWSGI\nas micro WSGI ;-)\n",
    "basename": "2015-03-05-uwsgi"
  },
  "2015-06-08-translating-pybossa": {
    "title": "Auto-translating PYBOSSA using PYBOSSA",
    "template": "entry",
    "slug": "translating-pybossa",
    "icon": "lostintranslation",
    "icon_author": "Alfonso",
    "icon_url": "https://www.flickr.com/photos/tochis/3081093838/",
    "tags": "Crowdcrafting, PYBOSSA, translation",
    "location": "Madrid, Spain",
    "meta_description": "",
    "headline": "If you talk to a man in a language he understands, that goes to his head. If you talk to him in his own language, that goes to his heart. -Nelson Mandela.",
    "layout": "blog",
    "preview": "How do you translate properly your product into different languages? …",
    "content": "\n\nHow do you translate properly your product into different languages? More importantly, how do you do it involving your own community?\n\nThe answer is easy: using a crowdsourcing solution like PYBOSSA.\n\n# Translating PYBOSSA using PYBOSSA\n\nSince the creation of PYBOSSA, I've translated it to Spanish. Other languages, like French, were added by a volunteer. However, these translations usually\nget outdated as PYBOSSA was updated with new strings. These solo efforts, usually end up in a translation that's not updated, and you end up with a mix of translated strings.\n\nFor these reasons we decided to eat our own dog food, and I created a [crowdsourcing \nproject](http://crowdcrafting.org/project/pybossaitalian) to translate PYBOSSA using PYBOSSA. Why? Because PYBOSSA uses the open \nstandard [Gettext](https://www.gnu.org/software/gettext/) for its translations, and each string could become a task in a \nPYBOSSA project. \n\nAlso **I loved the idea that anyone, even without an account, can help in the translation.**\nThe current platforms usually need an account to just translate a few strings, and that's usually too much\nfor users who want to see the product they use in their own language. Obviously some\npeople will add fake translations, but that's not an issue as the crowd will help to clean the bad ones\nand keep the best one.\n\nAs I started working on it, I realized this could be very useful not only for me and PYBOSSA\nbut also to anyone using the Gettext technology in their projects. Thus, I created a PYBOSSA template \n[project](https://github.com/PYBOSSA/app-translations) that anyone can re-use and adapt today to translate their own projects.\n\n\n# The Translation Template Project\n\nThe template can be used in any PYBOSSA server, so if you don't have one, don't \nhesitate and go to [Crowdcrafting](http://crowdcrafting.org) to create an account and \nstart using it. \n\nThe [ translation template ](https://github.com/PYBOSSA/app-translations) is very simple. It has been designed to have two phases:\n\n* The Translation: 3 people translate the same string.\n* The Voting: 5 people vote for the best translation of the 3 translations.\n\nThe most voted, it's the one that it's going to be used as the final translated one. \n\nAs you can see the community of your project would be involved in translating but also\nin selecting the best translation for them. This will ensure that your audience will\nhave a better understanding about the text you write, leading to better results in engagement.\n\n## 1. The Translation phase\n\nThe first thing you need to do is to download the template. Then, install the required\ntools (see the [README](https://github.com/PYBOSSA/app-translations) file for more information), and you will be ready to start translating\nyour project.\n\nThen, all you have to do is get your PO file (it's a text file with the string \nto get translated from for example English to Spanish). Once you have it, you will pass it\nto [PBS](https://github.com/PYBOSSA/pbs) -our PYBOSSA command line tool- that will \nconvert untranslated strings to tasks for your PYBOSSA project:\n\n```\npbs add_tasks --task-file=messages.pot --tasks-type=po --redundancy=3\n```\n\nThis will add the untranslated strings as tasks to your PYBOSSA project. Each string will be shown to 3 different people, so you get\n3 translation for your own project. You can increase or reduce it as much as you want. It's up to you to decide.\n\nWhen all the strings have been translated, you can move to the next phase if you want: the voting phase. \n\n## 2. The Voting phase \nIn this phase, the 3 previous translations will be shown to people and they'll select \nthe best one for them. The most voted one will be the final translation for that string.\n\nHow do you move from one phase to the next one? As simple as this. First we create the voting project:\n\n```bash\npbs --project project_voting.json create_project\npbs --project project_voting.json update_project\n```\n\nSecondly, we get the translated strings and pass them to the new voting project:\n\n```bash\npython vote.py\npbs --project project_voting.json add_tasks --task-file=/tmp/translations_voting_tasks.json --redundancy=5\n```\n\nThen, 5 people will vote on which is the best translation. When all the strings have \nbeen curated by your community, in other words when the project is completed, all you \nhave to do to create the final translation file is running the following command:\n\n```bash\npython create_mo.py\n```\n\nCopy the new created file into your translations project, and you'll be done! As simple as that.\n\n## Firefox extensions\n\nYes, PYBOSSA also supports Firefox extensions. Thus, if you are writing a Firefox \nextension and you want to translate it to different languages, you can use\nPYBOSSA too. It's pretty similar and you have all the documentation about it \n[here](https://github.com/PYBOSSA/app-translations).\n\n\n# Summary\n\nWith our PYBOSSA translation template anyone can translate their open source project\nwith their community, involving them not only in the translation but also curating which\nis the best translation for every string.\n\nThus, don't get lost in translation anymore!\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<video autoplay loop>\n<source src=\"http://media.giphy.com/media/ZCnP8OtmVRbPi/giphy.mp4\">\n</video>\n</div>\n",
    "basename": "2015-06-08-translating-pybossa"
  },
  "2015-07-01-the-art-of-graceful-reloading": {
    "title": "The Art of Graceful Reloading",
    "template": "entry",
    "slug": "gracefulreloading",
    "icon": "loop",
    "icon_author": "Leo Prieto",
    "icon_url": "https://www.flickr.com/photos/leoprieto/582310541/",
    "tags": "Crowdcrafting, architecture, infrastructure",
    "location": "Madrid, Spain",
    "meta_description": "Reloading your server without your users noticing it",
    "headline": "Reloading your server without your users noticing it",
    "layout": "blog",
    "preview": "The holy grail of web developers is to do deployments without …",
    "content": "\n\nThe holy grail of web developers is to do deployments without interrupting your users.\nIn this blog post I explain how we have achieved it using [uWSGI Zerg Mode](http://uwsgi-docs.readthedocs.org/en/latest/articles/TheArtOfGracefulReloading.html#zerg-mode) for our\n[Crowdcrafting](http://crowdcrafting.org) servers.\n\n<!--more-->\n\nIn a previous post I've already said that I love [uWSGI](http://uwsgi-docs.readthedocs.org/). \nThe main reason? You can do lots of nice tricks in your stack without having to add other\nlayers to it, like for example: **graceful reloading**.\n\nThe documentation from uWSGI is really great, and it covers most of the cases for graceful\nreloading, however due to our current [stack](/blog/2015/02/10/infrastructure.html) and our [auto deployments solution](/blog/2015/02/25/autodeployments.html) \nwe needed something that integrated well with the so called: [Zerg dance](http://uwsgi-docs.readthedocs.org/en/latest/articles/TheArtOfGracefulReloading.html#the-zerg-dance-pausing-instances).\n\n## Zerg Mode\n\nThe Zerg mode is a nice feature from uWSGI that allows you to run your web application passing\nfile descriptors over Unix sockets. As stated on the [official docs](http://uwsgi-docs.readthedocs.org/en/latest/articles/TheArtOfGracefulReloading.html#zerg-mode):\n\n*Zerg mode works by making use of the venerable “fd passing over Unix sockets” technique.*\n \n*Basically, an external process (the zerg server/pool) binds to the various sockets required by your app. Your uWSGI instance, instead of binding by itself, asks the zerg server/pool to pass it the file descriptor. This means multiple unrelated instances can ask for the same file descriptors and work together.*\n\n\nThis is really great, as you only need to enable a Zerg server and then you are ready to use it.\n\nAs we use Supervisor, configuring uWSGI to run as a Zerg server is really simple:\n\n{% highlight bash %}\n[uwsgi]\nmaster = true\nzerg-pool = /tmp/zerg_pool_1:/tmp/zerg_master.sock\n{% endhighlight %}\n\nThen, you configure your web application to use the zerg server:\n\n{% highlight bash %}\n[uwsgi]\nzerg = /tmp/zerg_master.sock\n{% endhighlight %}\n\nAnd you are done! That will configure your server to run in Zerg mode. However,\nwe can configure it to handle reloading in a more useful way: keeping a binary copy of \nthe previous running instance, pausing it, and deploying the new code on a new Zerg.\nThis is known as Zerg Dance, so let's dance!\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/GFBME4lzPVwxW\" width=\"480\" height=\"270\" frameBorder=\"0\" style=\"max-width: 100%\" class=\"giphy-embed\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n</div>\n\n## Zerg Dance\n\nWith the Zerg dance we'll be able to do deployments while the users keep using your\nweb application, as the Zerg server will be always handling those requests properly.\n\nThe neat trick from uWSGI is that it will handle those requests pausing them, so the user\nthinks it's getting slower, while the new deployment is taking place. As soon as the new\ndeployment is running it moves the \"paused request\" to the new code and keeps the old copy\nin case you broke something. Nice, right?\n\nTo achieve this situation all you have to do is use 3 different FIFOs in uWSGI. Why?\nBecause uWSGI can have as many master FIFOs as you want allowing you to pause zerg servers\nand move between them. This feature allows us to keep a binary copy of previously deployed code\non the server, that you can pause/resume and use it when something goes wrong.\n\nThis is really fast. The only issue is that you'll need more memory on your server, but I \nthink it's worthy as you'll be able to rollback a deployment with just two commands (we'll see\nthat in a moment).\n\n### Configuring the 3 FIFOs\n\nThe documentation has a really good example. All you have to do is to add 3 FIFOs to\nyour web application uWSGI config file:\n\n{% highlight bash %}\n[uwsgi]\n; fifo '0'\nmaster-fifo = /var/run/new.fifo\n; fifo '1'\nmaster-fifo = /var/run/running.fifo\n; fifo '2'\nmaster-fifo = /var/run/sleeping.fifo\n; attach to zerg\nzerg = /var/run/pool1\n; other options ...\n\n; hooks\n\n; destroy the currently sleeping instance\nif-exists = /var/run/sleeping.fifo\n  hook-accepting1-once = writefifo:/var/run/sleeping.fifo Q\nendif =\n; force the currently running instance to became sleeping (slot 2) and place it in pause mode\nif-exists = /var/run/running.fifo\n  hook-accepting1-once = writefifo:/var/run/running.fifo 2p\nendif =\n; force this instance to became the running one (slot 1)\nhook-accepting1-once = writefifo:/var/run/new.fifo 1\n{% endhighlight %}\n\nAfter the FIFOs there is a section where we declare some hooks. These hooks will handle\nautomatically which FIFO has to be used in case of a server is started again. \n\nThe usual work flow will be the following:\n\n - You start the server.\n - There is not sleeping or running fifo, so those conditions fail\n - Therefore, once the server is ready to accept requests (thanks to hook-accepting1-once) it moves the server from the new.fifo to running.fifo\n\nRight now you've a server running as before. Imagine now you have to change something in the config\nor you have a new deployment. You do the changes, and start a new server with the same uWSGI config\nfile. This will happen:\n\n - You start the second server.\n - There is not sleeping fifo, so this condition fails\n - There is a running fifo, so this condition is met. Thus, the previous server is moved to the sleeping fifo and its paused when the new server is ready to accept requests.\n - Finally, once the server is ready to accept requests t moves the server from the new.fifo to running.fifo.\n\nAt this moment we've two servers: one running (the new one with your new code or config changes) and the old one\nwich is paused consuming only some memory.\n\nImagine now you realize that you have a bug in your new deployed code. How do you recover from this situation? Simple!\n\nYou just pause the new server and unpause the previous one. How do you do it? Like this:\n\n{% highlight bash %}\necho 1p > /tmp/running.fifo\necho 2p > /tmp/sleeping.fifo\n{% endhighlight %}\n\n## Our setup\n\nWith our [auto deployments](/blog/2015/02/25/autodeployments.html) solution, we needed to find a simple way to integrate\nthis feature with supervisor. In the previous example you do the deployment manually,\nbut we want to have everything automated.\n\nHow we have achieved this? Simple! Using two PYBOSSA servers within Supervisor.\n\nWe have the default PYBOSSA server, and another one named pybossabak in Supervisor.\n\nWhen a new deployment is done, the auto deployments solution boots the pybossa Backup server\njust to have a copy of the running state of the server. Then, it gets all the new changes,\napplies patches, etc. and restarts the default server. This procedure triggers the following:\n\n - Start backup server: this moves the current running PYBOSSA server to the pause fifo, so we've a copy of it.\n - The backup server accepts the requests, so users don't see anything wrong.\n - Autodeployments applies changes to the source code, updates libraries, etc.\n - Then, it restarts the default PYBOSSA server (note: for supervisor the paused PYBOSSA server is running).\n - This restart moves the previous backup server to the pause fifo (it has the old code running), and boots the new code into production.\n\nIf something goes wrong with the new changes, all we have to do is pause the current server and resume the previous one.\n\nThis is done by hand, as we want to have control over this specific issue, but overall we are always covered\nwhen doing deployments automatically. We only have to click in the Merge Button of Github to do a deployment\nand we know a backup binary copy is hold on memory in case that we commit an error.\n\nMoreover, the whole process of having uWSGI moving the requests of users from one server to another is great!\n\nWe've seen some users getting a 502, but that's because they ask for a request when the file descriptor is being\nmoved to the new server. Obviously, this is not 100% bullet proof, but much better than showing to *all* your users\na maintenance page while you do the upgrade.\n\nWe've been using this new work flow for a few weeks now, and all our production deployments\nare done automatically. Since we adopted this approach we've not have any issues, and we are more\nfocused only on developing more code. We employ less time handling deployments, which is great!\n\nIn summary: if you are using uWSGI, use the Zerg Dance, and enjoy the dance!\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/2tDQZuljhwHTi\" width=\"480\" height=\"182\" frameBorder=\"0\" style=\"max-width: 100%\" class=\"giphy-embed\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n</div>\n",
    "basename": "2015-07-01-the-art-of-graceful-reloading"
  },
  "2015-08-12-17k-users": {
    "title": "17000 Volunteers Contribute to a PhD",
    "template": "entry",
    "slug": "thesis-17k",
    "icon": "together",
    "icon_author": "Lennart Tange",
    "icon_url": "https://www.flickr.com/photos/lennartt/9537775212/",
    "tags": "Crowdcrafting, thesis, phd",
    "location": "Madrid, Spain",
    "meta_description": "You have to take risks. We will only understand the miracle of life fully when we allow the unexpected to happen. Paulo Coelho",
    "headline": "You have to take risks. We will only understand the miracle of life fully when we allow the unexpected to happen.<br/> Paulo Coelho",
    "layout": "blog",
    "preview": "Doing a PhD is laborious, hard, demanding, exhausting... Your thesis …",
    "content": "\n\nDoing a PhD is laborious, hard, demanding, exhausting... Your thesis is usually the\nresult of blood, sweat and tears. And you are usually alone. Well, what woud you say\nif I tell you that a researcher got helped by more than 17 thousand volunteers?\n\n<!--more-->\n\nYes, you've read it right: more than 17 thousand people have helped Alejandro Sánchez to do his \nresearch, publishing his thesis as a result and getting the best possible mark: *cum laude*. Amazing, right?\n\nBut how this happened? How did he managed to involve such a big crowd? I mean, most people\nthink science is boring, tedious, *difficult*, add here your adjective... However, this\nguy managed to get 17 thousand people from all over the world to help him on:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/fqIBaMWI7m7O8\" width=\"480\" height=\"270\" frameBorder=\"0\" style=\"max-width: 100%\" class=\"giphy-embed\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n</div>\n\nBest part? They did it because they wanted to help. No money involved! Just pure kindness.\n\nIn other words, *the unexpected happened*, and thanks to *sharing* his work and also *asking for help* for his research -studying light pollution on cities- he managed to achieve the \nunconceivable: involving more than 17 thousand people on scientific research.\n\nHow this started? Well, let's start from the beginning. \n\n## The beginning: laying down the ideas\n\nThis adventure started in 2014, in London, UK. I was participating at the [Citizen Cyberscience Summit](http://cybersciencesummit.org/)\nand Alejandro was there because someone told him to learn more about [Crowdcrafting](http://crowdcrafting.org).\n\nAt the summit there was a workshop where scientists and hackers joined forces to \ncreate new *citizen science* projects. Wait, let me explain first what's citizen \nscience so we can enjoy the trip later on (like this kid, I promise).\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/nMjnOuMRUBeW4\" width=\"480\" height=\"404\" frameBorder=\"0\" style=\"max-width: 100%\" class=\"giphy-embed\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n</div>\n\n**Citizen science** is the active contribution of people who are not professional scientists to science. It provides volunteers with the opportunity to contribute intellectually to the research of others, to share resources or tools at their disposal, or even to start their own research projects. Volunteers provide real value to ongoing research while they themselves acquire a better understanding of the scientific method.\n\nIn other words, **citizen science** opens the doors of laboratories and makes science accessible to all. It facilitates a direct conversation between scientists and enthusiasts who wish to contribute to scientific endeavor.\n\nNow, with this idea in our minds let's get back to Alejandro's research. \n\nAt this workshop Alejandro told me that he was studying light pollution on cities. He and his team realized\nthat the astronauts from the International Space Station take pictures of the earth with a \nregular camera. Those pictures are then saved in a big archive. However, there are some issues:\n\n * The pictures could be from cities at night or day.\n * They take selfies too (who doesn't?)\n * The moon, stars and Aurora Borealis are also pretty, so they photograph them too.\n * The archive does not have any order or filter, everything is mixed in there.\n\nIn summary, he needs pictures at night of cities (sharp and without clouds) but the archive\nis a mess. The archive has too many different photos and possible scenarios that algorithms cannot help\nhim to classify them (or at a later stage geolocate them). However, you and me are pretty good\nat identifying cities at night with a glimpse, so we decided to create a prototype in Crowdcrafting.\n\nThe first project was [Dark Skies](http://crowdcrafting.org/project/darkskies/). We had the first\nprototype in a few hours and we basically asked people to help us to classify the pictures\nin different categories:\n\n * City at night\n * Aurora Borealis\n * Stars\n * None of these\n * Black\n * Astronaut\n * I don't know\n\nThe project was simple and fun. I remember enjoying a lot classifying beautiful \npictures from the ISS. It make me feel I was an astronaut, and I loved that feeling so \nwe share it with our friends and colleagues. \n\nWe really believed on the project, specially Alejandro, so he invited me to meet his PhD advisor and his\ncolleagues. We met and studied how we could improve it. As a result two new projects were born in the next months: [Lost at night](http://crowdcrafting.org/project/LostAtNight/) and [Night Cities ISS](http://crowdcrafting.org/project/nightcitiesiss/)\n\n## The small announcement that became huge\n\nAfter a lot of work, Alejandro thought that the projects were good enough to send \nthem to NASA and ESA. Alejandro wrote a press release and share with them what we \nwere doing.\n\nIn the beginning we thought that they will ignore us, but something happened. It started\nlike a tremble. With a tweet:\n\n<blockquote class=\"twitter-tweet\" lang=\"es\" align=\"center\"><p><a href=\"https://twitter.com/hashtag/Citizenscience?src=hash\">#Citizenscience</a> at work RT <a href=\"https://twitter.com/teleyinex\">@teleyinex</a>: <a href=\"https://twitter.com/esa\">@esa</a> thanks to your help on Twitter <a href=\"https://twitter.com/cities4tnight\">@cities4tnight</a> has 3000 tasks classified in <a href=\"https://twitter.com/crowdcrafting\">@crowdcrafting</a></p>&mdash; ESA (@esa) <a href=\"https://twitter.com/esa/status/487228335018475521\">julio 10, 2014</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nThen, almost one month later [NASA wrote a full article about the project](http://www.nasa.gov/mission_pages/station/research/news/crowdsourcing_night_images) and tweeted about it:\n\n<blockquote class=\"twitter-tweet\" lang=\"es\" align=\"center\"><p>Space station sharper images of Earth at night crowdsourced for science: <a href=\"http://t.co/bHBiLwvZSv\">http://t.co/bHBiLwvZSv</a>   <a href=\"https://twitter.com/hashtag/ISS?src=hash\">#ISS</a> <a href=\"http://t.co/bL9LymQ6cq\">pic.twitter.com/bL9LymQ6cq</a></p>&mdash; NASA (@NASA) <a href=\"https://twitter.com/NASA/status/499963958552711168\">agosto 14, 2014</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\nThat was the spark, as since that moment everything exploded! The project was covered internationally by the press. Media like [Fox TV](http://video.foxnews.com/v/3742323090001/nasa-asks-for-help-tracking-locations-in-iss-photos/?#sp=show-clips), [Gizmodo](http://gizmodo.com/nasa-wants-you-to-sift-through-its-astronauts-photos-1623753892), [CNN](http://edition.cnn.com/2014/08/17/tech/nasa-earth-images-help-needed/index.html?hpt=hp_t2), [...](http://pybossa.com/press/) share the project and invited people to help.\n\nThanks to this coverage, in just one month we were able to classify more than 100 thousand images. One day Crowdcrafting servers stored more than 1.5 answers per second! We were like this:\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/gmhJbyiaeqoX6\" width=\"480\" height=\"270\" frameBorder=\"0\" style=\"max-width: 100%\" class=\"giphy-embed\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>\n</div>\n\n## The calm after the storm\n\nAs with any press coverage after a few weeks everything went back to normal. However,\nlots of people kept coming and helping the projects from Alejandro.\n\nOver a year we kept fixing bugs, adding new tasks, answering questions from volunteers, sharing progress,\netc. In July Alejandro defended his thesis with all this work. Amazing!\n\nFrom my side I'm so happy and proud about it for two reasons. First, while the thesis \nhas been presented, the projects keeps going. \n\nAt the time of this writing\nthe Dark Skies project has classified almost 700 images in the last 15 days. Amazing!\n\nThe other two projects have less activity, as those projects are more complicated. \n[Lost at Night](http://crowdcrafting.org/project/LostAtNight/) has located more than 200\nphotos on a map, and [Night Cities ISS](http://crowdcrafting.org/project/nightcitiesiss/) has\ngeo-referenced almost 25 pictures.\n\nSecondly, because this is the very first thesis that uses PYBOSSA and Crowdcrafting \nfor doing open research. I'm impressed and I think this is just the beginning for many\nmore researchers doing their research on the open inviting society to take part on it.\n\nThe future? Well, Alejandro has launched a [Kickstarter](https://www.kickstarter.com/projects/1550160587/cities-at-night) campaign to get financial support\nto keep running the research his doing. If he gets the financial support more data will\nbe analyzed, new results will be produced and it will help to keep running Crowdcrafting\nand PYBOSSA. Thus, if you like the project help Alejandro to build the most beautiful\natlas of earth at night!\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/xTiTnlmaAy5VZFAqFq\" width=\"480\" height=\"270\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n</div>\n",
    "basename": "2015-08-12-17k-users"
  },
  "2015-12-22-year-in-review": {
    "layout": "blog",
    "title": "Year in review",
    "date": "2015-12-22T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "road",
    "icon_author": "Dustin Graffke",
    "icon_url": "https://www.flickr.com/photos/onepointfour/21478226510/",
    "description": "2015 has been an incredible year for team SciFabric.",
    "author": "teleyinex",
    "tags": "yearinreview",
    "preview": "Wow, another year is almost over! Time flies and, like everyone …",
    "content": "\n\nWow, another year is almost over! Time flies and, like everyone else, we love to\nreflect on our achievements over the past 12 months...\n\n## PyBossa\n\nAs you know, or at least should know <i class=\"twa twa-smile\"></i>, [PyBossa](http://pybossa.com) is the king of our products. \nIt lets you *build your own research platform* in just a few easy steps. This year, we've \nintroduced some cool new features so you'll be able to do more\nwith less hacking. (Don't worry developers, we've created amazing tools for\nyou too... read on!)\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n  <iframe class=\"embed-responsive-item\" src=\"//giphy.com/embed/oKVs1VY0MKfvO\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"http://giphy.com/gifs/nervous-indiana-jones-waiting-oKVs1VY0MKfvO\"></a></p>\n  </div>\n\n\n### Third Party Integrations\n\nWe know that data can be stored in different places. We've therefore introduced three new integrations\nto PyBossa:\n\n *  Flickr\n *  Dropbox\n *  Twitter\n\n**You can now import any album from Flickr**, or log in with your Flickr account and select the album\nyou want to import. As simple as that. This feature is pretty handy for galleries, libraries, archives\nand museums (GLAM) as they usually have tons of pictures on [Flickr commons](https://www.flickr.com/commons/). Thus, if you\nhave photos in Flickr already, import them with a click or upload new photos to it. Flickr gives 1TB of space\nfor free!\n\nWell, if you think Flickr is handy, what about our Dropbox integration? Yes, **you can import any\npicture, audio file, PDF or video file from your own Dropbox account**. As many people use Dropbox to store their\ndata sets, we thought: what about integrating it into PyBossa? And voilà, here you have it! Another\nintegration that allows you to easily select your own data sets from a file viewer.\n\nIn December, we've also added a Twitter importer. Were you looking for **sentiment analysis using Twitter hashtags**? You've found it!\nYou only have to type the query that you want to import, and PyBossa will do the hard work for you. We've created [two templates](https://github.com/PyBossa/project-twitter-templates)\nso you can conduct sentiment analysis in under 10 minutes!\n\n### Auto importers\n\nWhile these integrations are cool, we realized that lots of projects are updated *by hand* when\nnew data becomes available. For instance, when you used our [EpiCollect+](http://plus.epicollect.net/) integration\nto capture data with phones, you *had* to manually import the project again from time-to-time to capture\nthe new entries. It was simple, but you had to remember to do it. Thus, we decided to automate it, \nand so the **auto importers** were born.\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n  <iframe class=\"embed-responsive-item\" src=\"//giphy.com/embed/YXpp9YxWhyWBy\" width=\"480\" height=\"360\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"http://giphy.com/gifs/angry-king-burger-YXpp9YxWhyWBy\"></a></p>\n  </div>\n\nAuto importers let you transfer data directly into your PyBossa project without having to do it \nmanually. This feature allows you to push data to your projects while you can do other\nstuff, such as enjoy a cup of tea and check your emails. PyBossa will notify you when the new data\nhas been added. Enjoy!\n\n### Mailchimp\n\nThe integrations discussed so far benefit you: the project owner. But what about your community? Well, don't worry,\nwe've got them covered too. \n\nWe know that your community wants to get connected, get the latest news about your\ncrowdsourcing platform, and keep up-to-date on what you have achieved etc. So we've integrated PyBossa\nwith [Mailchimp](http://mailchimp.com/). The integration asks new users whether they want to\nsubscribe to your newsletter at registration. Simple but effective. Now you have no excuse for not communicating to your community, right?\n\n<div class=\"embed-responsive embed-responsive-4by3\">\n  <iframe class=\"embed-responsive-item\" src=\"//giphy.com/embed/XB6pGqvOfJqY8\" width=\"480\" height=\"368\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"http://giphy.com/gifs/dancing-happy-jimmy-fallon-XB6pGqvOfJqY8\"></a></p>\n  </div>\n\n### Administrator's dashboard\n\nWith this approach in mind, we also wanted you to have \nmore insight about what's going on in the platform. Hence, **we've developed a dashboard for administrators** \nwhere you can check the number of registered users in your site in the last week, \nfind out when new projects are created, published and updated, and when new versions of PyBossa are released, \nas well as view a live feed of what's going on the platform. \n\n### For da geeks!\n\nFinally (I told you we've not forgotten about the developers), **we've developed a plugin\nsystem in PyBossa**. This feature allows you to actually extend what PyBossa can do.\n\nFor example, the British Library and their PyBossa-powered platform [LibCrowds](https://github.com/LibCrowds/Z3950-pybossa-plugin) have \ncreated two cool plugins:\n\n * [Discourse integration](https://github.com/LibCrowds/discourse-pybossa-plugin): now you can have your own forum in PyBossa.\n * [Z39.50 integration](https://github.com/LibCrowds/Z3950-pybossa-plugin): if your institution uses this protocol, you can easily integrate it.\n\n\nNot bad for a year's work, right? Then, go and grab the latest version!\n\n## Be your own research platform\n\nThis year four institutions have chosen PyBossa as their solution to become their\nown crowdsourcing research platform (and we're so happy and proud about it).\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n  <iframe class=\"embed-responsive-item\" src=\"//giphy.com/embed/Fbyam9ZAJ3J1m\" width=\"480\" height=\"270\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"http://giphy.com/gifs/time-school-chemistry-Fbyam9ZAJ3J1m\"></a></p>\n  </div>\n\n### National Library of Israel\n\nThe National Library contacted us this year to showcase what they have achieved with\ntheir PyBossa-powered platform [nlics.org](http://nlics.org). Their goal is to improve \ntheir metadata by asking contributors to tag, transcribe and answer questions \nrelating to historical Israeli documents.\n\n### The Guardian\n\n[The guardian](http://www.theguardian.com/australia-news/datablog/2015/mar/09/why-were-crowdsourcing-the-nsw-pecuniary-interests-register-faq) \nnewspaper wanted to bring more transparency to New South Wales' (Australia)\npecuniary interests. They used PyBossa to make the first crowdsourcing \nproject of its kind. \n\nIn New South Wales politicians are required to declare details of gifts, investments, \nbusiness interests, and other items that could influence their decisions in parliament. They do this in the pecuniary interests register. However, half of New South Wales \npecuniary interests register has never been available to view online by the public, and the \nother half is hard to find. All of the declarations are only available as scanned \nPDFs, sometimes handwritten, which makes it difficult to properly scrutinise the register.\n\nThe solution? Use PyBossa to transform human readable documents into machine readable documents, \npublishing a [search interface](http://www.theguardian.com/global/datablog/ng-interactive/2015/mar/27/search-the-nsw-register-of-pecuniary-interests-to-see-what-politicians-have-declared) and a [data set in GitHub](https://github.com/nickjevershed/pecuniaryinterests/blob/master/data.json), as well as a nice [article about this\ninvestigation](http://www.theguardian.com/australia-news/2015/mar/27/exclusive-nsw-liberal-mps-failed-to-declare-financial-interests-on-register), where the journalists explains that their research *forced New South Wales fair trading minister and the Liberal party’s whip to correct their pecuniary interest disclosures*. Amazing!\n\n### The British Library\n\nThe British Library joined the PyBossa club with their [LibCrowds](http://libcrowds.com/) platform.\nAs with the National Library of Israel, they have several projects where they use PyBossa \nto ask volunteers to transcribe text from printed card catalogues into electronic records \nin order to make them available to a worldwide audience. \n\nThe project is initially focused on the library's Asian and African collections, \nparticularly the Chinese and Indian catalogues. Data identified, transcribed or \ntranslated as part of the project will be freely accessible from the \nBritish Library's Explore catalogue.\n\nYou can find the [first results of the project in their blog](http://www.libcrowds.com/blog/6).\n\n### University of Heidelberg\n\nThe [University of Heidelberg](http://www.heidelberg.edu/) and [Disaster Mappers](https://disastermappers.wordpress.com/) have used our PyBossa technology to create two amazing projects.\n\nThe [first project](http://crowdmap.geog.uni-heidelberg.de/app/missing_maps_follow_up/) supports the [Missing Maps Project](http://www.missingmaps.org/) by classifying \nBing aerial imagery. Volunteers were asked to assess whether there are human \nsettlements or major roads in the satellite imagery, [building the first data set](http://umap.openstreetmap.fr/es/map/missing-maps-south-kivu-region-human-settlements-a_53739#9/-2.9842/28.9970) for the\nanalyzed areas.\n\nThe [second project](http://crowdmap.geog.uni-heidelberg.de/app/shelter_dynamics/) monitors the temporal and spatial dynamics of camps of \ninternally displaced persons (IDP) using satellite imagery to provide credible and \nup-to-date information from the Nepal Earthquake.\n\n## Events\n\nThis year we have participated in several conferences, workshops and symposiums. We were at\n[EmpoderaLIVE](http://live.empodera.org/) and we helped present the results of Micropasts: a PyBossa\npowered project by the British Museum and University College of London. \n\n![Spear head 3D printed thanks to PyBossa](/assets/img/blog/spear.jpg){: .img-responsive}\n<p class=\"post-caption\">Spear head 3D printed built thanks to PyBossa powered photomasking projects at Micropasts.</p>\n\n\nAt [Zincshower](http://zincshower.com/) we won the award for **Best startup of the event**. The prize was to go to\n[Sonar+D](http://sonarplusd.com/), a very cool event where we had the opportunity to learn about the latest trends in\nelectronic music, as well as hang out with some of amazing start-ups.\n\n<div class=\"embed-responsive embed-responsive-4by3\">\n  <iframe class=\"embed-responsive-item\" src=\"//giphy.com/embed/26tPghhb310muUkEw\" width=\"480\" height=\"360\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"http://giphy.com/gifs/90s-retro-commercials-26tPghhb310muUkEw\"></a></p>\n  </div>\n\n## Crowdcrafting\n\n### New jazzy projects\n\nCrowdcrafting has also seen some amazing projects this year. Like [Landfill Hunter](http://crowdcrafting.org/project/landfill/), where the researcher\nwants to increase awareness of landfill sites, as well as contribute data to help better understand landfills and bring that information into the public domain.\n\nThe [European Illegal Parking](http://crowdcrafting.org/project/Illegal_Parking/) project wants to rank European cities in order of the level of illegal parking observed on their streets.\nHopefully, this ranking will raise awareness of the problem in Europe and put pressure on national and local institutions to pursue more effective measures to tackle the problem. Awesome, right?\n\nFinally, [Localizing Pune's Budget](http://crowdcrafting.org/project/localpunebudget/). \nThis project is trying to analyze and evaluate the ward-level infrastructure and public spending for Pune. \nOut of 9,614 budget items listed in the annual budget book for 2015-16, \nabout 1,997 do not have any ward number or whole-city marking alloted. \nIt would be great if you could assign the proper ward numbers to these works: \nso they then can have a more accurate picture about the investment made in various areas of Pune.\n\n### The stats\n\nIf we're to reveal our **vanity checks**, we can say that **people have uploaded more than 200K tasks to Crowdcrafting**, and close to \n**half a million answers have been submitted by volunteers from all over the world**.\n\n**2000 new users joined us this year**, while almost **4000 anonymous people participated in a project**.\n\nMore than **600 projects were created** and there will no doubt be lots of new ones in 2016.\n\n## Looking ahead...\n\nWhile this year has been amazing, we think that 2016 will be even cooler with new additions to PyBossa (like a new\nimporter for Youtube videos) as well as some secret stuff that we cannot share yet (yes, I'm trying to build anticipation\nso you'll check the blog next year <i class=\"twa twa-wink\"></i>).\n\nMerry Christmas and Happy 2016!\n",
    "iso8601Date": "2015-12-22T01:00:00+01:00",
    "basename": "2015-12-22-year-in-review"
  },
  "2016-02-09-atomic-force-microscope": {
    "title": "The DIY atomic force microscope",
    "layout": "blog",
    "icon": "cottoncell",
    "icon_author": "Project-128",
    "icon_url": "https://www.flickr.com/photos/project-128/11612131236/",
    "tags": "Crowdcrafting, thesis, phd, hardware, maker",
    "meta_description": "Build an atomic force microscope in less than 2 hours!",
    "headline": "Build an atomic force microscope in less than 2 hours!",
    "preview": "Here's a question: what image would you choose to represent …",
    "content": "\n\nHere's a question: what image would you choose to represent 'science'? If you search 'science' in [Google Images](https://www.google.com/search?q=science&biw=1535&bih=764&source=lnms&tbm=isch&sa=X&ved=0ahUKEwio_bbKoO3KAhVIVxQKHbgjCGkQ_AUIBygC),\nyou'll see a fair few images of microscopes. The microscope is a tool that people widely associate with science and research. But how did this happen? Why did the microscope become so popular?\n\nWell, it became popular thanks to Robert Hooke's book [**Micro**graphia (1665)](https://en.wikipedia.org/wiki/Micrographia). \nThis book marked a milestone in scientific history. It showed the science community how to use microscopes to analyze and study the **micro** world, with hand made drawings of cells, fleas and insects. \n\nNow, 351 years later, we might be experiencing another profound turning point in science. And I've though of a title for the associated book: **Nano**graphia! \nThe popularization of low cost, do-it-yourself atomic force microscopes will allow us to explore the **nano** world. But, how did we get here? How old is the microscope? Let's start... from the beginning.\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n    <iframe src=\"//giphy.com/embed/1unWthRtNnzkA\" width=\"100%\" height=\"auto\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p></p>\n</div>\n\nIndeed! It's a long story, but worth it. Bear with me. \n\n## The early years of the microscope\n\nEvidence suggests that the first compound microscope was built in the Netherlands in 1620. Almost 400 years ago!\n\nBut at this point, the microscope didn't have a name. Huh! \nWe had to wait another 5 years before it was named. \nIn 1625 [Giovanni Faber](https://en.wikipedia.org/wiki/Giovanni_Faber) – a fellow of the [Lincean Academy](https://en.wikipedia.org/wiki/Accademia_dei_Lincei) - coined the name *microscope*, after [Galileo Galilei](https://en.wikipedia.org/wiki/Galileo_Galilei) presented it one year before.\nThe name comes from the Greek words μικρόν (micron) meaning \"small\", and σκοπεῖν (skopein) meaning \"to look at\".\n\nSo we know when it was named, but when did it become a popular instrument? Well, guess what, another 40 years elapsed before the microscope becomes a popular tool used in science. \n\nAs noted, in 1665 [Robert Hooke](https://en.wikipedia.org/wiki/Robert_Hooke) published [Micrographia (PDF)](https://ia800504.us.archive.org/5/items/mobot31753000817897/mobot31753000817897.pdf) – a book that inspired the use of microscopes for scientific exploration. The details and quality of his hand made drawings of insects and plant cells (engraved in copper plates) popularized his work. Check out the following images!\n\n![Hooke's microscope](/assets/img/blog/Hooke-microscope.png){: .img-responsive}\n\n<p class=\"post-caption\">Hooke's microscope, from an engraving in Micrographia. Photo by <a href=\"https://en.wikipedia.org/wiki/Robert_Hooke#/media/File:Hooke-microscope.png\">Wikipedia</a>.</p>\n\nThe engravings were very detailed, but *the most awesome feature was that you could unfold them, making them larger than the book itself, reinforcing the tremendous power of the microscope*. For example, see this flea:\n\n![Cork cells](/assets/img/blog/HookeFlea01.jpg){: .img-responsive}\n\n<p class=\"post-caption\">Hooke's drawing of a flea. Photo by <a href=\"https://en.wikipedia.org/wiki/Robert_Hooke#/media/File:HookeFlea01.jpg\">Wikipedia</a>.</p>\n\nWhile these achievements are amazing, you might better know Hooke as the person who coined the term *cell*, as the small structures he observed in a cork sample reminded him of honeycomb cells.\n\n![Cork cells](/assets/img/blog/cork.jpg){: .img-responsive}\n\n<p class=\"post-caption\">Cell structure of Cork by Hooke. Photo by <a href=\"https://en.wikipedia.org/wiki/Robert_Hooke#/media/File:RobertHookeMicrographia1665.jpg\">Wikipedia</a>.</p>\n\n## The microscope becomes a scientific tool\n\nWe know when the microscope was invented, when it was named and when it became popular. Now, it's time to learn about its impact in the scientific world (and TV shows!).\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n    <iframe src=\"//giphy.com/embed/Fbyam9ZAJ3J1m\" width=\"480\" height=\"270\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n</div>\n\n\nIn the years after Hooke's publication, the microscope was widely used in Italy, the Netherlands and England. The greatest contribution in this period came from [Antonie van Leeuwenhoek](https://en.wikipedia.org/wiki/Antonie_van_Leeuwenhoek) who \nhas been credited with the discoverery of red blood cells and, consequently, helping to popularize [microscopy](https://en.wikipedia.org/wiki/Microscopy) as a technique. On 1676, \nVan Leeuwenhoek reported [the discovery of micro-organisms](https://en.wikipedia.org/wiki/Microorganism#History_of_microorganisms.27_discovery).\n\n**[Side note](http://www.med-ed.virginia.edu/courses/cell/resources/blooddisc.htm)**:  In truth, Van Leeuwenhoek was not the first person to describe \"red particles\" in blood. However, his observations were more detailed and numerous than his predecessors ([Malpighi](https://en.wikipedia.org/wiki/Marcello_Malpighi) and [Swammerdam](https://en.wikipedia.org/wiki/Jan_Swammerdam)).\n\n\nWhile microscopes became widely used, they faced a problem: *how to light the samples*. \n\nLighting is key in microscopy in order to see the sample properly, so it was not  until electric lamps were available as light sources that new advances and  discoveries were made. \n\nAfter improving the lighting, scientists started to question the limits of this technology. What's the smallest thing that you can see with it? To answer this question [August Köhler](https://en.wikipedia.org/wiki/August_K%C3%B6hler) developed the [Köhler illumination](https://en.wikipedia.org/wiki/K%C3%B6hler_illumination) principle, which is central to achieving the theoretical [limits of light microscopy](https://en.wikipedia.org/wiki/Microscopy#Limitations).  This was 1893.\n\nKnowing microscopy's limits, the pursuit of further knowledge pushed scientists to use electrons instead of light, and electromagnets in the  place of glass lenses, creating the first electron microscope: the [transmission electron microscope](https://en.wikipedia.org/wiki/Transmission_electron_microscopy). This was 1931.\n\n![Polio virus image](/assets/img/blog/polio.png){: .img-responsive}\n\n<p class=\"post-caption\">A TEM image of the polio virus. The polio virus is 30 nm in size. Photo by <a href=\"https://en.wikipedia.org/wiki/Transmission_electron_microscopy#/media/File:Polio_EM_PHIL_1875_lores.PNG\">Wikipedia</a>.</p>\n\nOther researchers tried different techniques. For example, in the 1980s some scientists started the development of the first [scanning probe microscopes](https://en.wikipedia.org/wiki/Scanning_probe_microscopy).  \nThe first one was the [scanning tunneling microscope](https://en.wikipedia.org/wiki/Scanning_tunneling_microscope) developed by [Gerd Binning](https://en.wikipedia.org/wiki/Gerd_Binnig) and [Heinrich Rohrer](https://en.wikipedia.org/wiki/Heinrich_Rohrer) (1981). Five years later Gerd Binning, [Quate](https://en.wikipedia.org/wiki/Calvin_Quate) and [Gerber](https://en.wikipedia.org/wiki/Christoph_Gerber) invented the **atomic force microscope** (AFM).\n\n## Atomic Force Microscope (AFM)\n\nThe AFM is a big step forward because it improves the quality of the images and  gives us access to the nano world. An optical microscope is limited by the wavelength of light it can detect. This is called the[Abbe limit](https://en.wikipedia.org/wiki/Diffraction-limited_system), which is around 250 nm (0.25 μm). \n\nWhile this resolution allows us to see most biological cells (1 μm to 100 μm), it fails if you try to study viruses (100 nm), proteins (10 nm) or less complex molecules (1 nm). On the other hand, the AFM has a demonstrated resolution in the order of fractions of a nanometer, more than 1000 times better than the optical diffraction (Abbe) limit.\n\nWhile these microscopes are amazing, they're really expensive too. One will set you back around 300,000 USD. \n\nThis basically becomes a huge problem to educators, well, to anyone, as not many Institutions often do not have the cash to buy this research tool.\n\n<div class=\"embed-responsive embed-responsive-4by3\">\n<iframe src=\"//giphy.com/embed/5u0uZecUZlUsM\" width=\"480\" height=\"327\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n</div>\n\nWith this problem in mind, in 2015 the [LEGO Foundation](http://www.legofoundation.com/) sponsored a summer school program to develop an affordable do-it-yourself (DIY) atomic force microscope suitable for use in schools by children. The [result](http://www.nature.com/nnano/journal/v10/n5/full/nnano.2015.95.html) has been an [open source AFM](http://openafm.com/) that children can build using LEGO pieces, Arduino, 3D printable parts and local components. \nAt the same time, [Edwin Hwu](http://www.phys.sinica.edu.tw/directory_user_en.php?id_key=94&eng=T) and his team (who are also developing an open AFM) licensed a low cost closed-source version, [the Strømlingo DIY AFM](http://www.stromlinet-nano.com/), which costs 98% less than the ones available on the market.\nThese big savings make AFM affordable, enabling institutions to buy cheap AFM microscopes that their students can build themselves. Moreover, this building process only takes a few hours and the kids can start operating it in a matter of minutes. \n<div class=\"embed-responsive embed-responsive-4by3\">\n<iframe src=\"//giphy.com/embed/LgwoVr7YgUkrC\" width=\"480\" height=\"342\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n</div>\n\nDue to this success, students will be able to see nano structures that would otherwise be impossible to view with a regular microscope, such as particles with an [aerodynamic diameter of 2.5 micrometers or less](http://www3.epa.gov/pmdesignations/faq.htm) (known as PM 2.5). Why are these particles particularly important? Well, because they're among the most harmful for human health as they are small enough to penetrate deeply in the lungs and may even cross into the blood. Scary, right?\n\nEdwin and his students have shown how affordable nanoscopes can be used to analyze and take samples of these particles and, indeed,  schools all over the world have participated in his projects. \n\n## Nanographia, the drawings of PM 2.5\n\nAs Robert Hooke's book Micrographia popularized microscopes, the low cost DIY nanoscope will enable kids and anyone with interest in this field to write the next book that will popularize this new type of microscope. If Micrographia was a milestone due to new discoveries in the micro world, affordable do-it-yourself nanoscopes will help to write the book that will make history again: Nanographia.\n\nThe book will describe how you can build the nanoscope and how you can use it for studying  PM 2.5 particles. Instead of drawings, the book will feature photos showing the discoveries. As you can see, Nanographia will help to spread the word about this technology, as Hooke did almost 400 years ago. \n\nAs with any other scientific publication, there will be a subject to be studied: the PM2.5 particles. A chapter might explain what PM2.5 particles are, as well as how you can make [microscope sample slides](https://en.wikipedia.org/wiki/Microscope_slide) by cutting up DVD ROMs and placing them outside for at least 10 minutes. \n\nWhy are we going to use DVDs? Because the distance between the tracks in a DVD are known (740 nanometers) and we can see them with the AFM. \n\nThe next chapter will be about the analysis. Once you have the samples, it will describe how you can analyze them by hand: calibrating the samples and measuring the area covered by the candidate PM2.5 particles.\n\nIncredible right? As we discovered this story, we wanted to contribute a few chapters for the book. One about crowdsourcing as we think it would be amazing for the crowd to analyze samples (following the citizen science approach of this project), and a second one running workshops about the project where you can learn, build the nanoscope, and analyze the samples with the crowd.\n\n## SciFabric's chapters for Nanographia\n\nOur desire to contribute to this book became a reality when  Edwin and his team contacted us to use our citizen science [Crowdcrafting](http://crowdcrafting.org)  platform for analyzing the samples with the crowd.  \n\nThis first chapter will be about citizen science, and it will describe what is [crowdsourcing and citizen science](http://scifabric.com/blog/2016/01/27/crowdsourcing-vs-crowdfunding.html).  \nIt will explain the project where anyone can analyze samples, including links to the  [prototype](http://crowdcrafting.org/project/lego2nano/). \n\nAlso, there will be a sub-section where we will describe the [tools that we use for building the prototype](http://pybossa.com), so others can replicate it (like science does!).\n\nThe next chapter will be about citizen science workshops, as [we offer them to students and teachers](http://scifabric.com/crowdsourcing/#education) as a new way to discover science and learn by doing. The chapter will include a new course on how you can build the nanoscope, use its technology and learn the citizen science approach.\n\nIn November 2015 we wrote the first draft. We proposed that [Medialab-Prado](http://medialab-prado.es/) (Madrid, Spain) organize an event like this, and they accepted. \n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/11sBLVxNs7v6WA\" width=\"480\" height=\"216\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n</div>\n\nThe workshop became popular and Spanish national TV show [La aventura del saber](http://www.rtve.es/television/la-aventura-del-saber/) (the adventure of knowing) interviewed us, showing how we built the microscope and analyzed some of the samples (it starts at minute 14:00, only in Spanish): \n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe frameborder=\"0\" src=\"http://www.rtve.es/drmn/embed/video/3468356\"\nname=\"La aventura del saber - 02/02/16\" scrolling=\"no\" style=\"width:100%;height:90%;position:absolute;left:0;top:0;overflow:hidden;\"  ></iframe>\n<div style=\"position:absolute;bottom:0;left:0;font-family:arial,helvetica,sans-serif;font-size:12px;line-height:1.833;display:inline-block;padding:5px 0 5px 10px;\">\n<span style=\"float:left;margin-right:10px;\"><img\nstyle=\"height:20px;width:auto;background: transparent;padding:0;margin:0;\"\nsrc=\"http://img.irtve.es/css/rtve.commons/rtve.header.footer/i/logoRTVEes.png\"></span> <a\nstyle=\"color:#333;font-weight:bold;\" title=\"La aventura del saber - 02/02/16\"\nhref=\"http://www.rtve.es/alacarta/videos/la-aventura-del-saber/aventura-del-saber-02-02-16/3468356/\"><strong>La aventura del saber - 02/02/16</strong></a></div>\n</div>\n\nDue to the success of the workshop, we were invited by [Medialab-Prado to share what we built at their open day event](http://medialab-prado.es/article/festilab-1-ano-en-un-dia). In the following video you can see me talking about the project (in Spanish):\n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"https://player.vimeo.com/video/154714664\" width=\"500\" height=\"281\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n<p><a href=\"https://vimeo.com/154714664\">Microsc&oacute;pio de fuerza at&oacute;mica</a> from <a href=\"https://vimeo.com/medprado\">Medialab-Prado</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>\n</div>\n\n## The final chapter \n\nWe've seen the evolution of microscopes and how human pursuit of knowledge has got  us here today. Nowadays we can build nanoscopes at home and explore the nano world in a few hours. The possibilities are endless, but are nanoscopes the new edition of Micrographia? Are nanoscopes making history?\n\nWell, I would say it's early to know, but the truth is that it has lots of potential.  \n\nI can barely imagine how kids will be building and using this tool trying to understand what lies in the nano world, and the best part is that we're exploring it together!\n\nBy the way: High Five! You are awesome! You just read until here, so cool! \n\n<div class=\"embed-responsive embed-responsive-16by9\">\n<iframe src=\"//giphy.com/embed/CDMz3fckRXXDG\" width=\"480\" height=\"269\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></p>\n</div>\n\nNow, get away from your laptop, tablet or phone and enjoy a beer, coffee, whatever you like. It's been a long read and you deserve it! \n\n\n",
    "basename": "2016-02-09-atomic-force-microscope"
  },
  "2017-10-13-jekyll-vuejs-bulma": {
    "layout": "blog",
    "title": "Building static websites with Jekyll, VueJS & Bulma",
    "date": "2017-10-13T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "code",
    "icon_author": "Luca Bravo",
    "icon_url": "https://unsplash.com/photos/XJXWbfSo2f0",
    "description": "Hacking with Jekyll, VueJS & Bulma",
    "tags": "jekyll, hacking, vuejs, bulma, frontend",
    "preview": "We love static websites. Why? Because they're fast. Really fast. …",
    "content": "\n\nWe love static websites. Why? Because they're fast. Really fast. Moreover, you don't have to take care of a database, and you like that a lot.\n\nYou are probably building your site already with Jekyll, and while this is cool, sometimes you want to add some magic into the mix to have some fancy JS frameworks like VueJS or React for developing cool stuff. Also, you got so used to Babel and Webpack that you don't know how to write JS code anymore without this toolchain. Hence, you have a question: can I still use my Jekyll site and add as toppings VueJS + Babel + Webpack? Yes, you can! Let me explain how. Let's begin the hacking!\n\n<div style=\"width:100%;height:0;padding-bottom:100%;position:relative;\"><iframe src=\"https://giphy.com/embed/JIX9t2j0ZTN9S\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/JIX9t2j0ZTN9S\">via GIPHY</a></p>\n\n## Making Jekyll speak JSON\nAs you will be using a framework like [VueJS](https://vuejs.org/) (we love love love love it so much, that from now on we will only talk about this framework), we need to instruct Jekyll to serve our content as JSON. \n\nThere are several options for serving your content as JSON, but one that we like a lot is using the _data folder to store inside of it YAML (or JSON directly) that will be transformed into JSON in the HTML page that your VueJS app will be rendered.\n\nWait, yes, you read that we will be writing content in YAML to transform it into JSON and then load it into VueJS. Why so much trouble? Well, because we want to allow non-coders to be able to add or edit content quickly, and YAML is simple on that (in other words, less curly braces ;-)).\n\nThus, go to your _data folder and create a file named **mydata.yml**. This file could have something like this:\n\n```yaml\n- title: How crowdsourcing can help beat cancer\n  person: profile.jpg\n  cover: logo.png\n  person_name: John Doe\n  person_position: Digital Solutions Architect\n```\n\nThen go to your Jekyll folder where you will be rendering your VueJS app. Let's see you want to build a hello world URL, so go to the folder helloworld and edit a file named index.html.\n\nThis file will have the front matter as any other Jekyll file, but you will have to modify it to render your VueJS app:\n\n```html\n---\nlayout: default\ntitle: Hello World\ndescription: Jekyll and Vuejs\n---\n<section id=\"vuejs\" class=\"section\">\n    <App></App>\n</section>\n<script>\nwindow.mydata = {{ site.data.mydata | jsonify }};\n</script>\n<script src=\"/assets/js/myvuejs.min.js\"></script>\n```\nThat's all! We provide the DOM for mounting the VueJS app. Then, we create a script section where we can load our mydata.yaml as a JSON object, and below it our minified version of our VueJS app (thanks Webpack!).\n\nObviously, you will need to compile webpack and Jekyll commands to build everything correctly. As both command support a --watch flag, you can run both of them in parallel and forget about running the commands by hand while you develop your excellent new site.\n\n## VueJS, Bulma CSS, and Webpack\n\nWell, this has been easy, right? But, how do we adequately integrate our webpack toolchain into the site as well? We just instruct webpack to do it properly.\n\n### Webpack \n\n```javascript\n// webpack.config.js\nvar htmlWebpackPlugin = require('html-webpack-plugin');\nvar webpack = require(\"webpack\");\nmodule.exports = {\n  // entry point of our application\n  entry: './helloworld.js',\n  // where to place the compiled bundle\n  output: {\n    path: '../',\n    publicPath: '../',\n    filename: 'helloworld.min.js'\n  },\n  module: {\n    // `loaders` is an array of loaders to use.\n    // here we are only configuring vue-loader\n    loaders: [\n      {\n        test: /\\.vue$/, // a regex for matching all files that end in `.vue`\n        loader: 'vue-loader'   // loader to use for matched files\n      },\n      {\n        test: /\\.js$/,\n        loader: 'babel-loader',\n        exclude: /node_modules/\n      },\n      {\n      test: /\\.[sa|sc|c]ss$/,\n      loader: \"style!css!sass\"\n      }, {\n        test: /\\.(png|jpg|jpeg|gif|svg|woff|woff2|ttf|eot)$/,\n        loader: 'file-loader?outputPath=../img/search/&publicPath=../img/search/'\n      }\n    ],\n    noParse: /dist\\/ol.js/,\n  },\nvue: {\n  loaders: {\n    scss: 'style!css!sass'\n  }},\n\n  resolve: {\n    alias: {\n      'vue$': 'vue/dist/vue.common.js'\n    }\n  },\n  plugins: [\n    new webpack.optimize.DedupePlugin(),\n    new webpack.optimize.UglifyJsPlugin({minimize: true})\n  ]\n\n}\n```\n\nNOTE: adapt the paths to your specific needs. These are just examples, so feel free to modify them to whatever you like.\n\nAs you can see, we're telling webpack to compile our code, minimize it and put it in the right place that we want. Now, you can jump into your VueJS app.\n\n### VueJS\n\nWe only use VueJS components. Thus, our example will be using a component that will render our data. The first element that we have to add is our entry point: helloworld.js\n\n```javascript\nimport Vue from \"vue\"\nimport App from \"./components/App.vue\"\n\n\nVue.config.debug = true\nVue.config.devtools = true\n\nvar app = new Vue({\n    data() {\n        return {foo: 'all'}\n    },\n    el: '#vuejs',\n    components: { App },\n})\n```\n\nThen, create a components folder and create a file named App.vue:\n\n```javascript\n<template>\n    <h1 v-for=\"datum in mydata\">{{datum.title}}</h1>\n</template>\n<script>\nexport default {\n   data() {\n      return {mydata: window.mydata}\n},\n// your stuff\n}\n</script>\n<style>\n</style>\n```\n\nDone! Now you are loading your data created in mydata.yml file into your VueJS app. Now you are free to do whatever you want, as you are in the field of VueJS. Enjoy!\n\n**NOTE**: Jekyll sometimes does not recompile the _data folder, so you will need to re-run it to be sure that your data is updated.\n\n### Bulma and Buefy\n\nWe use SaSS to style our Jekyll sites; I guess you do it too. If this is the case, you don't want to have separate sass folders to build your website and your VueJS apps. You can solve it by instructing your VueJS to re-use the Bulma CSS framework. How? Like this:\n\n```\n<style lang=\"scss\">\n@import \"../../../_sass/_scifabric.scss\";\n@import \"~buefy/src/scss/buefy\";\n\n// your SCSS\n</style>\n```\n\n## Cavebeats\n\nWhile this is an excellent **hack** it's not the best solution. It would be much much better to use only a static website generator built with Node.JS or just something like [nuxt.js](https://nuxtjs.org/), but we needed to re-use our Jekyll infrastructure and therefore the hack.\n\nIn any case, this hack has space for improvement. The most noticeable one would be to not include the CSS from Jekyll where we only use VueJS to avoid downloading the same stuff twice. If you like it, let us know and share this article with your colleagues and friends!\n\n### Final notes\n\nThis was originally posted in our [Scifabric](https://scifabric.com) site.\n",
    "iso8601Date": "2017-10-13T02:00:00+02:00",
    "basename": "2017-10-13-jekyll-vuejs-bulma"
  },
  "2018-01-08-on-the-press": {
    "layout": "blog",
    "title": "On the press",
    "date": "2018-01-08T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "press",
    "icon_author": "Samule Sun",
    "icon_url": "https://unsplash.com/photos/XJXWbfSo2f0",
    "description": "Press coverage about my work",
    "tags": "press, pybossa, scifabric, video, conference",
    "preview": "The end of 2018 was really good. The Spanish newspaper Expansion …",
    "content": "\n\nThe end of 2018 was really good. The Spanish newspaper Expansion (about business and economy) and the National Spanish TV RTVE interviewed me\nto show our work as examples of innovatives ways of supporting cultural enterprises or\ninstitutions from all the world.\n\nThese interviews have been thanks to the support that I'm getting from [Factoría Cultural](http://factoriacultural.es/).\nThey are amazing, so if you have a project in your head, just talk to them, you will not regret it.\n\nHere you have the interviews:\n\n * [Expansión](http://www.expansion.com/pymes/2017/12/13/5a294f9fe2704ee74d8b45b1.html), and\n * [La fábrica de ideas](http://www.rtve.es/alacarta/videos/fabrica-de-ideas/fdi-incuba/4327223/)\n\nThe Spanish TV show, can be viewed here. Enjoy!\n\n<div style=\"width:100%;padding-top:64%;position:relative;border-bottom:1px solid #aaa;display:inline-block;background:#eee;background:rgba(255,255,255,0.9);\">\n    <iframe frameborder=\"0\" src=\"http://www.rtve.es/drmn/embed/video/4327223\" name=\"Incuba: Factoría Cultural\" scrolling=\"no\" style=\"width:100%;height:90%;position:absolute;left:0;top:0;overflow:hidden;\" allowfullscreen></iframe>\n    <div style=\"position:absolute;bottom:0;left:0;font-family:arial,helvetica,sans-serif;font-size:12px;line-height:1.833;display:inline-block;padding:5px 0 5px 10px;\">\n        <span style=\"float:left;margin-right:10px;\">\n        \t<img style=\"height:20px;width:auto;background: transparent;padding:0;margin:0;\" src=\"http://img.irtve.es/css/rtve.commons/rtve.header.footer/i/logoRTVEes.png\">\n        </span>\n       \t<a style=\"color:#333;font-weight:bold;\" title=\"Incuba: Factoría Cultural\" href=\"http://www.rtve.es/alacarta/videos/fabrica-de-ideas/fdi-incuba/4327223/\">\n            <strong>Incuba: Factoría Cultural</strong>\n\t\t</a>\n\t</div>\n</div>\n",
    "iso8601Date": "2018-01-08T01:00:00+01:00",
    "basename": "2018-01-08-on-the-press"
  },
  "2018-12-20-year-in-review": {
    "layout": "blog",
    "title": "Year in review",
    "date": "2018-12-20T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "back",
    "icon_author": "Jay Toor",
    "icon_url": "https://unsplash.com/photos/PjABCLdM6DY",
    "description": "What we have done this 2018",
    "tags": "yearinreview, pybossa, scifabric",
    "author": "teleyinex",
    "preview": "2018 is finishing, and I would like to reflect on the numbers that …",
    "content": "\n\n2018 is finishing, and I would like to reflect on the numbers that we have achieved this year working hard. Hey, after all, we want to prove to Santa that we were good kids over the year so we can enjoy this Christmas with wonderful toys (and the family, of course!).\n\n<div style=\"width:100%;height:0;padding-bottom:56%;position:relative;\"><iframe src=\"https://giphy.com/embed/3ofT5EtPNBpIjC8jTy\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/filmeditor-will-ferrell-elf-3ofT5EtPNBpIjC8jTy\">via GIPHY</a></p>\n\n## Projects\nThis year we have launched several projects, but two of them are Solar maps, La memoria del Circo and a European Space Agency project.\n\n### Solar maps\nWe worked with Greenpeace Spain to launch a crowdsourcing project where we ask people to measure the area of the Spanish public buildings. The goal? Put solar panels on those roofs and learn how much energy and CO2 we can save. \n\nThe project was launched in May, and as of today, more than 900 buildings have been completed, resulting in more than 820 thousand square meters to install solar panels. This measured area means that the Spanish government can save more than 230 million, yes MILLION, of Euros in 25 years, and 36 thousand tons of CO2. Not bad, right? Moreover, if we install these panels, it is like if we remove from the roads more than 284 thousand cars!!! Amazing.\n\n\n<video controls style=\"max-width: 300px;\" autoplay>\n\t<source src=\"{{site.cdn}}/assets/video/greenpeace.mp4\" type=\"video/mp4\">\n</video>\n\n## La memoria del Circo\nThis project is so beautiful that we can only love it. PYBOSSA has been quite popular lately within the GLAM (Galleries, Libraries, Archives, and Museums) sector and this project is one of those that get your childhood back. Because who has not gone to a Circus? Older or younger, with the changes in it, we always have been in this magical place. \n\nFor these reasons, Factoría Cultural wanted to launch a crowdsourcing project where we could recover the history of the circus. Because the past is missing in private collections and almost no one can access it.\n\nThe project has started with a tiny and unique collection: the last performance in the mythical Spanish Circus PRICE. \n\n## Detecting buried archaeological sites\nWhen we were contacted by ESA (European Space Agency) about using our PYBOSSA powered platform to detect buried archaeological sites we were amazed. \n\nThe project is cool. ESA is using aerial images to find archaeological structures beneath the ground in areas surrounding the city of Rome, Italy. And right now you should be asking yourself: how do they do it? Well, it turns out that crop marks in this area are usually of ancient Roman origin. They tend to have clear geometric patterns, revealing the foundations of buildings. Roman roads can be distinguished as very straight lines of parched vegetation (overlying ancient paving stones), with strips of greener vegetation on either side, where ditches used to be.\n\n<img src=\"https://raw.githubusercontent.com/ESA-PhiLab/pybossa-eo-browser-imagery-microtasking/master/data/CropMarkExamples.png\" style=\"width:100%;\"/>\n\nHow cool is that?!\n\n\n## PYBOSSA\nWe have worked hard in improving PYBOSSA, and we launched what we think it's the most prominent feature of the year: you can now capture data directly within PYBOSSA. Yes, you don't need EpiCollect+ or any other tool to capture videos, images, audio and geolocalize them. \n\n<div style=\"width:100%;height:0;padding-bottom:57%;position:relative;\"><iframe src=\"https://giphy.com/embed/WwCGttCAHbwMYsOnBz\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/WwCGttCAHbwMYsOnBz\">via GIPHY</a></p>\n\nPYBOSSA now allows you to build surveys and spice them with some videos of the area. You can ask users to read aloud some text on the street and save it, or well, use your imagination :-) This feature is fantastic because you can even get your Internet of Things (IoT) recording data that can be automatically sent to a PYBOSSA server so the crowd can analyze/validate it. \n\n## Numbers\n<div style=\"width:100%;height:0;padding-bottom:56%;position:relative;\"><iframe src=\"https://giphy.com/embed/ccQ8MSKkjHE2c\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/adventure-time-math-ccQ8MSKkjHE2c\">via GIPHY</a></p>\n\nRegarding numbers this year, all our \"known\" PYBOSSA servers have saved more than 730 thousand classifications, analysis, transcriptions, geolocalization, validation of tasks. Not bad, as this is only from 12 servers, just from our clients. We know that there are many more PYBOSSA servers out there, so PYBOSSA is helping tons of people all over the world. For example, [LibCrowds](https://www.libcrowds.com/) from the British Library has more than 127 thousand contributions (since its creation), and [Burntheregister](https://burntheregister.com/) has around 9 thousand contributions. \n\nWe have written more than 218 thousand lines of frontend code. We use Vuejs, Nuxtjs to build Progressive Web Applications for our clients. \n\nFor PYBOSSA code we have added 4438 lines of code, removed 1246 and modified 142 files this year. Not bad at all.\n\n## Summary\nTwo thousand eighteen has been an excellent year, and we hope that 2019 will be even better! Merry Christmas and Happy New Year!!!\n",
    "iso8601Date": "2018-12-20T01:00:00+01:00",
    "basename": "2018-12-20-year-in-review"
  },
  "2019-01-18-python2-to-3": {
    "layout": "blog",
    "title": "101 Migrating Python 2 to 3",
    "date": "2019-01-18T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "migration",
    "icon_author": "Ethan Weil",
    "icon_url": "https://unsplash.com/photos/IrI889hknhc",
    "description": "A few basic tips to migrate your Python 2 to 3",
    "tags": "python, pybossa, migration, backend",
    "author": "teleyinex",
    "preview": "We've just started 2019, but hey, the clock is ticking, and we only …",
    "content": "\n\nWe've just started 2019, but hey, the clock is ticking, and we only have 11 months to port all our Python2.X code to version 3. Are you ready? In this blog post, we will share a few tips that we have learned while migrating our PYBOSSA code to python 3.\n<div style=\"width:100%;height:0;padding-bottom:75%;position:relative;\"><iframe src=\"https://giphy.com/embed/thNsW0HZ534DC\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/countdown-thNsW0HZ534DC\">via GIPHY</a></p>\n\n## Creating a virtual environment\n\nWe always run our Python libraries within a virtual env. However, you don't create them in the same way. In Python 2, you do it like this:\n\n```\nvirtualenv env\n```\nWhile for Python 3 you should do the following:\n\n```\npython3 -m venv env\n```\nThen you can activate it as usual:\n\n```\nsource env/bin/activate\n```\n# Using 2to3 to save some time\nEvery project varies in size and number of lines of code, but you, the developer want to be fast on this, right? Or at least not too painful.\n\nPython has a beneficial tool that will help you to migrate your old code to the new version by only running a command line: 2to3.\n\nThis command is pretty simple. If you want to see the changes that it will do to your code, you just run it like this:\n\n```\n2to3 example.py\n```\nIf you want to write the changes directly into your example.py file, pass a -w. Be sure to use git (or something similar so that you can check the differences):\n```\n2to3 -w example.py\n```\nAnd if you want, you can do something crazy like changing all files at once:\n```\n2to3 -w *.py\n```\nThen, check the damage ;-)\n\n## Strings, Bytes, UTF-8, aka your nightmare\nWhile migrating PYBOSSA to Python3 most of the issues on our side were related to UTF-8 and how we had the strings in Python 2.\n\nThe general advice is to check for the type of the strings, and based on that do whatever you need.\n\n### Bytes to Strings\n```\nif type(foo) == bytes:\n\tbytes.decode('utf-8')\n```\n### Strings to Bytes\nYou will have several places where your code will fail because now Python 3 expects Bytes instead of strings. For those cases, you can do the following:\n\n```\nif type(foo) == str:\n\tbytes.encode('utf-8')\n```\n\n### StringIO and BytesIO\nYou have probably used StringIO in your code. Well, it will not work probably in your migration. You will need to adapt it. Usually, based on the type of the text, you will have to use StringIO or BytesIO. Both will solve it for you. A typical case is like this:\n\n```\nif type(foo) == bytes:\n\tdata = BytesIO(foo)\nelse:\n\tdata = StringIO(foo)\n```\n\n### CSV and UTF-8\nWe used the old Python 2 CSV UTF-8 reader example given in the documentation. For Python 2 it works great, but when you try it in Python 3, everything goes to hell :-)\n\nThe solution? Use Pandas. Yes, Pandas. It will make your life so much easier.\n\nPandas will handle everything for you. All you have to do is replace your csv_reader with this:\n\n```\nimport pandas as pd\ndf = pd.read_csv(yourfile)\n```\nIf you then need to test this code, and you don't have a file, use the previous StringIO or BytesIO to do the magic. Then, load it, and you are done!\n\n```\ndef do_something(csvfile):\n\treturn pandas.read_csv(csvfile)\n\ndef test_01():\n\tfakefile = StringIO('foo,bar\\n,1,2')\n\tdf = do_something(fakefile)\n        # Check whatever you want\n```\nAnd that's it. Nothing else ;-) Well, yes, put all your prints with () :D\n",
    "iso8601Date": "2019-01-18T01:00:00+01:00",
    "basename": "2019-01-18-python2-to-3"
  },
  "2019-01-25-jekyll2nuxtjs": {
    "layout": "blog",
    "title": "Migrate your Jekyll's site to NuxtJS",
    "date": "2019-01-18T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "moving",
    "icon_author": "Ethan Weil",
    "icon_url": "https://unsplash.com/photos/9Q7PqDxCZeQ?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText",
    "description": "Learn how you can migrate your static site to NuxtJS",
    "tags": "frontend, vuejs, nuxtjs, migration, staticsite, jekyll",
    "author": "teleyinex",
    "preview": "Static sites are becoming really popular nowadays. Basically, …",
    "content": "\n\nStatic sites are becoming really popular nowadays. Basically, because you don't need to have a server, and there're plenty of free hosting services on the web: Netlify, Github Pages, Zeit now, etc.\n\nYou can actually choose from more than [200 frameworks](https://www.staticgen.com/). Actually, you should be able to find one that meets your preferences, because there's almost a framework for each programming language.\n\nThe most popular one is [Jekyll](https://www.staticgen.com/jekyll). It has more than 36 thousand stars in Github, followed closely by [Next](https://www.staticgen.com/next) and [Hugo](https://www.staticgen.com/hugo).\n\nAs you can guess, I've been using Jekyll for a very long time. Actually, all my websites are built with it: [Scifabric.com](https://scifabric.com), [PYBOSSA](https://pybossa.com) and obviously my home page (where you are right now).\n\nThe simplicity of Jekyll is amazing. Anyone can jump automatically, and start building a site within minutes. However, if you are an experienced JS developer, or if you want to create a Progressive Web Application, Jekyll is not the best option. Which was my personal situation.\n\nHence, what could I do? Well, I started coding with VueJS since its version 1.0, then, 2.0 and then I fall in love with [NuxtJS](https://nuxtjs.org/). \n\nNuxtJS is AWESOME because it handles for you tons of stuff that you don't want to deal with, and have a decent template to start with:\n\n* Performance: it uses VueJS and NodeJS best practices, so you can get the best and minimal application for your site.\n* Modular: it has more than 50 modules that will save you tons of work. You can build a Progressive Web Application (PWA) without having to write a single line of code. How cool is that?\n* Enjoyable: probably one of the best features. As I said before, I'm in deep love with this framework and VueJS. It provides meaningful information, good docs, and a better community.\n\nMy love with NuxtJS has been driving my late front-end development for Scifabric. Basically, we are using for every client, as we can build really quickly any PWA that will fit the client's needs. One example is our project for Greenpeace Spain: [https://solarmaps.greenpeace.org](https://solarmaps.greenpeace.org).\n\nAs I worked more and more with NuxtJS, I was wondering how difficult would it be to migrate my home page to NuxtJS and leave behind Jekyll. Why? Because I've become used to Axios, Components, Stylus, and modern tools to build sites, so I wanted to do it for my own site.\n\nThe only problem: my Jekyll site has several folders with Markdown files, that rely heavily on the front-matter of them to build the site. In any case, I started to search if this could be possible, and obviously, it is :D\n\n## Handling Markdown in NuxtJS\nThere're plenty of solutions out there for importing and handling your Markdown files. For example, you can use the awesome [Webpack Loader for Markdown](https://www.npmjs.com/package/frontmatter-markdown-loader) to get your files imported into NuxtJS.\n\nThis was my first try. Using it. While in development mode all worked well, the problem came when I built the static site. The web browser's console was complaining that it was missing the webpack loader for the Markdown files, and therefore, my blog posts didn't show up.\n\nThen, I thought, what if I keep things even more simple. What if I can transform the Markdown files into JSON? I did a quick search in the npm registry and I found this amazing library: [markdown-to-json](https://www.npmjs.com/package/markdown-to-json).\n\nThis library was PERFECT. Why? Because you can pass a folder with all your Markdown files, and it will generate a JSON object with all your blog posts in there. The front-matter is parsed and you get it as JSON as well. As everything is JSON, you can request the data using HTTP requests, and you will have everything in place. \n\nI tried with my current blog posts and everything worked like a charm. Thus, what I did was the following: in my NuxtJS project, I created a folder named **content**. In this folder, I have my blog posts and projects sub-folders with all the Markdown files. Then, I can run a command like this to create the JSON file with all the information:\n\n```\n$ m2j -c content/blogposts/*.md -o static/blogposts.json\n```\nBy placing the blogposts.json file in the /static folder, I can use within NuxtJS, Axios to get all the blog posts easily. The only problem is that when you are generating your static site, you need to actually generate all the routes for it. \n\n## Creating routes for NuxtJS\nHence, how do we handle this? Well, easily. We only need to modify our package.json file to add a few more build commands:\n\n```\n    \"dev\": \"yarn md2json && nuxt\",\n    \"build\": \"yarn md2json && nuxt build\",\n    \"md2json\": \"m2j -c content/blogposts/*.md -o static/blogposts.json\"\n\n```\nThanks to this solution, when you run yarn dev (or npm run dev) your blog posts will be converted into JSON, and the site will be ready to work with it. The same approach is used for building the static site. This is wonderful because then you can use it with Zeit, Github Pages or Netlify to build it automatically for you.\n\nWith this solved, I only had to build my site. It was more or less easy, as I was already using webpack and components in JS for my Jekyll site. In most of the cases, I copied and paste chunks of code, and then adapted them to the NuxtJS world.\n\nThe result? This page. Which as you can see is amazingly fast (it's deployed on Zeit). If you are browsing the site from a phone on Android (or iOS) and you are using a modern web browser, you will see that you can install my site in your phone ;-)\n\nMoreover, the audits are really cool. Check them out:\n\n![Audits from Google](/assets/img/blog/audits-daniel.png)\n\n## A template\nAs I was doing the migration of my site I realized that this could be really useful for others as well, so I've created a basic template that you can re-use. The code is available here: [https://github.com/teleyinex/jekyll2nuxt](https://github.com/teleyinex/jekyll2nuxt).\n\nThis template uses [VuetifyJS](http://vuetifyjs.com/) so you can get a material design out of the box for your site. It has the PWA and Markdown modules enabled as well, so everything will work for you out of the box. You only have to copy your blog posts into the content folder and run yarn dev. As simple as that!\n\nIf you have read until here, THANKS a lot for your time :raised_hands:. I would appreciate if you share this blog post within your Twitter (or Facebook) friends, so more people can know about it. \n",
    "iso8601Date": "2019-01-18T01:00:00+01:00",
    "basename": "2019-01-25-jekyll2nuxtjs"
  },
  "2019-01-25-lambdawebscrapping": {
    "layout": "blog",
    "title": "Visualizing The Air Quality With Lambda Functions",
    "date": "2019-01-18T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "pollution",
    "icon_author": "Thomas Millot",
    "icon_url": "https://unsplash.com/photos/q5jKHtV4hWc",
    "description": "How to transform open data into APIs using lambda functions",
    "author": "teleyinex",
    "preview": "# Lambda functions\n …",
    "content": "\n\n# Lambda functions\n\n[Lambda](https://en.wikipedia.org/wiki/AWS_Lambda) functions have been with us since 2014.\nHowever, while I understand how they work, I didn't find a way to use them in a real case. Until this weekend :stuck_out_tongue_winking_eye:.\n\nIf this is the first time that you hear this term, a lambda function is a web\nservice, it could be Amazon, Zeit or Netlify, that will allow you to run a\nfunction (in your preferred programming language) when you access a given URL.\n\nIn theory, the advantages are that you don't have a real server running all the\ntime, and that they will only charge for each call to that URL, so it should be\n\"cheaper.\"\n\nBut, as it is serverless, you don't have access to a DB, and therefore you\ncannot do \"too many things.\" That's why I didn't see its use cases. Until I\nfigured out.\n\n## Building APIs using lambda functions\n\nDue to my job, I work a lot with data. Broadly speaking, I usually work with\nopen data, so, the data is publicly available, but generally, there's not an API\nfor them.\n\nIn those cases, they usually offer you a CSV, XML or similar file, that you will\nhave to download every single day (or every hour) to work with it. In other\nwords, a bit messy.\n\nBut, OK, yes, it works. But it's not the best. Especially if you want to make\nqueries, because in those cases you will have to build your own search\nfunctionality or store all the data in your preferred DB.\n\nObviously, all the previous comments depend on what you want to do with your\nproject. If you only want to show old data (or aggregated one), the CSV or XML\nis perfect. But if you're going to do something more dynamic, then, you have a\nproblem, and you will  have to hack a bit :smile:\n\nFrom this requirement, I found the answer to my question: what if we could use\nlambda functions to do scrapping of open data and return everything in JSON?\nThe answer: yes you can, and it works like a charm.\n\n## An API for Madrid's air quality portal\n\nI discovered this idea because some time ago, I was trying to see how the air\nquality data of Madrid, Spain was published. The result was a bit sad, as you\ncan have access to the CSV and XML that I mentioned before, an hour prediction\nper station, and a PDF. In other words, nothing really \"useful\" if you want to\nbuild something cool with your #jamstack. Something like a Progressive Web\nApplication.\n\n![pantallazo de la web del Ayuntamiento](/assets/img/blog/aire1.png)\n\nThe council's web page shows you the data, but obviously it's not responsive, it\nhas five tables (one within the other) to create the layout... :scream:\n(somebody should talk to them about HTML5, flexbox, and grid).\n\nSo, I started to work on it. After checking the web, I found that in their forecast per hours, you can select a station and a date (or pollutant) to get the data in\na nice table. After opening the dev tools from Chrome, I found that this form\nsends the following information:\n\n![pantallazo del formulario](/assets/img/blog/form.png)\n\nEasy peasy. I only needed to make the same request, passing the station ID and a\ngiven date. The web page will return me an HTML that I could parse. \n\n### Lambda functions in Zeit\n\nMy toolchain always involves Python and Js, so Zeit was a perfect option for\nme. Zeit allows you to create lambda functions for both languages, so it is\nperfect.\n\nIn my case, I chose Python and BeautifulSoup4 for doing the parsing.\n\nThe web page with the result has 5 tables (not a single div, OMG). I found the one that has the real data, and with that info I return this beautiful JSON:\n\n```\n{\n\"Dióxido de Azufre[µg/m³]\": [], // 24 items\n\"Dióxido de Nitrógeno[µg/m³]\": [], // 24 items\n\"Ozono[µg/m³]\": [\n22,\n32,\n36,\n41,\n52,\n50,\n46,\n46,\n44,\n-1,\n51,\n54,\n55,\n57,\n59,\n60,\n55,\n52,\n47,\n40,\n41,\n43,\n39,\n39\n],\n\"Hora\": [], // 24 items\n\"estacion\": \"Estación de Villaverde\",\n\"fecha\": \"10/02/2019\"\n}\n```\n\nReady to be consumed by any JS library that can paint any beautiful chart. The API allows me to use any station from Madrid an any given date.\n\nWith this ready, all I had to do was to adapt my script to become a lambda\nfunction.\n\nThe documentation about how to do it in Zeit is a bit [scarce](https://zeit.co/examples/python). However, doing a bit of googling you will get what you need.\nWith only 47 lines of code, I have a lambda function that parses the air quality\nof Madrid in real time :sunglasses::\n\n```\nfrom http.server import BaseHTTPRequestHandler\nimport requests\nimport json\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse, parse_qs\n\n\nclass handler(BaseHTTPRequestHandler):\n\n    def do_GET(self):\n        self.send_response(200)\n        self.send_header('Content-type', 'application/json')\n        self.send_header('Access-Control-Allow-Origin', '*')\n        self.end_headers()\n        estacion = parse_qs(urlparse(self.path).query).get('estacion')[0]\n        date = parse_qs(urlparse(self.path).query).get('date')[0]\n        data = {'menu':  'consulta', 'smenu': 'reports', 'link': 'data',\n                'view': 'data', 'magnitud': '', 'estacion': int(estacion),\n                'date': date}\n        url = 'http://www.mambiente.munimadrid.es/sica/scripts/index.php?lang=es'\n        r = requests.post(url, data=data)\n        soup = BeautifulSoup(r.content, 'html.parser')\n        tables = soup.find_all('table')\n        table = tables[4]\n        meta = table.find_all(class_='hs')\n        headers = table.find_all(class_='hd')\n        tmp = dict()\n        data = table.find_all(class_='datos')\n        val = []\n        for i in range(len(headers)):\n            val.append(list())\n        for idx, d in enumerate(data):\n            k = idx % len(headers)\n            v = d.get_text()\n            if ((':' not in v) and ('-' not in v)):\n                v = float(v)\n            else:\n                if ('-' in v):\n                    v = -1.0\n            val[k].append(v)\n        for idx, h in enumerate(headers):\n            k = idx % len(headers)\n            v = h.get_text()\n            tmp[v] = val[k]\n        tmp['estacion'] = meta[0].get_text()\n        tmp['fecha'] = meta[1].get_text()\n        self.wfile.write(str(json.dumps(tmp)).encode())\n```\n\nEasy, simple and very clean. The best of all is that you can set any headers\nthat you want, so you can (you should actually) enable CORS so your frontend\nserver can use it.\n\nOnce you have done it, all you have to do is deploy it with one command: now.\n\n## Progressive Web Application\n\nAt this point, the only remaining part was the frontend server. Thus, I use:\nNuxtJS and created a SPA that will be deployed as well in Zeit. The SPA is a\nPWA so you can install it on your phone.\n\nFor the UI toolkit, I'm using Vuetify and its Sparklines components. They are\nsuper cool, and you can create these amazing animations:\n\n<video width=\"100%\" controls>\n  <source src=\"/vid/aire.mp4\" type=\"video/mp4\">\n  Your browser does not support HTML5 video.\n</video>\n\nIf you want to try the SPA go here: \n[https://aire.daniellombrana.es](https://aire.daniellombrana.es)\n\n## Final thoughts\n\nI'm really surprised about how well everything works. Especially, knowing that I\nbuilt everything in 4 hours (I didn't try before lambda functions).\n\nI love the idea that I can use this for doing really quick prototypes, that will\nsave me time and effort in setting up a server with its DB.\n\nWhat do you think? Did you like it? Let me know on Twitter!\n",
    "iso8601Date": "2019-01-18T01:00:00+01:00",
    "basename": "2019-01-25-lambdawebscrapping"
  },
  "2019-02-20-fulltextsearchclient": {
    "layout": "blog",
    "title": "Full text search: a simple guide for your static site",
    "date": "2019-01-18T00:00:00.000Z",
    "quote": "Lorem ipsum dolor sit amet, consectetur adipisicing elit",
    "icon": "searching",
    "icon_author": "Forest simon",
    "icon_url": "https://unsplash.com/photos/-TX0ufDSCV4",
    "description": "How to index your static site and make it searchable without a DB",
    "author": "teleyinex",
    "preview": "# Searching in your static site\n\nAs you already know,  static web …",
    "content": "\n\n# Searching in your static site\n\nAs you already know,  static web pages are pretty popular nowadays. However you wil have a little\nissue: it's difficult to add a search engine within your site, as you don't have\na DB behind it.\n\n<div style=\"width:100%;height:0;padding-bottom:73%;position:relative;\"><iframe src=\"https://giphy.com/embed/l3q2PZSVUUEsajBIY\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/oscars-academy-awards-1952-l3q2PZSVUUEsajBIY\">via GIPHY</a></p>\n\nThere're some solutions out there like [Algolia](https://www.algolia.com/), or specific servers like [Solr](http://lucene.apache.org/solr/).\nThe first is simple, but at the end you are giving your data to a third party.\nThe next option, Solr, is really cool, but well, we're setting up a serverless\nsolution, right? So we cannot use it :smile:.\n\nThus, what solution do we have?\n\n## Lunr.JS a bit like Solr, but much smaller and not as bright\n\n<div style=\"width:100%;height:0;padding-bottom:100%;position:relative;\"><iframe src=\"https://giphy.com/embed/rZKXaQyfvZQv6\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/moon-cute-black-rZKXaQyfvZQv6\">via GIPHY</a></p>\n\nWhile I was looking for a solution to this issue, I found this library [LunrJS](https://lunrjs.com/)\n\nThe library is pretty small, it has not dependencies, you can install it with\nyarn (or if y ou prefer with npm). You can index hundreds of documents without\nissues, and all you have to do is to specify what do you want to index, and what\nfield are you gonna use to identify the document.\n\nIf on top of that I'll tell you that it does stemming and it supports several\nlanguages, well, you would be thinking that this solution is :fire:\n\nNOTE, from its website: \"Stemming is the process of reducing inflected or derived words to their base or stem form. For example, the stem of *searching*, *searched* and *searchable* should be *search*. This has two benefits: firstly the number of tokens in the search index, and therefore its size, is significantly reduced, and in addition, it increases the recall when performing a search. A document containing the word *searching* is likely to be relevant to a query for *search*\".\n\n### Integrating LunrJS in NuxtJS\n\n<div style=\"width:100%;height:0;padding-bottom:42%;position:relative;\"><iframe src=\"https://giphy.com/embed/AvMJCeu1EMmhG\" width=\"100%\" height=\"100%\" style=\"position:absolute\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe></div><p><a href=\"https://giphy.com/gifs/reaction-this-is-the-end-breaks-AvMJCeu1EMmhG\">via GIPHY</a></p>\n\nThe integration is pretty straightforward. All you have to do is importing the\nlibrary like this:\n\n```\nvar lunr = require(\"lunr\")\nrequire('lunr-languages/lunr.stemmer.support')(lunr)\nrequire('lunr-languages/lunr.multi')(lunr)\nrequire('lunr-languages/lunr.es')(lunr)\n```\n\nThen, you can use your asyncData or created methods (depending on where are you\ngetting the data to get indexed) and you write something like this:\n\n```\n  created() {\n    const self = this\n    const idx = lunr(function() {\n      this.use(lunr.multiLanguage('en', 'es'))\n      this.ref('basename')\n      this.field('content')\n      self.blogposts.forEach(blog => this.add(blog), this)\n    })\n    this.$store.commit('setIdx', idx)\n  },\n```\n\nWith those lines we are telling Lunr what is the unique identifier, in this case\nthe *basename*. Then, we tell it what do we want to index, in this case\n*content* (which is our compiled Markdown from my blogposts). Finally, we pass\nall the blogposts to the index, and we move it to the store, so you can use it\nfrom any component. Simple?\n\n### Searching\n\nNow all you have to do is doing a search. For this task, you can write a method\nlike this:\n\n```\nsearch() {\n  const found = this.$store.state.idx.search(this.query)\n  this.$store.commit('setFound', found)\n}\n```\n\nWith this method we get the results and we save them in the store.\n\nNOTE: if you want to do searches while the user is typing, you can. The only\nthing you have to do is to not fire with every key stroke the search method, so\nyou should use [debounce](https://lodash.com/docs/4.17.11#debounce) from Lodash\nto throttle the calls.\n\n## Result\n\nThe result of this blog post is that I've applied it to my blog site, and now\nyou can search within all the blog posts that I have written. As soon as you are\ntyping, the search method is called (via the debounce function) and the results\nget updated in real time. The search is reeeaaallly fast, because everything is\nin memory.\n\nI love this solution because it's clean, fast an simple.\n",
    "iso8601Date": "2019-01-18T01:00:00+01:00",
    "basename": "2019-02-20-fulltextsearchclient"
  }
}
